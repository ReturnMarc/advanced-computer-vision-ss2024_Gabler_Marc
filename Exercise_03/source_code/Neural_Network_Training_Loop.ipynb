{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying From Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Marc\\miniconda3\\envs\\pa\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To classify tabular data we use fastai.tabular\n",
    "from fastai.tabular.all import * \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import random\n",
    "import unittest\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data set\n",
    "# Test data get partitioned into a separate file already now\n",
    "from pathlib import Path\n",
    "data_path = Path('./data/mecs/MECS_2-Phase-Steels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10735 entries, 0 to 10734\n",
      "Columns: 107 entries, class to std. convexity\n",
      "dtypes: float64(106), object(1)\n",
      "memory usage: 8.8+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>equiv. diameter</th>\n",
       "      <th>major axis length</th>\n",
       "      <th>minor axis length</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>equiv. radius</th>\n",
       "      <th>max feret diameter</th>\n",
       "      <th>min feret diameter</th>\n",
       "      <th>mean feret diameter</th>\n",
       "      <th>convex perimeter</th>\n",
       "      <th>...</th>\n",
       "      <th>std. relativ area</th>\n",
       "      <th>std. convex area/filled area</th>\n",
       "      <th>std. axial ratio</th>\n",
       "      <th>std. aspect ratio</th>\n",
       "      <th>std. roundness</th>\n",
       "      <th>std. circularity</th>\n",
       "      <th>std. sphericity</th>\n",
       "      <th>std. convex per./filled per.</th>\n",
       "      <th>std. form factor</th>\n",
       "      <th>std. convexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>category_3</td>\n",
       "      <td>15.225748</td>\n",
       "      <td>11.478270</td>\n",
       "      <td>6.516211</td>\n",
       "      <td>101.586092</td>\n",
       "      <td>7.612874</td>\n",
       "      <td>28.476331</td>\n",
       "      <td>14.601479</td>\n",
       "      <td>21.538905</td>\n",
       "      <td>69.846721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.360265</td>\n",
       "      <td>0.247471</td>\n",
       "      <td>0.172865</td>\n",
       "      <td>0.167522</td>\n",
       "      <td>0.150141</td>\n",
       "      <td>0.991127</td>\n",
       "      <td>0.257917</td>\n",
       "      <td>0.254766</td>\n",
       "      <td>0.725854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>category_3</td>\n",
       "      <td>1.151113</td>\n",
       "      <td>0.974243</td>\n",
       "      <td>0.385487</td>\n",
       "      <td>5.609659</td>\n",
       "      <td>0.575557</td>\n",
       "      <td>2.580962</td>\n",
       "      <td>0.888856</td>\n",
       "      <td>1.734909</td>\n",
       "      <td>5.555157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193671</td>\n",
       "      <td>0.345320</td>\n",
       "      <td>0.108488</td>\n",
       "      <td>0.097143</td>\n",
       "      <td>0.104520</td>\n",
       "      <td>0.107560</td>\n",
       "      <td>0.357665</td>\n",
       "      <td>0.126119</td>\n",
       "      <td>0.195196</td>\n",
       "      <td>0.270121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>category_3</td>\n",
       "      <td>1.450391</td>\n",
       "      <td>1.068283</td>\n",
       "      <td>0.524281</td>\n",
       "      <td>5.629028</td>\n",
       "      <td>0.725196</td>\n",
       "      <td>2.312959</td>\n",
       "      <td>1.144310</td>\n",
       "      <td>1.728635</td>\n",
       "      <td>5.545392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102618</td>\n",
       "      <td>0.441363</td>\n",
       "      <td>0.205836</td>\n",
       "      <td>0.167468</td>\n",
       "      <td>0.075052</td>\n",
       "      <td>0.079354</td>\n",
       "      <td>0.199565</td>\n",
       "      <td>0.134383</td>\n",
       "      <td>0.116518</td>\n",
       "      <td>0.255049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>category_3</td>\n",
       "      <td>3.561641</td>\n",
       "      <td>2.350421</td>\n",
       "      <td>1.400185</td>\n",
       "      <td>13.099335</td>\n",
       "      <td>1.780821</td>\n",
       "      <td>5.162738</td>\n",
       "      <td>3.039382</td>\n",
       "      <td>4.101060</td>\n",
       "      <td>12.632860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040841</td>\n",
       "      <td>0.347140</td>\n",
       "      <td>0.234101</td>\n",
       "      <td>0.159523</td>\n",
       "      <td>0.160860</td>\n",
       "      <td>0.144939</td>\n",
       "      <td>1.299666</td>\n",
       "      <td>0.328635</td>\n",
       "      <td>0.253970</td>\n",
       "      <td>1.025188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>category_3</td>\n",
       "      <td>6.676158</td>\n",
       "      <td>5.226249</td>\n",
       "      <td>2.784733</td>\n",
       "      <td>35.279133</td>\n",
       "      <td>3.338079</td>\n",
       "      <td>12.536480</td>\n",
       "      <td>6.907373</td>\n",
       "      <td>9.721926</td>\n",
       "      <td>30.297397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009716</td>\n",
       "      <td>0.257991</td>\n",
       "      <td>0.253047</td>\n",
       "      <td>0.162753</td>\n",
       "      <td>0.156650</td>\n",
       "      <td>0.134192</td>\n",
       "      <td>1.073103</td>\n",
       "      <td>0.273935</td>\n",
       "      <td>0.238783</td>\n",
       "      <td>0.793738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class  equiv. diameter  major axis length  minor axis length  \\\n",
       "0  category_3        15.225748          11.478270           6.516211   \n",
       "1  category_3         1.151113           0.974243           0.385487   \n",
       "2  category_3         1.450391           1.068283           0.524281   \n",
       "3  category_3         3.561641           2.350421           1.400185   \n",
       "4  category_3         6.676158           5.226249           2.784733   \n",
       "\n",
       "    perimeter  equiv. radius  max feret diameter  min feret diameter  \\\n",
       "0  101.586092       7.612874           28.476331           14.601479   \n",
       "1    5.609659       0.575557            2.580962            0.888856   \n",
       "2    5.629028       0.725196            2.312959            1.144310   \n",
       "3   13.099335       1.780821            5.162738            3.039382   \n",
       "4   35.279133       3.338079           12.536480            6.907373   \n",
       "\n",
       "   mean feret diameter  convex perimeter  ...  std. relativ area  \\\n",
       "0            21.538905         69.846721  ...           0.002630   \n",
       "1             1.734909          5.555157  ...           0.193671   \n",
       "2             1.728635          5.545392  ...           0.102618   \n",
       "3             4.101060         12.632860  ...           0.040841   \n",
       "4             9.721926         30.297397  ...           0.009716   \n",
       "\n",
       "   std. convex area/filled area  std. axial ratio  std. aspect ratio  \\\n",
       "0                      0.360265          0.247471           0.172865   \n",
       "1                      0.345320          0.108488           0.097143   \n",
       "2                      0.441363          0.205836           0.167468   \n",
       "3                      0.347140          0.234101           0.159523   \n",
       "4                      0.257991          0.253047           0.162753   \n",
       "\n",
       "   std. roundness  std. circularity  std. sphericity  \\\n",
       "0        0.167522          0.150141         0.991127   \n",
       "1        0.104520          0.107560         0.357665   \n",
       "2        0.075052          0.079354         0.199565   \n",
       "3        0.160860          0.144939         1.299666   \n",
       "4        0.156650          0.134192         1.073103   \n",
       "\n",
       "   std. convex per./filled per.  std. form factor  std. convexity  \n",
       "0                      0.257917          0.254766        0.725854  \n",
       "1                      0.126119          0.195196        0.270121  \n",
       "2                      0.134383          0.116518        0.255049  \n",
       "3                      0.328635          0.253970        1.025188  \n",
       "4                      0.273935          0.238783        0.793738  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading CSV file containing training and validation data\n",
    "import pandas as pd\n",
    "dataframe = pd.read_csv(data_path, sep=';')\n",
    "\n",
    "# Looking at the first data\n",
    "print(dataframe.info())\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "n_features = len(dataframe.columns)-1\n",
    "\n",
    "def extract_numpy_from_df( dataframe, y_column ):\n",
    "    X = dataframe.drop(columns=dataframe.columns[y_column]).to_numpy()\n",
    "    Y = dataframe.iloc[:,y_column].to_numpy()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "def clean_data( dataframe ):\n",
    "    dataframe.dropna(inplace=True)\n",
    "    if dataframe.shape[0] < 100:\n",
    "        raise ValueError(\"Data set too small after cleaning, less than 100 rows\") \n",
    "\n",
    "    return dataframe\n",
    "\n",
    "    \n",
    "def get_validation_and_training_indices( dataset_length ):\n",
    "    random.seed(42)\n",
    "    indices_train = random.sample( range( dataset_length ), int( dataset_length * 0.8 ) )\n",
    "    indices_validation = list( set(range( dataset_length )) - set(indices_train) )\n",
    "    \n",
    "    return indices_train,indices_validation\n",
    "\n",
    "def normalize( X ):\n",
    "    if X.std() == 0:\n",
    "        raise ZeroDivisionError(\"The data cant be normalized, the standard deviation is zero. Data is constant.\")\n",
    "    else:\n",
    "        X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "        return X\n",
    "\n",
    "\n",
    "def hot_1_encode( Y, codes ):\n",
    "    Y_encoded = numpy.zeros( (Y.shape[0], len(codes)) )\n",
    "    for i in range( Y.shape[0] ):\n",
    "        Y_encoded[i][codes[Y[i]]] = 1\n",
    "    \n",
    "    return Y_encoded\n",
    "\n",
    "def create_batch( permutation, batch_no, batch_size, X ):\n",
    "    x_batch = torch.zeros( [ batch_size, X.shape[1] ], dtype=torch.float32 )\n",
    "    for i in range( batch_size ):\n",
    "        x_batch[i] = torch.tensor( X[ permutation[batch_no*batch_size + i] ] )\n",
    "    \n",
    "    return x_batch\n",
    "\n",
    "codes = { 'category_1' : 0, 'category_2' : 1, 'category_3' : 2 }\n",
    "\n",
    "dataframe = clean_data( dataframe )\n",
    "X,Y = extract_numpy_from_df( dataframe, 0 )\n",
    "\n",
    "indices_train,indices_validation = get_validation_and_training_indices( X.shape[0] )\n",
    "\n",
    "X   = normalize( X )\n",
    "Y   = hot_1_encode( Y, codes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_clean_data (__main__.TestNotebook) ... ok\n",
      "test_create_batch (__main__.TestNotebook) ... ok\n",
      "test_extract_numpy_from_df (__main__.TestNotebook) ... ok\n",
      "test_get_validation_and_training_indices (__main__.TestNotebook) ... ok\n",
      "test_hot_1_encode (__main__.TestNotebook) ... ok\n",
      "test_normalize (__main__.TestNotebook) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.022s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x2faa56f1de0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestNotebook(unittest.TestCase):\n",
    "\n",
    "    def test_clean_data(self):\n",
    "        data_too_small = pd.DataFrame({'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n",
    "\n",
    "        data_with_nan = pd.DataFrame({'Value': np.random.randn(200)})\n",
    "        nan_indices = np.random.choice(data_with_nan.index, size=20, replace=False)  # Choose 20 indices to be NaN\n",
    "        data_with_nan.loc[nan_indices, 'Value'] = np.nan\n",
    "        \n",
    "        self.assertRaises(ValueError, clean_data,data_too_small)\n",
    "        self.assertEqual(clean_data(data_with_nan).shape[0], 180)\n",
    "        \n",
    "\n",
    "    def test_extract_numpy_from_df(self):\n",
    "        df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [6, 7, 8, 9, 10], 'C': [11, 12, 13, 14, 15], 'D': [16, 17, 18, 19, 20]})\n",
    "        X,Y = extract_numpy_from_df(df, 2)\n",
    "\n",
    "        self.assertEqual(X.shape, (5,3))\n",
    "        self.assertEqual(Y.shape[0], 5)\n",
    "        self.assertEqual(Y[0], 11)\n",
    "\n",
    "        X,Y = extract_numpy_from_df(df, 0)\n",
    "        self.assertEqual(X.shape, (5,3))\n",
    "        self.assertEqual(Y.shape[0], 5)\n",
    "        self.assertEqual(Y[0], 1)\n",
    "\n",
    "    def test_normalize(self):\n",
    "        X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "        X = normalize(X)\n",
    "        self.assertAlmostEqual(X.mean(), 0)\n",
    "        self.assertAlmostEqual(X[1].mean(), 0)\n",
    "        \n",
    "        X = np.array([np.ones(5)])\n",
    "        self.assertRaises(ZeroDivisionError, normalize, X)\n",
    "\n",
    "    def test_hot_1_encode(self):\n",
    "        Y = np.array(['category_1', 'category_2', 'category_3', 'category_1', 'category_2'])\n",
    "        Y_encoded = hot_1_encode(Y, codes)\n",
    "\n",
    "        self.assertEqual(Y_encoded.shape, (5, 3))\n",
    "        self.assertEqual(Y_encoded[0][0], 1)\n",
    "        self.assertEqual(Y_encoded[1][1], 1)\n",
    "        self.assertEqual(Y_encoded[2][2], 1)\n",
    "        self.assertEqual(Y_encoded[3][0], 1)\n",
    "        self.assertEqual(Y_encoded[4][1], 1)\n",
    "    \n",
    "    def test_get_validation_and_training_indices(self):\n",
    "        indices_train, indices_validation = get_validation_and_training_indices(100)\n",
    "        self.assertEqual(len(indices_train), 80)\n",
    "        self.assertEqual(len(indices_validation), 20)\n",
    "        self.assertEqual(set(indices_train).intersection(set(indices_validation)) == set(), True)\n",
    "\n",
    "    def test_create_batch(self):\n",
    "        permutation = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "        X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26, 27], [28, 29, 30]])\n",
    "        x_batch = create_batch(permutation, 1, 4, X)\n",
    "        \n",
    "        self.assertEqual(x_batch.shape, (4, 3))\n",
    "        self.assertEqual(x_batch[0][0], 13)\n",
    "        self.assertEqual(x_batch[1][1], 17)\n",
    "        self.assertEqual(x_batch[2][2], 21)\n",
    "        self.assertEqual(x_batch[3][0], 22)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model( n_in, n_hidden, n_out ):\n",
    "    layers = []\n",
    "    \n",
    "    # Input layer to first hidden layer\n",
    "    layers.append(nn.Linear(n_in, n_hidden[0]))\n",
    "    layers.append(nn.ReLU())\n",
    "    \n",
    "    # Adding subsequent hidden layers\n",
    "    for i in range(1, len(n_hidden)):\n",
    "        layers.append(nn.Linear(n_hidden[i-1], n_hidden[i]))\n",
    "        layers.append(nn.ReLU())\n",
    "    \n",
    "    # Output layer\n",
    "    layers.append(nn.Linear(n_hidden[-1], n_out))\n",
    "    \n",
    "    # Create the sequential model\n",
    "    model = nn.Sequential(*layers)\n",
    "    return model\n",
    "\n",
    "model = create_model(X.shape[1], [50, 25], Y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=106, out_features=50, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=50, out_features=25, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=25, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric( y, y_hat ):\n",
    "    y     = torch.argmax(y, dim=1).to(torch.float32)\n",
    "    y_hat = torch.argmax(y_hat, dim=1).to(torch.float32)\n",
    "    difference = y_hat-y\n",
    "    return 1.0 - torch.mean( torch.abs( difference ) ).item() \n",
    "\n",
    "def train_one_epoch( epoch_index, indices_train, X, Y, optimizer, loss_fn, batch_size, writer ):\n",
    "    no_iterations = int( len(indices_train) / batch_size )\n",
    "    losses           = []\n",
    "    accuracies       = []\n",
    "    \n",
    "    for batch_no in range( no_iterations ):\n",
    "        x_batch = create_batch( indices_train, batch_no, batch_size, X )\n",
    "        y_batch = create_batch( indices_train, batch_no, batch_size, Y )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model( x_batch )\n",
    "        loss = loss_fn( y_hat, torch.argmax(y_batch, dim=1) ) # CrossEntropyLoss expects the class index, not the one-hot encoding\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        accuracy = accuracy_metric( y_batch, y_hat )\n",
    "        losses.append( loss.item() )\n",
    "        accuracies.append( accuracy )\n",
    "\n",
    "        writer.add_scalar(\"Loss/train\", np.mean(losses), epoch_index)\n",
    "        writer.add_scalar(\"Accuracy/train\", np.mean(accuracies), epoch_index)\n",
    "\n",
    "    return np.mean(losses), np.mean(accuracies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 / 50 accuracy 0.8025886194029851\n",
      "epoch 1 / 50 accuracy 0.9203591417910447\n",
      "epoch 2 / 50 accuracy 0.9378498134328358\n",
      "epoch 3 / 50 accuracy 0.9457789179104478\n",
      "epoch 4 / 50 accuracy 0.9505597014925373\n",
      "epoch 5 / 50 accuracy 0.9542910447761194\n",
      "epoch 6 / 50 accuracy 0.9576725746268657\n",
      "epoch 7 / 50 accuracy 0.9622201492537313\n",
      "epoch 8 / 50 accuracy 0.9645522388059702\n",
      "epoch 9 / 50 accuracy 0.9673507462686567\n",
      "epoch 10 / 50 accuracy 0.9692164179104478\n",
      "epoch 11 / 50 accuracy 0.9718983208955224\n",
      "epoch 12 / 50 accuracy 0.972714552238806\n",
      "epoch 13 / 50 accuracy 0.9734141791044776\n",
      "epoch 14 / 50 accuracy 0.9751632462686567\n",
      "epoch 15 / 50 accuracy 0.9772621268656716\n",
      "epoch 16 / 50 accuracy 0.9783115671641791\n",
      "epoch 17 / 50 accuracy 0.9777285447761194\n",
      "epoch 18 / 50 accuracy 0.9807602611940298\n",
      "epoch 19 / 50 accuracy 0.9825093283582089\n",
      "epoch 20 / 50 accuracy 0.9822761194029851\n",
      "epoch 21 / 50 accuracy 0.9832089552238806\n",
      "epoch 22 / 50 accuracy 0.9848414179104478\n",
      "epoch 23 / 50 accuracy 0.9850746268656716\n",
      "epoch 24 / 50 accuracy 0.9849580223880597\n",
      "epoch 25 / 50 accuracy 0.9867070895522388\n",
      "epoch 26 / 50 accuracy 0.9862406716417911\n",
      "epoch 27 / 50 accuracy 0.9867070895522388\n",
      "epoch 28 / 50 accuracy 0.9871735074626866\n",
      "epoch 29 / 50 accuracy 0.988222947761194\n",
      "epoch 30 / 50 accuracy 0.9895055970149254\n",
      "epoch 31 / 50 accuracy 0.9888059701492538\n",
      "epoch 32 / 50 accuracy 0.9898554104477612\n",
      "epoch 33 / 50 accuracy 0.9906716417910447\n",
      "epoch 34 / 50 accuracy 0.9913712686567164\n",
      "epoch 35 / 50 accuracy 0.9926539179104478\n",
      "epoch 36 / 50 accuracy 0.988339552238806\n",
      "epoch 37 / 50 accuracy 0.9857742537313433\n",
      "epoch 38 / 50 accuracy 0.988222947761194\n",
      "epoch 39 / 50 accuracy 0.9924207089552238\n",
      "epoch 40 / 50 accuracy 0.9888059701492538\n",
      "epoch 41 / 50 accuracy 0.9910214552238806\n",
      "epoch 42 / 50 accuracy 0.9910214552238806\n",
      "epoch 43 / 50 accuracy 0.9914878731343284\n",
      "epoch 44 / 50 accuracy 0.9940531716417911\n",
      "epoch 45 / 50 accuracy 0.9947527985074627\n",
      "epoch 46 / 50 accuracy 0.9942863805970149\n",
      "epoch 47 / 50 accuracy 0.9850746268656716\n",
      "epoch 48 / 50 accuracy 0.9903218283582089\n",
      "epoch 49 / 50 accuracy 0.9925373134328358\n"
     ]
    }
   ],
   "source": [
    "model = create_model(X.shape[1], [50, 25], Y.shape[1])\n",
    "optimizer = torch.optim.Adam( params = model.parameters(), lr=0.001 )\n",
    "loss_fn   = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "losses           = []\n",
    "accuracies       = []\n",
    "n_epochs = 50\n",
    "for i in range (n_epochs):\n",
    "    metrics = train_one_epoch( i, indices_train, X, Y, optimizer, loss_fn, 32, writer )\n",
    "    loss_per_epoch, accuracy_per_epoch = metrics\n",
    "    losses.append(loss_per_epoch)\n",
    "    accuracies.append(accuracy_per_epoch)\n",
    "    print(\"epoch\",i,\"/\",n_epochs,\"accuracy\",accuracy_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axis = plt.subplots( 1,1, figsize=(16,8) )\n",
    "axis.plot( losses )\n",
    "axis.plot( accuracies )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we experimentally determine optimal training parameters. You will need to organize your source\n",
    "code to allow for the following experiments. Consider that a network is initialized with random values\n",
    "only when the model is created. If training is interrupted and continued, weights and biases are preserved.\n",
    "1. Find an optimal learning rate. Argue which rates you tested and how you determined an optimal\n",
    "value.\n",
    "2. Find an optimal learning batch size. Argue which batch sizes you tested and how you determined\n",
    "an optimal value.\n",
    "3. Find an optimal network architecture (number of layers and nuber of features for each layer). Argue\n",
    "which architectures you tested and how you determined an optimal setup.\n",
    "I needed the following time to complete the task:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch( epoch_index, indices_val, X, Y, model, loss_fn, batch_size, writer ):\n",
    "    no_val_batches = int( len(indices_val) / batch_size )\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for batch_no in range( no_val_batches ):\n",
    "        x = create_batch( indices_val, batch_no, batch_size, X )\n",
    "        y = create_batch( indices_val, batch_no, batch_size, Y )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred = model( x )\n",
    "            loss = loss_fn( y_pred, torch.argmax(y, dim=1) )\n",
    "            accuracy = accuracy_metric( y, y_pred )\n",
    "            losses.append( loss.item() )\n",
    "            accuracies.append( accuracy )\n",
    "\n",
    "        writer.add_scalar(\"Loss/val\", np.mean(losses), epoch_index)\n",
    "        writer.add_scalar(\"Accuracy/val\", np.mean(accuracies), epoch_index)\n",
    "    \n",
    "    return np.mean( losses ), np.mean( accuracies )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 / 50 loss_train: 0.9858759661663824 acc_train: 0.48903917910447764 loss_val: 0.9614936905120736 acc_val: 0.5013992537313433\n",
      "epoch 1 / 50 loss_train: 0.9554073583723893 acc_train: 0.49486940298507465 loss_val: 0.9617459996422725 acc_val: 0.5013992537313433\n",
      "epoch 2 / 50 loss_train: 0.9555306078782723 acc_train: 0.494169776119403 loss_val: 0.9618418697100967 acc_val: 0.5013992537313433\n",
      "epoch 3 / 50 loss_train: 0.9555803470647157 acc_train: 0.49486940298507465 loss_val: 0.9618892367206403 acc_val: 0.5013992537313433\n",
      "epoch 4 / 50 loss_train: 0.9556065315186087 acc_train: 0.49486940298507465 loss_val: 0.961916565005459 acc_val: 0.5013992537313433\n",
      "epoch 5 / 50 loss_train: 0.9556221719553222 acc_train: 0.49486940298507465 loss_val: 0.9619334117690129 acc_val: 0.5013992537313433\n",
      "epoch 6 / 50 loss_train: 0.9556322206756962 acc_train: 0.49486940298507465 loss_val: 0.9619446121045013 acc_val: 0.5013992537313433\n",
      "epoch 7 / 50 loss_train: 0.9556390169396329 acc_train: 0.49486940298507465 loss_val: 0.9619522717461657 acc_val: 0.5013992537313433\n",
      "epoch 8 / 50 loss_train: 0.9556437439438122 acc_train: 0.49486940298507465 loss_val: 0.9619576681905718 acc_val: 0.5013992537313433\n",
      "epoch 9 / 50 loss_train: 0.9556470955930539 acc_train: 0.49486940298507465 loss_val: 0.9619616439093405 acc_val: 0.5013992537313433\n",
      "epoch 10 / 50 loss_train: 0.9556495324889226 acc_train: 0.49486940298507465 loss_val: 0.9619644399899155 acc_val: 0.5013992537313433\n",
      "epoch 11 / 50 loss_train: 0.9556512981653214 acc_train: 0.49486940298507465 loss_val: 0.9619665350486983 acc_val: 0.5013992537313433\n",
      "epoch 12 / 50 loss_train: 0.9556526081330741 acc_train: 0.49486940298507465 loss_val: 0.9619680874383272 acc_val: 0.5013992537313433\n",
      "epoch 13 / 50 loss_train: 0.9556535891632536 acc_train: 0.49486940298507465 loss_val: 0.9619690393334004 acc_val: 0.5013992537313433\n",
      "epoch 14 / 50 loss_train: 0.9556542975244238 acc_train: 0.49486940298507465 loss_val: 0.9619699316238289 acc_val: 0.5013992537313433\n",
      "epoch 15 / 50 loss_train: 0.955654837524713 acc_train: 0.49486940298507465 loss_val: 0.9619704653967672 acc_val: 0.5013992537313433\n",
      "epoch 16 / 50 loss_train: 0.955655244304173 acc_train: 0.49486940298507465 loss_val: 0.9619709760395448 acc_val: 0.5013992537313433\n",
      "epoch 17 / 50 loss_train: 0.9556555603422335 acc_train: 0.49486940298507465 loss_val: 0.961971242926014 acc_val: 0.5013992537313433\n",
      "epoch 18 / 50 loss_train: 0.955655786973327 acc_train: 0.49486940298507465 loss_val: 0.9619715089228615 acc_val: 0.5013992537313433\n",
      "epoch 19 / 50 loss_train: 0.9556559602271265 acc_train: 0.49486940298507465 loss_val: 0.9619717162046859 acc_val: 0.5013992537313433\n",
      "epoch 20 / 50 loss_train: 0.9556560896670641 acc_train: 0.49486940298507465 loss_val: 0.9619720711636899 acc_val: 0.5013992537313433\n",
      "epoch 21 / 50 loss_train: 0.9556561959768409 acc_train: 0.49486940298507465 loss_val: 0.9619719679675885 acc_val: 0.5013992537313433\n",
      "epoch 22 / 50 loss_train: 0.9556562629208636 acc_train: 0.49486940298507465 loss_val: 0.9619720524816371 acc_val: 0.5013992537313433\n",
      "epoch 23 / 50 loss_train: 0.9556563127396712 acc_train: 0.49486940298507465 loss_val: 0.9619722535361105 acc_val: 0.5013992537313433\n",
      "epoch 24 / 50 loss_train: 0.955656366116965 acc_train: 0.49486940298507465 loss_val: 0.9619722019380598 acc_val: 0.5013992537313433\n",
      "epoch 25 / 50 loss_train: 0.9556564030362599 acc_train: 0.49486940298507465 loss_val: 0.9619722268474635 acc_val: 0.5013992537313433\n",
      "epoch 26 / 50 loss_train: 0.9556564221631235 acc_train: 0.49486940298507465 loss_val: 0.9619722873417299 acc_val: 0.5013992537313433\n",
      "epoch 27 / 50 loss_train: 0.955656430392123 acc_train: 0.49486940298507465 loss_val: 0.9619723478359963 acc_val: 0.5013992537313433\n",
      "epoch 28 / 50 loss_train: 0.9556564535222837 acc_train: 0.49486940298507465 loss_val: 0.9619723478359963 acc_val: 0.5013992537313433\n",
      "epoch 29 / 50 loss_train: 0.9556564706474987 acc_train: 0.49486940298507465 loss_val: 0.9619723914274528 acc_val: 0.5013992537313433\n",
      "epoch 30 / 50 loss_train: 0.9556564708699041 acc_train: 0.49486940298507465 loss_val: 0.9619724172264782 acc_val: 0.5013992537313433\n",
      "epoch 31 / 50 loss_train: 0.9556564784316874 acc_train: 0.49486940298507465 loss_val: 0.9619724465839898 acc_val: 0.5013992537313433\n",
      "epoch 32 / 50 loss_train: 0.9556564811005521 acc_train: 0.49486940298507465 loss_val: 0.9619723905378313 acc_val: 0.5013992537313433\n",
      "epoch 33 / 50 loss_train: 0.9556564842142276 acc_train: 0.49486940298507465 loss_val: 0.9619724056613979 acc_val: 0.5013992537313433\n",
      "epoch 34 / 50 loss_train: 0.9556564806557414 acc_train: 0.49486940298507465 loss_val: 0.9619723051341612 acc_val: 0.5013992537313433\n",
      "epoch 35 / 50 loss_train: 0.955656478209282 acc_train: 0.49486940298507465 loss_val: 0.9619723416086453 acc_val: 0.5013992537313433\n",
      "epoch 36 / 50 loss_train: 0.9556564797661198 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 37 / 50 loss_train: 0.9556564851038492 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 38 / 50 loss_train: 0.9556564833246061 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 39 / 50 loss_train: 0.9556564824349845 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 40 / 50 loss_train: 0.9556564795437144 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 41 / 50 loss_train: 0.9556564855486599 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 42 / 50 loss_train: 0.9556564795437144 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 43 / 50 loss_train: 0.9556564831022006 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 44 / 50 loss_train: 0.9556564871054977 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 45 / 50 loss_train: 0.9556564855486599 acc_train: 0.49486940298507465 loss_val: 0.9619723843104804 acc_val: 0.5013992537313433\n",
      "epoch 46 / 50 loss_train: 0.9556564837694168 acc_train: 0.49486940298507465 loss_val: 0.9619723860897235 acc_val: 0.5013992537313433\n",
      "epoch 47 / 50 loss_train: 0.9556564891071462 acc_train: 0.49486940298507465 loss_val: 0.9619724750518799 acc_val: 0.5013992537313433\n",
      "epoch 48 / 50 loss_train: 0.9556564926656325 acc_train: 0.49486940298507465 loss_val: 0.9619724723830152 acc_val: 0.5013992537313433\n",
      "epoch 49 / 50 loss_train: 0.9556564891071462 acc_train: 0.49486940298507465 loss_val: 0.9619724225642076 acc_val: 0.5013992537313433\n",
      "Validation accuracy for lr 0.1 bs 32 hl [30, 15] : 0.5013992537313433\n",
      "epoch 0 / 50 loss_train: 1.0168163182575312 acc_train: 0.48530783582089554 loss_val: 0.9614966823093927 acc_val: 0.5013992537313433\n",
      "epoch 1 / 50 loss_train: 0.9554149277174651 acc_train: 0.49486940298507465 loss_val: 0.9617518649172427 acc_val: 0.5013992537313433\n",
      "epoch 2 / 50 loss_train: 0.955534452822671 acc_train: 0.494169776119403 loss_val: 0.9618462003878693 acc_val: 0.5013992537313433\n",
      "epoch 3 / 50 loss_train: 0.9555826498501336 acc_train: 0.49486940298507465 loss_val: 0.9618923655196802 acc_val: 0.5013992537313433\n",
      "epoch 4 / 50 loss_train: 0.9556080074007831 acc_train: 0.49486940298507465 loss_val: 0.9619185960114892 acc_val: 0.5013992537313433\n",
      "epoch 5 / 50 loss_train: 0.9556231683314736 acc_train: 0.49486940298507465 loss_val: 0.9619350006331259 acc_val: 0.5013992537313433\n",
      "epoch 6 / 50 loss_train: 0.9556329421587845 acc_train: 0.49486940298507465 loss_val: 0.9619458655812847 acc_val: 0.5013992537313433\n",
      "epoch 7 / 50 loss_train: 0.9556395269151944 acc_train: 0.49486940298507465 loss_val: 0.9619531756016746 acc_val: 0.5013992537313433\n",
      "epoch 8 / 50 loss_train: 0.9556440971235731 acc_train: 0.49486940298507465 loss_val: 0.9619583807774444 acc_val: 0.5013992537313433\n",
      "epoch 9 / 50 loss_train: 0.9556473560297667 acc_train: 0.49486940298507465 loss_val: 0.9619620175503972 acc_val: 0.5013992537313433\n",
      "epoch 10 / 50 loss_train: 0.9556497306521259 acc_train: 0.49486940298507465 loss_val: 0.9619646828566024 acc_val: 0.5013992537313433\n",
      "epoch 11 / 50 loss_train: 0.9556514534042843 acc_train: 0.49486940298507465 loss_val: 0.961966725427713 acc_val: 0.5013992537313433\n",
      "epoch 12 / 50 loss_train: 0.9556527255631205 acc_train: 0.49486940298507465 loss_val: 0.9619681514910797 acc_val: 0.5013992537313433\n",
      "epoch 13 / 50 loss_train: 0.9556536567744924 acc_train: 0.49486940298507465 loss_val: 0.9619693471424615 acc_val: 0.5013992537313433\n",
      "epoch 14 / 50 loss_train: 0.9556543635788248 acc_train: 0.49486940298507465 loss_val: 0.9619700944245752 acc_val: 0.5013992537313433\n",
      "epoch 15 / 50 loss_train: 0.9556548817833858 acc_train: 0.49486940298507465 loss_val: 0.9619705650343824 acc_val: 0.5013992537313433\n",
      "epoch 16 / 50 loss_train: 0.9556552836699272 acc_train: 0.49486940298507465 loss_val: 0.9619710783460247 acc_val: 0.5013992537313433\n",
      "epoch 17 / 50 loss_train: 0.9556555852516374 acc_train: 0.49486940298507465 loss_val: 0.9619714911304303 acc_val: 0.5013992537313433\n",
      "epoch 18 / 50 loss_train: 0.9556558174428655 acc_train: 0.49486940298507465 loss_val: 0.9619715373907516 acc_val: 0.5013992537313433\n",
      "epoch 19 / 50 loss_train: 0.9556559693457475 acc_train: 0.49486940298507465 loss_val: 0.961971877226189 acc_val: 0.5013992537313433\n",
      "epoch 20 / 50 loss_train: 0.9556561001201174 acc_train: 0.49486940298507465 loss_val: 0.9619718834535399 acc_val: 0.5013992537313433\n",
      "epoch 21 / 50 loss_train: 0.9556562068747051 acc_train: 0.49486940298507465 loss_val: 0.9619721699116836 acc_val: 0.5013992537313433\n",
      "epoch 22 / 50 loss_train: 0.9556562604744043 acc_train: 0.49486940298507465 loss_val: 0.9619722348540577 acc_val: 0.5013992537313433\n",
      "epoch 23 / 50 loss_train: 0.9556563311993186 acc_train: 0.49486940298507465 loss_val: 0.9619722971275672 acc_val: 0.5013992537313433\n",
      "epoch 24 / 50 loss_train: 0.9556563710098835 acc_train: 0.49486940298507465 loss_val: 0.9619721796975207 acc_val: 0.5013992537313433\n",
      "epoch 25 / 50 loss_train: 0.9556564088188001 acc_train: 0.49486940298507465 loss_val: 0.9619723682972923 acc_val: 0.5013992537313433\n",
      "epoch 26 / 50 loss_train: 0.9556564290576907 acc_train: 0.49486940298507465 loss_val: 0.9619724706037721 acc_val: 0.5013992537313433\n",
      "epoch 27 / 50 loss_train: 0.955656426388826 acc_train: 0.49486940298507465 loss_val: 0.9619723460567531 acc_val: 0.5013992537313433\n",
      "epoch 28 / 50 loss_train: 0.9556564570807698 acc_train: 0.49486940298507465 loss_val: 0.9619724101095057 acc_val: 0.5013992537313433\n",
      "epoch 29 / 50 loss_train: 0.9556564697578772 acc_train: 0.49486940298507465 loss_val: 0.9619723763038863 acc_val: 0.5013992537313433\n",
      "epoch 30 / 50 loss_train: 0.9556564717595257 acc_train: 0.49486940298507465 loss_val: 0.9619725533385775 acc_val: 0.5013992537313433\n",
      "epoch 31 / 50 loss_train: 0.9556564828797952 acc_train: 0.49486940298507465 loss_val: 0.9619724465839898 acc_val: 0.5013992537313433\n",
      "epoch 32 / 50 loss_train: 0.9556564837694168 acc_train: 0.49486940298507465 loss_val: 0.9619724510320976 acc_val: 0.5013992537313433\n",
      "epoch 33 / 50 loss_train: 0.955656474205985 acc_train: 0.49486940298507465 loss_val: 0.9619724892858249 acc_val: 0.5013992537313433\n",
      "epoch 34 / 50 loss_train: 0.9556564875503084 acc_train: 0.49486940298507465 loss_val: 0.9619723914274528 acc_val: 0.5013992537313433\n",
      "epoch 35 / 50 loss_train: 0.9556564750956066 acc_train: 0.49486940298507465 loss_val: 0.9619723914274528 acc_val: 0.5013992537313433\n",
      "epoch 36 / 50 loss_train: 0.9556564864382815 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 37 / 50 loss_train: 0.9556564846590384 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 38 / 50 loss_train: 0.9556564811005521 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 39 / 50 loss_train: 0.9556564828797952 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 40 / 50 loss_train: 0.9556564842142276 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 41 / 50 loss_train: 0.9556564824349845 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 42 / 50 loss_train: 0.955656479321309 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 43 / 50 loss_train: 0.9556564837694168 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 44 / 50 loss_train: 0.9556564857710653 acc_train: 0.49486940298507465 loss_val: 0.9619724723830152 acc_val: 0.5013992537313433\n",
      "epoch 45 / 50 loss_train: 0.9556564853262546 acc_train: 0.49486940298507465 loss_val: 0.9619724723830152 acc_val: 0.5013992537313433\n",
      "epoch 46 / 50 loss_train: 0.9556564895519569 acc_train: 0.49486940298507465 loss_val: 0.9619724750518799 acc_val: 0.5013992537313433\n",
      "epoch 47 / 50 loss_train: 0.9556564888847408 acc_train: 0.49486940298507465 loss_val: 0.9619723834208588 acc_val: 0.5013992537313433\n",
      "epoch 48 / 50 loss_train: 0.9556564899967678 acc_train: 0.49486940298507465 loss_val: 0.9619724216745861 acc_val: 0.5013992537313433\n",
      "epoch 49 / 50 loss_train: 0.9556564926656325 acc_train: 0.49486940298507465 loss_val: 0.9619724750518799 acc_val: 0.5013992537313433\n",
      "Validation accuracy for lr 0.1 bs 32 hl [50, 25] : 0.5013992537313433\n",
      "epoch 0 / 50 loss_train: 0.8329705411373679 acc_train: 0.6958955223880597 loss_val: 0.545772901945471 acc_val: 0.7635261194029851\n",
      "epoch 1 / 50 loss_train: 0.7854066352123645 acc_train: 0.6518190298507462 loss_val: 0.8108024895191193 acc_val: 0.679570895522388\n",
      "epoch 2 / 50 loss_train: 0.9304504921632026 acc_train: 0.5670475746268657 loss_val: 0.962011798993865 acc_val: 0.5013992537313433\n",
      "epoch 3 / 50 loss_train: 0.955588333197494 acc_train: 0.49486940298507465 loss_val: 0.9619940812908002 acc_val: 0.5013992537313433\n",
      "epoch 4 / 50 loss_train: 0.9556098533655281 acc_train: 0.49486940298507465 loss_val: 0.9619870337087717 acc_val: 0.5013992537313433\n",
      "epoch 5 / 50 loss_train: 0.9556236300450652 acc_train: 0.49486940298507465 loss_val: 0.9619827190441872 acc_val: 0.5013992537313433\n",
      "epoch 6 / 50 loss_train: 0.9556328322905213 acc_train: 0.49486940298507465 loss_val: 0.9619797717279462 acc_val: 0.5013992537313433\n",
      "epoch 7 / 50 loss_train: 0.9556392073186476 acc_train: 0.49486940298507465 loss_val: 0.9619777932095883 acc_val: 0.5013992537313433\n",
      "epoch 8 / 50 loss_train: 0.9556437299322726 acc_train: 0.49486940298507465 loss_val: 0.9619763715943294 acc_val: 0.5013992537313433\n",
      "epoch 9 / 50 loss_train: 0.9556469935089794 acc_train: 0.49486940298507465 loss_val: 0.9619753387436938 acc_val: 0.5013992537313433\n",
      "epoch 10 / 50 loss_train: 0.9556493850341484 acc_train: 0.49486940298507465 loss_val: 0.9619745363050433 acc_val: 0.5013992537313433\n",
      "epoch 11 / 50 loss_train: 0.9556511604963843 acc_train: 0.49486940298507465 loss_val: 0.9619741092866926 acc_val: 0.5013992537313433\n",
      "epoch 12 / 50 loss_train: 0.955652470464137 acc_train: 0.49486940298507465 loss_val: 0.9619735559420799 acc_val: 0.5013992537313433\n",
      "epoch 13 / 50 loss_train: 0.9556534606129375 acc_train: 0.49486940298507465 loss_val: 0.961973431395061 acc_val: 0.5013992537313433\n",
      "epoch 14 / 50 loss_train: 0.955654198109214 acc_train: 0.49486940298507465 loss_val: 0.9619731395991881 acc_val: 0.5013992537313433\n",
      "epoch 15 / 50 loss_train: 0.9556547574587723 acc_train: 0.49486940298507465 loss_val: 0.9619730497474102 acc_val: 0.5013992537313433\n",
      "epoch 16 / 50 loss_train: 0.9556551626813945 acc_train: 0.49486940298507465 loss_val: 0.9619727810816978 acc_val: 0.5013992537313433\n",
      "epoch 17 / 50 loss_train: 0.9556554985135349 acc_train: 0.49486940298507465 loss_val: 0.9619727588411587 acc_val: 0.5013992537313433\n",
      "epoch 18 / 50 loss_train: 0.9556557353752763 acc_train: 0.49486940298507465 loss_val: 0.9619726076054929 acc_val: 0.5013992537313433\n",
      "epoch 19 / 50 loss_train: 0.9556559239750477 acc_train: 0.49486940298507465 loss_val: 0.9619725987092772 acc_val: 0.5013992537313433\n",
      "epoch 20 / 50 loss_train: 0.9556560640904441 acc_train: 0.49486940298507465 loss_val: 0.9619726218394379 acc_val: 0.5013992537313433\n",
      "epoch 21 / 50 loss_train: 0.9556561588351407 acc_train: 0.49486940298507465 loss_val: 0.9619725133056072 acc_val: 0.5013992537313433\n",
      "epoch 22 / 50 loss_train: 0.9556562409027299 acc_train: 0.49486940298507465 loss_val: 0.9619724955131759 acc_val: 0.5013992537313433\n",
      "epoch 23 / 50 loss_train: 0.9556563156309412 acc_train: 0.49486940298507465 loss_val: 0.9619725293187953 acc_val: 0.5013992537313433\n",
      "epoch 24 / 50 loss_train: 0.955656345655669 acc_train: 0.49486940298507465 loss_val: 0.9619725008509052 acc_val: 0.5013992537313433\n",
      "epoch 25 / 50 loss_train: 0.9556563856886394 acc_train: 0.49486940298507465 loss_val: 0.9619723576218334 acc_val: 0.5013992537313433\n",
      "epoch 26 / 50 loss_train: 0.9556564199390696 acc_train: 0.49486940298507465 loss_val: 0.9619723682972923 acc_val: 0.5013992537313433\n",
      "epoch 27 / 50 loss_train: 0.9556564335057984 acc_train: 0.49486940298507465 loss_val: 0.9619724919546896 acc_val: 0.5013992537313433\n",
      "epoch 28 / 50 loss_train: 0.9556564444036626 acc_train: 0.49486940298507465 loss_val: 0.9619724732726368 acc_val: 0.5013992537313433\n",
      "epoch 29 / 50 loss_train: 0.9556564621960939 acc_train: 0.49486940298507465 loss_val: 0.9619725097471209 acc_val: 0.5013992537313433\n",
      "epoch 30 / 50 loss_train: 0.9556564777644713 acc_train: 0.49486940298507465 loss_val: 0.9619725026301483 acc_val: 0.5013992537313433\n",
      "epoch 31 / 50 loss_train: 0.9556564786540929 acc_train: 0.49486940298507465 loss_val: 0.9619725310980384 acc_val: 0.5013992537313433\n",
      "epoch 32 / 50 loss_train: 0.9556564790989036 acc_train: 0.49486940298507465 loss_val: 0.9619725328772816 acc_val: 0.5013992537313433\n",
      "epoch 33 / 50 loss_train: 0.9556564750956066 acc_train: 0.49486940298507465 loss_val: 0.9619724216745861 acc_val: 0.5013992537313433\n",
      "epoch 34 / 50 loss_train: 0.955656479321309 acc_train: 0.49486940298507465 loss_val: 0.9619724216745861 acc_val: 0.5013992537313433\n",
      "epoch 35 / 50 loss_train: 0.9556564864382815 acc_train: 0.49486940298507465 loss_val: 0.9619724750518799 acc_val: 0.5013992537313433\n",
      "epoch 36 / 50 loss_train: 0.955656484436633 acc_train: 0.49486940298507465 loss_val: 0.9619724750518799 acc_val: 0.5013992537313433\n",
      "epoch 37 / 50 loss_train: 0.9556564824349845 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 38 / 50 loss_train: 0.9556564837694168 acc_train: 0.49486940298507465 loss_val: 0.9619723843104804 acc_val: 0.5013992537313433\n",
      "epoch 39 / 50 loss_train: 0.9556564871054977 acc_train: 0.49486940298507465 loss_val: 0.9619723843104804 acc_val: 0.5013992537313433\n",
      "epoch 40 / 50 loss_train: 0.9556564835470114 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 41 / 50 loss_train: 0.9556564875503084 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 42 / 50 loss_train: 0.9556564862158761 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 43 / 50 loss_train: 0.9556564857710653 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 44 / 50 loss_train: 0.9556564879951193 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 45 / 50 loss_train: 0.9556564866606869 acc_train: 0.49486940298507465 loss_val: 0.9619724723830152 acc_val: 0.5013992537313433\n",
      "epoch 46 / 50 loss_train: 0.9556564871054977 acc_train: 0.49486940298507465 loss_val: 0.9619724359085311 acc_val: 0.5013992537313433\n",
      "epoch 47 / 50 loss_train: 0.9556564875503084 acc_train: 0.49486940298507465 loss_val: 0.9619724750518799 acc_val: 0.5013992537313433\n",
      "epoch 48 / 50 loss_train: 0.955656490663984 acc_train: 0.49486940298507465 loss_val: 0.9619724750518799 acc_val: 0.5013992537313433\n",
      "epoch 49 / 50 loss_train: 0.9556564931104432 acc_train: 0.49486940298507465 loss_val: 0.9619724216745861 acc_val: 0.5013992537313433\n",
      "Validation accuracy for lr 0.1 bs 32 hl [100, 50, 25] : 0.5013992537313433\n",
      "epoch 0 / 50 loss_train: 1.0265724556659586 acc_train: 0.49055503731343286 loss_val: 0.9764126846284578 acc_val: 0.49384469696969696\n",
      "epoch 1 / 50 loss_train: 0.9565377893732555 acc_train: 0.5051305970149254 loss_val: 0.9743409951527914 acc_val: 0.49384469696969696\n",
      "epoch 2 / 50 loss_train: 0.957055716372248 acc_train: 0.5013992537313433 loss_val: 0.973061184088389 acc_val: 0.49384469696969696\n",
      "epoch 3 / 50 loss_train: 0.9572909972560939 acc_train: 0.496035447761194 loss_val: 0.9723897612456119 acc_val: 0.49384469696969696\n",
      "epoch 4 / 50 loss_train: 0.9574141635823605 acc_train: 0.4918376865671642 loss_val: 0.9719908797379696 acc_val: 0.49384469696969696\n",
      "epoch 5 / 50 loss_train: 0.9574876329792079 acc_train: 0.4918376865671642 loss_val: 0.9717314044634501 acc_val: 0.49384469696969696\n",
      "epoch 6 / 50 loss_train: 0.9575355515551212 acc_train: 0.4918376865671642 loss_val: 0.9715519493276422 acc_val: 0.49384469696969696\n",
      "epoch 7 / 50 loss_train: 0.9575687713587462 acc_train: 0.4918376865671642 loss_val: 0.9714222225275907 acc_val: 0.49384469696969696\n",
      "epoch 8 / 50 loss_train: 0.9575928480767492 acc_train: 0.4904384328358209 loss_val: 0.971324749065168 acc_val: 0.49384469696969696\n",
      "epoch 9 / 50 loss_train: 0.9576108918261172 acc_train: 0.4881063432835821 loss_val: 0.9712501366933187 acc_val: 0.49384469696969696\n",
      "epoch 10 / 50 loss_train: 0.9576247828220253 acc_train: 0.4855410447761194 loss_val: 0.9711916121569547 acc_val: 0.49384469696969696\n",
      "epoch 11 / 50 loss_train: 0.9576356299777529 acc_train: 0.4855410447761194 loss_val: 0.9711450193867539 acc_val: 0.49384469696969696\n",
      "epoch 12 / 50 loss_train: 0.957644306012054 acc_train: 0.4855410447761194 loss_val: 0.9711074323365183 acc_val: 0.49384469696969696\n",
      "epoch 13 / 50 loss_train: 0.9576512735281417 acc_train: 0.4855410447761194 loss_val: 0.9710766420219884 acc_val: 0.49384469696969696\n",
      "epoch 14 / 50 loss_train: 0.9576569764471766 acc_train: 0.4855410447761194 loss_val: 0.9710515358231284 acc_val: 0.49384469696969696\n",
      "epoch 15 / 50 loss_train: 0.9576616638631963 acc_train: 0.4855410447761194 loss_val: 0.9710307247710951 acc_val: 0.49384469696969696\n",
      "epoch 16 / 50 loss_train: 0.9576655483957547 acc_train: 0.4855410447761194 loss_val: 0.9710132407419609 acc_val: 0.49384469696969696\n",
      "epoch 17 / 50 loss_train: 0.9576687790564636 acc_train: 0.4855410447761194 loss_val: 0.9709987911311063 acc_val: 0.49384469696969696\n",
      "epoch 18 / 50 loss_train: 0.9576714964055303 acc_train: 0.4855410447761194 loss_val: 0.9709866064967532 acc_val: 0.49384469696969696\n",
      "epoch 19 / 50 loss_train: 0.9576737604924103 acc_train: 0.4855410447761194 loss_val: 0.9709761558157025 acc_val: 0.49384469696969696\n",
      "epoch 20 / 50 loss_train: 0.9576756918608252 acc_train: 0.4855410447761194 loss_val: 0.9709675546848413 acc_val: 0.49384469696969696\n",
      "epoch 21 / 50 loss_train: 0.9576773421088262 acc_train: 0.4855410447761194 loss_val: 0.9709599722515453 acc_val: 0.49384469696969696\n",
      "epoch 22 / 50 loss_train: 0.9576787103467913 acc_train: 0.4855410447761194 loss_val: 0.9709537209886493 acc_val: 0.49384469696969696\n",
      "epoch 23 / 50 loss_train: 0.9576799091118485 acc_train: 0.4855410447761194 loss_val: 0.97094823736133 acc_val: 0.49384469696969696\n",
      "epoch 24 / 50 loss_train: 0.957680895257352 acc_train: 0.4855410447761194 loss_val: 0.9709437435323541 acc_val: 0.49384469696969696\n",
      "epoch 25 / 50 loss_train: 0.957681755966215 acc_train: 0.4855410447761194 loss_val: 0.9709396615172877 acc_val: 0.49384469696969696\n",
      "epoch 26 / 50 loss_train: 0.9576825005794639 acc_train: 0.4855410447761194 loss_val: 0.9709364627346848 acc_val: 0.49384469696969696\n",
      "epoch 27 / 50 loss_train: 0.9576831415518007 acc_train: 0.4855410447761194 loss_val: 0.9709333578745524 acc_val: 0.49384469696969696\n",
      "epoch 28 / 50 loss_train: 0.957683683331333 acc_train: 0.4855410447761194 loss_val: 0.9709308924097003 acc_val: 0.49384469696969696\n",
      "epoch 29 / 50 loss_train: 0.9576841534963295 acc_train: 0.4855410447761194 loss_val: 0.9709288351463549 acc_val: 0.49384469696969696\n",
      "epoch 30 / 50 loss_train: 0.9576845462642499 acc_train: 0.4855410447761194 loss_val: 0.9709267670458014 acc_val: 0.49384469696969696\n",
      "epoch 31 / 50 loss_train: 0.9576848852100657 acc_train: 0.4855410447761194 loss_val: 0.9709254521312136 acc_val: 0.49384469696969696\n",
      "epoch 32 / 50 loss_train: 0.9576852041393963 acc_train: 0.4855410447761194 loss_val: 0.9709239276972684 acc_val: 0.49384469696969696\n",
      "epoch 33 / 50 loss_train: 0.957685443447597 acc_train: 0.4855410447761194 loss_val: 0.9709227012865471 acc_val: 0.49384469696969696\n",
      "epoch 34 / 50 loss_train: 0.9576856725251497 acc_train: 0.4855410447761194 loss_val: 0.9709217566432375 acc_val: 0.49384469696969696\n",
      "epoch 35 / 50 loss_train: 0.9576858491150301 acc_train: 0.4855410447761194 loss_val: 0.970920878829378 acc_val: 0.49384469696969696\n",
      "epoch 36 / 50 loss_train: 0.9576860314874507 acc_train: 0.4855410447761194 loss_val: 0.9709200371395458 acc_val: 0.49384469696969696\n",
      "epoch 37 / 50 loss_train: 0.9576861671547392 acc_train: 0.4855410447761194 loss_val: 0.9709193869070574 acc_val: 0.49384469696969696\n",
      "epoch 38 / 50 loss_train: 0.9576862899225149 acc_train: 0.4855410447761194 loss_val: 0.9709189443877249 acc_val: 0.49384469696969696\n",
      "epoch 39 / 50 loss_train: 0.9576863975667241 acc_train: 0.4855410447761194 loss_val: 0.9709183627908881 acc_val: 0.49384469696969696\n",
      "epoch 40 / 50 loss_train: 0.957686485639259 acc_train: 0.4855410447761194 loss_val: 0.9709180503180532 acc_val: 0.49384469696969696\n",
      "epoch 41 / 50 loss_train: 0.9576865808287663 acc_train: 0.4855410447761194 loss_val: 0.9709176529537548 acc_val: 0.49384469696969696\n",
      "epoch 42 / 50 loss_train: 0.9576866350956817 acc_train: 0.4855410447761194 loss_val: 0.9709171761165966 acc_val: 0.49384469696969696\n",
      "epoch 43 / 50 loss_train: 0.9576867093790823 acc_train: 0.4855410447761194 loss_val: 0.9709170912251328 acc_val: 0.49384469696969696\n",
      "epoch 44 / 50 loss_train: 0.9576867556394036 acc_train: 0.4855410447761194 loss_val: 0.9709167841709021 acc_val: 0.49384469696969696\n",
      "epoch 45 / 50 loss_train: 0.957686795672374 acc_train: 0.4855410447761194 loss_val: 0.9709165529771284 acc_val: 0.49384469696969696\n",
      "epoch 46 / 50 loss_train: 0.9576868290331826 acc_train: 0.4855410447761194 loss_val: 0.9709164482174497 acc_val: 0.49384469696969696\n",
      "epoch 47 / 50 loss_train: 0.9576868726246393 acc_train: 0.4855410447761194 loss_val: 0.970916318170952 acc_val: 0.49384469696969696\n",
      "epoch 48 / 50 loss_train: 0.9576868801864226 acc_train: 0.4855410447761194 loss_val: 0.9709160111167214 acc_val: 0.49384469696969696\n",
      "epoch 49 / 50 loss_train: 0.9576869175505283 acc_train: 0.4855410447761194 loss_val: 0.9709160869771783 acc_val: 0.49384469696969696\n",
      "Validation accuracy for lr 0.1 bs 64 hl [30, 15] : 0.49384469696969696\n",
      "epoch 0 / 50 loss_train: 0.7010496599015905 acc_train: 0.6676772388059702 loss_val: 0.6857003699423689 acc_val: 0.7007575757575758\n",
      "epoch 1 / 50 loss_train: 0.6369473513827395 acc_train: 0.6659281716417911 loss_val: 0.5924192769169595 acc_val: 0.6931818181818182\n",
      "epoch 2 / 50 loss_train: 0.5637036057105705 acc_train: 0.6586986940298507 loss_val: 0.6999267676393638 acc_val: 0.6898674242424242\n",
      "epoch 3 / 50 loss_train: 0.7125804495455613 acc_train: 0.6961287313432836 loss_val: 0.7661952796307477 acc_val: 0.6770833333333334\n",
      "epoch 4 / 50 loss_train: 0.975352712531588 acc_train: 0.5225046641791045 loss_val: 0.9664410894567316 acc_val: 0.49384469696969696\n",
      "epoch 5 / 50 loss_train: 0.958230772569998 acc_train: 0.48111007462686567 loss_val: 0.9673392068256031 acc_val: 0.49384469696969696\n",
      "epoch 6 / 50 loss_train: 0.9581547165984539 acc_train: 0.47901119402985076 loss_val: 0.9680082725756096 acc_val: 0.49384469696969696\n",
      "epoch 7 / 50 loss_train: 0.9580747778735944 acc_train: 0.47901119402985076 loss_val: 0.9685451261924974 acc_val: 0.49384469696969696\n",
      "epoch 8 / 50 loss_train: 0.958006923323247 acc_train: 0.47947761194029853 loss_val: 0.9689745036038485 acc_val: 0.49384469696969696\n",
      "epoch 9 / 50 loss_train: 0.957951088894659 acc_train: 0.48134328358208955 loss_val: 0.9693175807143702 acc_val: 0.49384469696969696\n",
      "epoch 10 / 50 loss_train: 0.9579057115227428 acc_train: 0.48390858208955223 loss_val: 0.9695930589329113 acc_val: 0.49384469696969696\n",
      "epoch 11 / 50 loss_train: 0.957868903875351 acc_train: 0.4855410447761194 loss_val: 0.9698153318780841 acc_val: 0.49384469696969696\n",
      "epoch 12 / 50 loss_train: 0.9578389218494073 acc_train: 0.4855410447761194 loss_val: 0.9699959213083441 acc_val: 0.49384469696969696\n",
      "epoch 13 / 50 loss_train: 0.9578143958725146 acc_train: 0.4855410447761194 loss_val: 0.9701438835172942 acc_val: 0.49384469696969696\n",
      "epoch 14 / 50 loss_train: 0.9577942294860954 acc_train: 0.4855410447761194 loss_val: 0.9702657208298192 acc_val: 0.49384469696969696\n",
      "epoch 15 / 50 loss_train: 0.9577775308445319 acc_train: 0.4855410447761194 loss_val: 0.9703665177027384 acc_val: 0.49384469696969696\n",
      "epoch 16 / 50 loss_train: 0.9577636500792717 acc_train: 0.4855410447761194 loss_val: 0.9704505367712541 acc_val: 0.49384469696969696\n",
      "epoch 17 / 50 loss_train: 0.9577520716546187 acc_train: 0.4855410447761194 loss_val: 0.9705206860195507 acc_val: 0.49384469696969696\n",
      "epoch 18 / 50 loss_train: 0.9577423360810351 acc_train: 0.4855410447761194 loss_val: 0.9705797415791135 acc_val: 0.49384469696969696\n",
      "epoch 19 / 50 loss_train: 0.957734172023944 acc_train: 0.4855410447761194 loss_val: 0.9706293777986006 acc_val: 0.49384469696969696\n",
      "epoch 20 / 50 loss_train: 0.9577272685606089 acc_train: 0.4855410447761194 loss_val: 0.9706713484995293 acc_val: 0.49384469696969696\n",
      "epoch 21 / 50 loss_train: 0.9577214233021238 acc_train: 0.4855410447761194 loss_val: 0.9707068999608358 acc_val: 0.49384469696969696\n",
      "epoch 22 / 50 loss_train: 0.9577164641067163 acc_train: 0.4855410447761194 loss_val: 0.9707370526862867 acc_val: 0.49384469696969696\n",
      "epoch 23 / 50 loss_train: 0.9577122290632618 acc_train: 0.4855410447761194 loss_val: 0.9707627386757822 acc_val: 0.49384469696969696\n",
      "epoch 24 / 50 loss_train: 0.957708651450143 acc_train: 0.4855410447761194 loss_val: 0.9707845666191794 acc_val: 0.49384469696969696\n",
      "epoch 25 / 50 loss_train: 0.9577055978241251 acc_train: 0.4855410447761194 loss_val: 0.9708031542373426 acc_val: 0.49384469696969696\n",
      "epoch 26 / 50 loss_train: 0.9577029543136483 acc_train: 0.4855410447761194 loss_val: 0.9708191933053912 acc_val: 0.49384469696969696\n",
      "epoch 27 / 50 loss_train: 0.9577007213635231 acc_train: 0.4855410447761194 loss_val: 0.970832777745796 acc_val: 0.49384469696969696\n",
      "epoch 28 / 50 loss_train: 0.957698810456404 acc_train: 0.4855410447761194 loss_val: 0.9708444169073394 acc_val: 0.49384469696969696\n",
      "epoch 29 / 50 loss_train: 0.9576971562051061 acc_train: 0.4855410447761194 loss_val: 0.9708543618520101 acc_val: 0.49384469696969696\n",
      "epoch 30 / 50 loss_train: 0.9576957652817911 acc_train: 0.4855410447761194 loss_val: 0.970862865447998 acc_val: 0.49384469696969696\n",
      "epoch 31 / 50 loss_train: 0.9576945473898703 acc_train: 0.4855410447761194 loss_val: 0.9708701841758959 acc_val: 0.49384469696969696\n",
      "epoch 32 / 50 loss_train: 0.9576935100911269 acc_train: 0.4855410447761194 loss_val: 0.9708766937255859 acc_val: 0.49384469696969696\n",
      "epoch 33 / 50 loss_train: 0.9576926097941043 acc_train: 0.4855410447761194 loss_val: 0.970882029244394 acc_val: 0.49384469696969696\n",
      "epoch 34 / 50 loss_train: 0.9576918376025869 acc_train: 0.4855410447761194 loss_val: 0.9708867253679218 acc_val: 0.49384469696969696\n",
      "epoch 35 / 50 loss_train: 0.9576911690519817 acc_train: 0.4855410447761194 loss_val: 0.9708906430186648 acc_val: 0.49384469696969696\n",
      "epoch 36 / 50 loss_train: 0.9576905948012623 acc_train: 0.4855410447761194 loss_val: 0.9708941849795255 acc_val: 0.49384469696969696\n",
      "epoch 37 / 50 loss_train: 0.9576901121815639 acc_train: 0.4855410447761194 loss_val: 0.97089709296371 acc_val: 0.49384469696969696\n",
      "epoch 38 / 50 loss_train: 0.9576896971731044 acc_train: 0.4855410447761194 loss_val: 0.9708995891339851 acc_val: 0.49384469696969696\n",
      "epoch 39 / 50 loss_train: 0.9576893333178848 acc_train: 0.4855410447761194 loss_val: 0.9709019986065951 acc_val: 0.49384469696969696\n",
      "epoch 40 / 50 loss_train: 0.9576890237295805 acc_train: 0.4855410447761194 loss_val: 0.9709037054668773 acc_val: 0.49384469696969696\n",
      "epoch 41 / 50 loss_train: 0.9576887479468957 acc_train: 0.4855410447761194 loss_val: 0.9709053328542998 acc_val: 0.49384469696969696\n",
      "epoch 42 / 50 loss_train: 0.9576885193141539 acc_train: 0.4855410447761194 loss_val: 0.9709066513812903 acc_val: 0.49384469696969696\n",
      "epoch 43 / 50 loss_train: 0.9576883173700589 acc_train: 0.4855410447761194 loss_val: 0.9709079464276632 acc_val: 0.49384469696969696\n",
      "epoch 44 / 50 loss_train: 0.9576881434490432 acc_train: 0.4855410447761194 loss_val: 0.9709088585593484 acc_val: 0.49384469696969696\n",
      "epoch 45 / 50 loss_train: 0.9576880002199714 acc_train: 0.4855410447761194 loss_val: 0.9709098899003231 acc_val: 0.49384469696969696\n",
      "epoch 46 / 50 loss_train: 0.957687858325332 acc_train: 0.4855410447761194 loss_val: 0.970910679210316 acc_val: 0.49384469696969696\n",
      "epoch 47 / 50 loss_train: 0.9576877502363119 acc_train: 0.4855410447761194 loss_val: 0.9709111271482526 acc_val: 0.49384469696969696\n",
      "epoch 48 / 50 loss_train: 0.9576876590501017 acc_train: 0.4855410447761194 loss_val: 0.9709118442101912 acc_val: 0.49384469696969696\n",
      "epoch 49 / 50 loss_train: 0.9576875820978364 acc_train: 0.4855410447761194 loss_val: 0.9709123318845575 acc_val: 0.49384469696969696\n",
      "Validation accuracy for lr 0.1 bs 64 hl [50, 25] : 0.49384469696969696\n",
      "epoch 0 / 50 loss_train: 1.2269175350666046 acc_train: 0.5064132462686567 loss_val: 0.9768263643438165 acc_val: 0.49384469696969696\n",
      "epoch 1 / 50 loss_train: 0.9561875462532043 acc_train: 0.5109608208955224 loss_val: 0.9759902628985319 acc_val: 0.49384469696969696\n",
      "epoch 2 / 50 loss_train: 0.9567647616365063 acc_train: 0.503964552238806 loss_val: 0.9744032494949572 acc_val: 0.49384469696969696\n",
      "epoch 3 / 50 loss_train: 0.9570874238192145 acc_train: 0.5013992537313433 loss_val: 0.9734017939278574 acc_val: 0.49384469696969696\n",
      "epoch 4 / 50 loss_train: 0.9572702668496033 acc_train: 0.49836753731343286 loss_val: 0.9727715398326064 acc_val: 0.49384469696969696\n",
      "epoch 5 / 50 loss_train: 0.9573821616706564 acc_train: 0.4918376865671642 loss_val: 0.9723526817379575 acc_val: 0.49384469696969696\n",
      "epoch 6 / 50 loss_train: 0.9574556608698261 acc_train: 0.4918376865671642 loss_val: 0.9720598043817462 acc_val: 0.49384469696969696\n",
      "epoch 7 / 50 loss_train: 0.9575067287060752 acc_train: 0.4918376865671642 loss_val: 0.9718467159704729 acc_val: 0.49384469696969696\n",
      "epoch 8 / 50 loss_train: 0.9575436737999987 acc_train: 0.4918376865671642 loss_val: 0.9716868147705541 acc_val: 0.49384469696969696\n",
      "epoch 9 / 50 loss_train: 0.9575713121179324 acc_train: 0.4918376865671642 loss_val: 0.9715640562953372 acc_val: 0.49384469696969696\n",
      "epoch 10 / 50 loss_train: 0.9575925122446088 acc_train: 0.4904384328358209 loss_val: 0.9714676593289231 acc_val: 0.49384469696969696\n",
      "epoch 11 / 50 loss_train: 0.9576091005730984 acc_train: 0.4881063432835821 loss_val: 0.9713908831278483 acc_val: 0.49384469696969696\n",
      "epoch 12 / 50 loss_train: 0.9576222976641868 acc_train: 0.4855410447761194 loss_val: 0.9713292428941438 acc_val: 0.49384469696969696\n",
      "epoch 13 / 50 loss_train: 0.9576329277522528 acc_train: 0.4855410447761194 loss_val: 0.9712788516824896 acc_val: 0.49384469696969696\n",
      "epoch 14 / 50 loss_train: 0.9576415633087727 acc_train: 0.4855410447761194 loss_val: 0.9712373560125177 acc_val: 0.49384469696969696\n",
      "epoch 15 / 50 loss_train: 0.9576486842845803 acc_train: 0.4855410447761194 loss_val: 0.9712031122409936 acc_val: 0.49384469696969696\n",
      "epoch 16 / 50 loss_train: 0.9576545575661446 acc_train: 0.4855410447761194 loss_val: 0.971174464081273 acc_val: 0.49384469696969696\n",
      "epoch 17 / 50 loss_train: 0.9576594411437191 acc_train: 0.4855410447761194 loss_val: 0.9711504795334556 acc_val: 0.49384469696969696\n",
      "epoch 18 / 50 loss_train: 0.9576635667637213 acc_train: 0.4855410447761194 loss_val: 0.971130492109241 acc_val: 0.49384469696969696\n",
      "epoch 19 / 50 loss_train: 0.9576670149369026 acc_train: 0.4855410447761194 loss_val: 0.9711136005141519 acc_val: 0.49384469696969696\n",
      "epoch 20 / 50 loss_train: 0.9576699168824437 acc_train: 0.4855410447761194 loss_val: 0.9710991978645325 acc_val: 0.49384469696969696\n",
      "epoch 21 / 50 loss_train: 0.9576723989266068 acc_train: 0.4855410447761194 loss_val: 0.9710868019046206 acc_val: 0.49384469696969696\n",
      "epoch 22 / 50 loss_train: 0.9576744913165249 acc_train: 0.4855410447761194 loss_val: 0.9710766329909816 acc_val: 0.49384469696969696\n",
      "epoch 23 / 50 loss_train: 0.9576762616634369 acc_train: 0.4855410447761194 loss_val: 0.9710676868756613 acc_val: 0.49384469696969696\n",
      "epoch 24 / 50 loss_train: 0.9576777878092296 acc_train: 0.4855410447761194 loss_val: 0.9710600141322974 acc_val: 0.49384469696969696\n",
      "epoch 25 / 50 loss_train: 0.9576790946633068 acc_train: 0.4855410447761194 loss_val: 0.9710535587686481 acc_val: 0.49384469696969696\n",
      "epoch 26 / 50 loss_train: 0.9576801933459381 acc_train: 0.4855410447761194 loss_val: 0.9710480263738921 acc_val: 0.49384469696969696\n",
      "epoch 27 / 50 loss_train: 0.9576811519131732 acc_train: 0.4855410447761194 loss_val: 0.9710433754053983 acc_val: 0.49384469696969696\n",
      "epoch 28 / 50 loss_train: 0.9576819757027413 acc_train: 0.4855410447761194 loss_val: 0.9710391741810422 acc_val: 0.49384469696969696\n",
      "epoch 29 / 50 loss_train: 0.9576826727212365 acc_train: 0.4855410447761194 loss_val: 0.9710355906775503 acc_val: 0.49384469696969696\n",
      "epoch 30 / 50 loss_train: 0.9576832843360616 acc_train: 0.4855410447761194 loss_val: 0.9710327079801848 acc_val: 0.49384469696969696\n",
      "epoch 31 / 50 loss_train: 0.9576838007613794 acc_train: 0.4855410447761194 loss_val: 0.9710300600889957 acc_val: 0.49384469696969696\n",
      "epoch 32 / 50 loss_train: 0.9576842500202691 acc_train: 0.4855410447761194 loss_val: 0.9710278131745078 acc_val: 0.49384469696969696\n",
      "epoch 33 / 50 loss_train: 0.9576846436778111 acc_train: 0.4855410447761194 loss_val: 0.9710258624770425 acc_val: 0.49384469696969696\n",
      "epoch 34 / 50 loss_train: 0.957684972392979 acc_train: 0.4855410447761194 loss_val: 0.9710242206400092 acc_val: 0.49384469696969696\n",
      "epoch 35 / 50 loss_train: 0.9576852584063117 acc_train: 0.4855410447761194 loss_val: 0.9710227287176884 acc_val: 0.49384469696969696\n",
      "epoch 36 / 50 loss_train: 0.9576855083899711 acc_train: 0.4855410447761194 loss_val: 0.9710215709426187 acc_val: 0.49384469696969696\n",
      "epoch 37 / 50 loss_train: 0.9576857259024435 acc_train: 0.4855410447761194 loss_val: 0.9710206317179131 acc_val: 0.49384469696969696\n",
      "epoch 38 / 50 loss_train: 0.9576859024923239 acc_train: 0.4855410447761194 loss_val: 0.9710196762373953 acc_val: 0.49384469696969696\n",
      "epoch 39 / 50 loss_train: 0.9576860652930701 acc_train: 0.4855410447761194 loss_val: 0.9710187622995088 acc_val: 0.49384469696969696\n",
      "epoch 40 / 50 loss_train: 0.957686202294791 acc_train: 0.4855410447761194 loss_val: 0.9710180344003619 acc_val: 0.49384469696969696\n",
      "epoch 41 / 50 loss_train: 0.957686312163054 acc_train: 0.4855410447761194 loss_val: 0.9710174040360884 acc_val: 0.49384469696969696\n",
      "epoch 42 / 50 loss_train: 0.957686416248777 acc_train: 0.4855410447761194 loss_val: 0.9710170446020184 acc_val: 0.49384469696969696\n",
      "epoch 43 / 50 loss_train: 0.9576864945354746 acc_train: 0.4855410447761194 loss_val: 0.9710164991292086 acc_val: 0.49384469696969696\n",
      "epoch 44 / 50 loss_train: 0.957686589724982 acc_train: 0.4855410447761194 loss_val: 0.9710161956873807 acc_val: 0.49384469696969696\n",
      "epoch 45 / 50 loss_train: 0.9576866662324365 acc_train: 0.4855410447761194 loss_val: 0.9710158127726931 acc_val: 0.49384469696969696\n",
      "epoch 46 / 50 loss_train: 0.9576867204993519 acc_train: 0.4855410447761194 loss_val: 0.9710155906099261 acc_val: 0.49384469696969696\n",
      "epoch 47 / 50 loss_train: 0.9576867667596731 acc_train: 0.4855410447761194 loss_val: 0.9710153305169308 acc_val: 0.49384469696969696\n",
      "epoch 48 / 50 loss_train: 0.9576868112407514 acc_train: 0.4855410447761194 loss_val: 0.9710151119665666 acc_val: 0.49384469696969696\n",
      "epoch 49 / 50 loss_train: 0.9576868521633433 acc_train: 0.4855410447761194 loss_val: 0.9710150848735463 acc_val: 0.49384469696969696\n",
      "Validation accuracy for lr 0.1 bs 64 hl [100, 50, 25] : 0.49384469696969696\n",
      "epoch 0 / 50 loss_train: 0.6813898767108348 acc_train: 0.6646455223880597 loss_val: 0.501497995108366 acc_val: 0.7724609375\n",
      "epoch 1 / 50 loss_train: 0.6146210369779103 acc_train: 0.683535447761194 loss_val: 0.9327616700902581 acc_val: 0.728515625\n",
      "epoch 2 / 50 loss_train: 0.5761266546462899 acc_train: 0.7250466417910447 loss_val: 0.5984528418630362 acc_val: 0.71875\n",
      "epoch 3 / 50 loss_train: 0.4983155576150809 acc_train: 0.7856809701492538 loss_val: 0.4667603012640029 acc_val: 0.8427734375\n",
      "epoch 4 / 50 loss_train: 0.699432581218321 acc_train: 0.6626632462686567 loss_val: 0.8885389715433121 acc_val: 0.564453125\n",
      "epoch 5 / 50 loss_train: 0.870971184168289 acc_train: 0.46385261194029853 loss_val: 0.8125506844371557 acc_val: 0.54443359375\n",
      "epoch 6 / 50 loss_train: 0.7991541561795704 acc_train: 0.5412779850746269 loss_val: 0.7036974718794227 acc_val: 0.5986328125\n",
      "epoch 7 / 50 loss_train: 0.7427379399982851 acc_train: 0.5771921641791045 loss_val: 0.7520813718438148 acc_val: 0.52294921875\n",
      "epoch 8 / 50 loss_train: 0.7114498766500559 acc_train: 0.5724113805970149 loss_val: 0.693124258890748 acc_val: 0.626953125\n",
      "epoch 9 / 50 loss_train: 0.684487729819853 acc_train: 0.5966651119402985 loss_val: 0.7515760939568281 acc_val: 0.64111328125\n",
      "epoch 10 / 50 loss_train: 0.727988649660082 acc_train: 0.5972481343283582 loss_val: 0.6699015963822603 acc_val: 0.607421875\n",
      "epoch 11 / 50 loss_train: 0.7970669981259019 acc_train: 0.5278684701492538 loss_val: 0.8622667882591486 acc_val: 0.521484375\n",
      "epoch 12 / 50 loss_train: 0.7532978022276465 acc_train: 0.5423274253731343 loss_val: 0.9851124379783869 acc_val: 0.59033203125\n",
      "epoch 13 / 50 loss_train: 0.656995470844098 acc_train: 0.6168376865671642 loss_val: 0.9492450915277004 acc_val: 0.61328125\n",
      "epoch 14 / 50 loss_train: 0.6103958451925818 acc_train: 0.6526352611940298 loss_val: 0.8046001046895981 acc_val: 0.66259765625\n",
      "epoch 15 / 50 loss_train: 0.659629027790098 acc_train: 0.6350279850746269 loss_val: 0.7434968668967485 acc_val: 0.6416015625\n",
      "epoch 16 / 50 loss_train: 0.618795023480458 acc_train: 0.6616138059701493 loss_val: 0.611916090361774 acc_val: 0.66259765625\n",
      "epoch 17 / 50 loss_train: 0.5724871563377665 acc_train: 0.691581156716418 loss_val: 1.0945927361026406 acc_val: 0.720703125\n",
      "epoch 18 / 50 loss_train: 0.8795647763494235 acc_train: 0.5768423507462687 loss_val: 0.9989355988800526 acc_val: 0.52734375\n",
      "epoch 19 / 50 loss_train: 0.8411610295523458 acc_train: 0.531366604477612 loss_val: 0.8250651359558105 acc_val: 0.52490234375\n",
      "epoch 20 / 50 loss_train: 0.8090256772824188 acc_train: 0.5082789179104478 loss_val: 0.8146672882139683 acc_val: 0.52294921875\n",
      "epoch 21 / 50 loss_train: 0.8976966356163594 acc_train: 0.5268190298507462 loss_val: 0.9393749050796032 acc_val: 0.50830078125\n",
      "epoch 22 / 50 loss_train: 0.8452227284659201 acc_train: 0.5244869402985075 loss_val: 1.1145444065332413 acc_val: 0.51171875\n",
      "epoch 23 / 50 loss_train: 0.842298127822022 acc_train: 0.5124766791044776 loss_val: 1.5763783268630505 acc_val: 0.5048828125\n",
      "epoch 24 / 50 loss_train: 0.8215263921823075 acc_train: 0.5072294776119403 loss_val: 2.29897971637547 acc_val: 0.517578125\n",
      "epoch 25 / 50 loss_train: 0.8055290152777487 acc_train: 0.5158582089552238 loss_val: 2.3685139883309603 acc_val: 0.5205078125\n",
      "epoch 26 / 50 loss_train: 0.7896606664159405 acc_train: 0.5082789179104478 loss_val: 2.3486014660447836 acc_val: 0.517578125\n",
      "epoch 27 / 50 loss_train: 0.7507256216077662 acc_train: 0.5188899253731343 loss_val: 2.5734942965209484 acc_val: 0.53466796875\n",
      "epoch 28 / 50 loss_train: 0.8097626557990686 acc_train: 0.5542210820895522 loss_val: 0.8029491677880287 acc_val: 0.50439453125\n",
      "epoch 29 / 50 loss_train: 0.6942537062203706 acc_train: 0.5721781716417911 loss_val: 0.7369979005306959 acc_val: 0.599609375\n",
      "epoch 30 / 50 loss_train: 0.7578299428100017 acc_train: 0.5471082089552238 loss_val: 0.8858638554811478 acc_val: 0.52734375\n",
      "epoch 31 / 50 loss_train: 0.7768587185375726 acc_train: 0.5171408582089553 loss_val: 0.8854686319828033 acc_val: 0.52685546875\n",
      "epoch 32 / 50 loss_train: 0.7506123914647458 acc_train: 0.5250699626865671 loss_val: 0.8684375770390034 acc_val: 0.52734375\n",
      "epoch 33 / 50 loss_train: 0.7240779159674003 acc_train: 0.5457089552238806 loss_val: 0.8449031170457602 acc_val: 0.56005859375\n",
      "epoch 34 / 50 loss_train: 0.7096978105715851 acc_train: 0.5551539179104478 loss_val: 0.8611120767891407 acc_val: 0.55712890625\n",
      "epoch 35 / 50 loss_train: 0.7342185582687606 acc_train: 0.5422108208955224 loss_val: 1.0504101570695639 acc_val: 0.55322265625\n",
      "epoch 36 / 50 loss_train: 0.8481810092926025 acc_train: 0.5517723880597015 loss_val: 1.8573349192738533 acc_val: 0.5390625\n",
      "epoch 37 / 50 loss_train: 1.2394669643088954 acc_train: 0.5385960820895522 loss_val: 0.9056769497692585 acc_val: 0.51025390625\n",
      "epoch 38 / 50 loss_train: 0.8824906518210226 acc_train: 0.5265858208955224 loss_val: 0.9107665568590164 acc_val: 0.50830078125\n",
      "epoch 39 / 50 loss_train: 0.8831608749147671 acc_train: 0.5263526119402985 loss_val: 0.9107040241360664 acc_val: 0.50830078125\n",
      "epoch 40 / 50 loss_train: 0.8831553619299362 acc_train: 0.5263526119402985 loss_val: 0.9106361828744411 acc_val: 0.50830078125\n",
      "epoch 41 / 50 loss_train: 0.8831465333255369 acc_train: 0.5263526119402985 loss_val: 0.9105743356049061 acc_val: 0.50830078125\n",
      "epoch 42 / 50 loss_train: 0.8831382803062895 acc_train: 0.5263526119402985 loss_val: 0.9105180688202381 acc_val: 0.50830078125\n",
      "epoch 43 / 50 loss_train: 0.8831306776004051 acc_train: 0.5263526119402985 loss_val: 0.9104669280350208 acc_val: 0.50830078125\n",
      "epoch 44 / 50 loss_train: 0.8831236673824823 acc_train: 0.5263526119402985 loss_val: 0.9104202948510647 acc_val: 0.50830078125\n",
      "epoch 45 / 50 loss_train: 0.883117159800743 acc_train: 0.5263526119402985 loss_val: 0.9103780947625637 acc_val: 0.50830078125\n",
      "epoch 46 / 50 loss_train: 0.8831111415108638 acc_train: 0.5263526119402985 loss_val: 0.9103395044803619 acc_val: 0.50830078125\n",
      "epoch 47 / 50 loss_train: 0.8831055698110096 acc_train: 0.5263526119402985 loss_val: 0.9103044755756855 acc_val: 0.50830078125\n",
      "epoch 48 / 50 loss_train: 0.8831004206813983 acc_train: 0.5263526119402985 loss_val: 0.9102723672986031 acc_val: 0.50830078125\n",
      "epoch 49 / 50 loss_train: 0.8830956434136006 acc_train: 0.5263526119402985 loss_val: 0.9102432653307915 acc_val: 0.50830078125\n",
      "Validation accuracy for lr 0.1 bs 128 hl [30, 15] : 0.50830078125\n",
      "epoch 0 / 50 loss_train: 1.0176558378917069 acc_train: 0.5181902985074627 loss_val: 0.8806002214550972 acc_val: 0.568359375\n",
      "epoch 1 / 50 loss_train: 0.8843716729932757 acc_train: 0.5966651119402985 loss_val: 0.901496609672904 acc_val: 0.56787109375\n",
      "epoch 2 / 50 loss_train: 0.8786973036936859 acc_train: 0.6006296641791045 loss_val: 0.9896619096398354 acc_val: 0.48681640625\n",
      "epoch 3 / 50 loss_train: 0.9429862525925707 acc_train: 0.5215718283582089 loss_val: 0.9495765157043934 acc_val: 0.52978515625\n",
      "epoch 4 / 50 loss_train: 0.8973978001679948 acc_train: 0.5799906716417911 loss_val: 0.9185953140258789 acc_val: 0.55126953125\n",
      "epoch 5 / 50 loss_train: 0.9056264891553281 acc_train: 0.5757929104477612 loss_val: 0.9814533330500126 acc_val: 0.48388671875\n",
      "epoch 6 / 50 loss_train: 0.9554079299542442 acc_train: 0.49836753731343286 loss_val: 0.9831557683646679 acc_val: 0.47802734375\n",
      "epoch 7 / 50 loss_train: 0.9548314800902978 acc_train: 0.5044309701492538 loss_val: 0.9833205863833427 acc_val: 0.47802734375\n",
      "epoch 8 / 50 loss_train: 0.95485285413799 acc_train: 0.5044309701492538 loss_val: 0.9835136011242867 acc_val: 0.47802734375\n",
      "epoch 9 / 50 loss_train: 0.954881846015133 acc_train: 0.5044309701492538 loss_val: 0.9836659543216228 acc_val: 0.47802734375\n",
      "epoch 10 / 50 loss_train: 0.9549047119581877 acc_train: 0.5044309701492538 loss_val: 0.9837876223027706 acc_val: 0.47802734375\n",
      "epoch 11 / 50 loss_train: 0.9549229011606815 acc_train: 0.5044309701492538 loss_val: 0.9838859587907791 acc_val: 0.47802734375\n",
      "epoch 12 / 50 loss_train: 0.9549376421899938 acc_train: 0.5044309701492538 loss_val: 0.9839669167995453 acc_val: 0.47802734375\n",
      "epoch 13 / 50 loss_train: 0.9549497926413123 acc_train: 0.5044309701492538 loss_val: 0.9840343408286572 acc_val: 0.47802734375\n",
      "epoch 14 / 50 loss_train: 0.9549599209828163 acc_train: 0.5044309701492538 loss_val: 0.9840907603502274 acc_val: 0.47802734375\n",
      "epoch 15 / 50 loss_train: 0.9549685013827993 acc_train: 0.5044309701492538 loss_val: 0.9841386079788208 acc_val: 0.47802734375\n",
      "epoch 16 / 50 loss_train: 0.9549757838249207 acc_train: 0.5044309701492538 loss_val: 0.9841792546212673 acc_val: 0.47802734375\n",
      "epoch 17 / 50 loss_train: 0.9549820414230005 acc_train: 0.5044309701492538 loss_val: 0.9842145070433617 acc_val: 0.47802734375\n",
      "epoch 18 / 50 loss_train: 0.9549874396466497 acc_train: 0.5044309701492538 loss_val: 0.9842447079718113 acc_val: 0.47802734375\n",
      "epoch 19 / 50 loss_train: 0.9549921573098026 acc_train: 0.5044309701492538 loss_val: 0.9842709116637707 acc_val: 0.47802734375\n",
      "epoch 20 / 50 loss_train: 0.9549962771472646 acc_train: 0.5044309701492538 loss_val: 0.984294205904007 acc_val: 0.47802734375\n",
      "epoch 21 / 50 loss_train: 0.9549999254852978 acc_train: 0.5044309701492538 loss_val: 0.9843141250312328 acc_val: 0.47802734375\n",
      "epoch 22 / 50 loss_train: 0.9550031210059551 acc_train: 0.5044309701492538 loss_val: 0.9843319356441498 acc_val: 0.47802734375\n",
      "epoch 23 / 50 loss_train: 0.9550059446647986 acc_train: 0.5044309701492538 loss_val: 0.9843476191163063 acc_val: 0.47802734375\n",
      "epoch 24 / 50 loss_train: 0.9550084551768516 acc_train: 0.5044309701492538 loss_val: 0.9843615368008614 acc_val: 0.47802734375\n",
      "epoch 25 / 50 loss_train: 0.9550107361665413 acc_train: 0.5044309701492538 loss_val: 0.9843737483024597 acc_val: 0.47802734375\n",
      "epoch 26 / 50 loss_train: 0.9550127306980873 acc_train: 0.5044309701492538 loss_val: 0.9843846410512924 acc_val: 0.47802734375\n",
      "epoch 27 / 50 loss_train: 0.9550145339609971 acc_train: 0.5044309701492538 loss_val: 0.9843944609165192 acc_val: 0.47802734375\n",
      "epoch 28 / 50 loss_train: 0.9550161468448923 acc_train: 0.5044309701492538 loss_val: 0.9844032041728497 acc_val: 0.47802734375\n",
      "epoch 29 / 50 loss_train: 0.9550176245063099 acc_train: 0.5044309701492538 loss_val: 0.9844110384583473 acc_val: 0.47802734375\n",
      "epoch 30 / 50 loss_train: 0.9550189313603871 acc_train: 0.5044309701492538 loss_val: 0.9844179712235928 acc_val: 0.47802734375\n",
      "epoch 31 / 50 loss_train: 0.9550201332391198 acc_train: 0.5044309701492538 loss_val: 0.9844243079423904 acc_val: 0.47802734375\n",
      "epoch 32 / 50 loss_train: 0.9550212070123473 acc_train: 0.5044309701492538 loss_val: 0.9844298921525478 acc_val: 0.47802734375\n",
      "epoch 33 / 50 loss_train: 0.9550221740309872 acc_train: 0.5044309701492538 loss_val: 0.9844351001083851 acc_val: 0.47802734375\n",
      "epoch 34 / 50 loss_train: 0.9550230698799019 acc_train: 0.5044309701492538 loss_val: 0.9844399429857731 acc_val: 0.47802734375\n",
      "epoch 35 / 50 loss_train: 0.9550238910006054 acc_train: 0.5044309701492538 loss_val: 0.9844438880681992 acc_val: 0.47802734375\n",
      "epoch 36 / 50 loss_train: 0.9550246089252074 acc_train: 0.5044309701492538 loss_val: 0.9844477511942387 acc_val: 0.47802734375\n",
      "epoch 37 / 50 loss_train: 0.9550252699140293 acc_train: 0.5044309701492538 loss_val: 0.9844512306153774 acc_val: 0.47802734375\n",
      "epoch 38 / 50 loss_train: 0.9550259157792845 acc_train: 0.5044309701492538 loss_val: 0.9844541400671005 acc_val: 0.47802734375\n",
      "epoch 39 / 50 loss_train: 0.9550264673446541 acc_train: 0.5044309701492538 loss_val: 0.9844571724534035 acc_val: 0.47802734375\n",
      "epoch 40 / 50 loss_train: 0.9550269788770533 acc_train: 0.5044309701492538 loss_val: 0.9844596944749355 acc_val: 0.47802734375\n",
      "epoch 41 / 50 loss_train: 0.9550274352529156 acc_train: 0.5044309701492538 loss_val: 0.9844620935618877 acc_val: 0.47802734375\n",
      "epoch 42 / 50 loss_train: 0.9550278533750506 acc_train: 0.5044309701492538 loss_val: 0.9844640307128429 acc_val: 0.47802734375\n",
      "epoch 43 / 50 loss_train: 0.9550282225679996 acc_train: 0.5044309701492538 loss_val: 0.9844659082591534 acc_val: 0.47802734375\n",
      "epoch 44 / 50 loss_train: 0.9550285944298132 acc_train: 0.5044309701492538 loss_val: 0.9844678230583668 acc_val: 0.47802734375\n",
      "epoch 45 / 50 loss_train: 0.9550289484991956 acc_train: 0.5044309701492538 loss_val: 0.9844695329666138 acc_val: 0.47802734375\n",
      "epoch 46 / 50 loss_train: 0.9550292509705273 acc_train: 0.5044309701492538 loss_val: 0.9844707623124123 acc_val: 0.47802734375\n",
      "epoch 47 / 50 loss_train: 0.9550295054022946 acc_train: 0.5044309701492538 loss_val: 0.9844721481204033 acc_val: 0.47802734375\n",
      "epoch 48 / 50 loss_train: 0.9550297509378461 acc_train: 0.5044309701492538 loss_val: 0.9844734035432339 acc_val: 0.47802734375\n",
      "epoch 49 / 50 loss_train: 0.9550299715639939 acc_train: 0.5044309701492538 loss_val: 0.9844745770096779 acc_val: 0.47802734375\n",
      "Validation accuracy for lr 0.1 bs 128 hl [50, 25] : 0.47802734375\n",
      "epoch 0 / 50 loss_train: 1.4012436466430551 acc_train: 0.4899720149253731 loss_val: 1.0004200674593449 acc_val: 0.4775390625\n",
      "epoch 1 / 50 loss_train: 0.9542437578315166 acc_train: 0.5109608208955224 loss_val: 0.9836420156061649 acc_val: 0.47802734375\n",
      "epoch 2 / 50 loss_train: 0.9546695200364981 acc_train: 0.5044309701492538 loss_val: 0.9860243164002895 acc_val: 0.47802734375\n",
      "epoch 3 / 50 loss_train: 0.9549512160358145 acc_train: 0.5044309701492538 loss_val: 0.9870782755315304 acc_val: 0.47802734375\n",
      "epoch 4 / 50 loss_train: 0.9551098924964222 acc_train: 0.5044309701492538 loss_val: 0.987526398152113 acc_val: 0.47802734375\n",
      "epoch 5 / 50 loss_train: 0.9552015822325179 acc_train: 0.5069962686567164 loss_val: 0.9876882582902908 acc_val: 0.47802734375\n",
      "epoch 6 / 50 loss_train: 0.955249129836239 acc_train: 0.5044309701492538 loss_val: 0.9876853413879871 acc_val: 0.47802734375\n",
      "epoch 7 / 50 loss_train: 0.9552662114598858 acc_train: 0.5044309701492538 loss_val: 0.987573903053999 acc_val: 0.47802734375\n",
      "epoch 8 / 50 loss_train: 0.9552627454942731 acc_train: 0.5044309701492538 loss_val: 0.9873846955597401 acc_val: 0.47802734375\n",
      "epoch 9 / 50 loss_train: 0.9552459467702837 acc_train: 0.5044309701492538 loss_val: 0.9871366806328297 acc_val: 0.47802734375\n",
      "epoch 10 / 50 loss_train: 0.9552207473498672 acc_train: 0.5044309701492538 loss_val: 0.9868433997035027 acc_val: 0.47802734375\n",
      "epoch 11 / 50 loss_train: 0.9551906452250125 acc_train: 0.5044309701492538 loss_val: 0.9865158870816231 acc_val: 0.47802734375\n",
      "epoch 12 / 50 loss_train: 0.9551580058994578 acc_train: 0.5044309701492538 loss_val: 0.9861664175987244 acc_val: 0.47802734375\n",
      "epoch 13 / 50 loss_train: 0.9551244609391512 acc_train: 0.5044309701492538 loss_val: 0.985807478427887 acc_val: 0.47802734375\n",
      "epoch 14 / 50 loss_train: 0.9550912211190409 acc_train: 0.5044309701492538 loss_val: 0.9854518584907055 acc_val: 0.47802734375\n",
      "epoch 15 / 50 loss_train: 0.9550592881530079 acc_train: 0.5044309701492538 loss_val: 0.985113337635994 acc_val: 0.47802734375\n",
      "epoch 16 / 50 loss_train: 0.9550295498833727 acc_train: 0.5044309701492538 loss_val: 0.9848038479685783 acc_val: 0.47802734375\n",
      "epoch 17 / 50 loss_train: 0.954902074230251 acc_train: 0.5044309701492538 loss_val: 0.9832517690956593 acc_val: 0.47802734375\n",
      "epoch 18 / 50 loss_train: 0.9548324435504515 acc_train: 0.5044309701492538 loss_val: 0.9831396974623203 acc_val: 0.47802734375\n",
      "epoch 19 / 50 loss_train: 0.9548435380209738 acc_train: 0.5044309701492538 loss_val: 0.983263835310936 acc_val: 0.47802734375\n",
      "epoch 20 / 50 loss_train: 0.9548622885746743 acc_train: 0.5044309701492538 loss_val: 0.983381450176239 acc_val: 0.47802734375\n",
      "epoch 21 / 50 loss_train: 0.9548792705607059 acc_train: 0.5044309701492538 loss_val: 0.983486820012331 acc_val: 0.47802734375\n",
      "epoch 22 / 50 loss_train: 0.9548942784764873 acc_train: 0.5044309701492538 loss_val: 0.983581192791462 acc_val: 0.47802734375\n",
      "epoch 23 / 50 loss_train: 0.9549075987801623 acc_train: 0.5044309701492538 loss_val: 0.98366579413414 acc_val: 0.47802734375\n",
      "epoch 24 / 50 loss_train: 0.9549194227403669 acc_train: 0.5044309701492538 loss_val: 0.9837418124079704 acc_val: 0.47802734375\n",
      "epoch 25 / 50 loss_train: 0.9549299638662765 acc_train: 0.5044309701492538 loss_val: 0.9838100038468838 acc_val: 0.47802734375\n",
      "epoch 26 / 50 loss_train: 0.9549393974133392 acc_train: 0.5044309701492538 loss_val: 0.9838715232908726 acc_val: 0.47802734375\n",
      "epoch 27 / 50 loss_train: 0.9549478657210051 acc_train: 0.5044309701492538 loss_val: 0.9839269891381264 acc_val: 0.47802734375\n",
      "epoch 28 / 50 loss_train: 0.9549554710957542 acc_train: 0.5044309701492538 loss_val: 0.9839770719408989 acc_val: 0.47802734375\n",
      "epoch 29 / 50 loss_train: 0.9549623345261189 acc_train: 0.5044309701492538 loss_val: 0.9840223304927349 acc_val: 0.47802734375\n",
      "epoch 30 / 50 loss_train: 0.954968510279015 acc_train: 0.5044309701492538 loss_val: 0.9840633608400822 acc_val: 0.47802734375\n",
      "epoch 31 / 50 loss_train: 0.9549740784203828 acc_train: 0.5044309701492538 loss_val: 0.9841002747416496 acc_val: 0.47802734375\n",
      "epoch 32 / 50 loss_train: 0.9549791474840534 acc_train: 0.5044309701492538 loss_val: 0.9841339401900768 acc_val: 0.47802734375\n",
      "epoch 33 / 50 loss_train: 0.9549837512756462 acc_train: 0.5044309701492538 loss_val: 0.9841644875705242 acc_val: 0.47802734375\n",
      "epoch 34 / 50 loss_train: 0.9549879129253217 acc_train: 0.5044309701492538 loss_val: 0.9841923043131828 acc_val: 0.47802734375\n",
      "epoch 35 / 50 loss_train: 0.9549916973754541 acc_train: 0.5044309701492538 loss_val: 0.984217319637537 acc_val: 0.47802734375\n",
      "epoch 36 / 50 loss_train: 0.9549951357627983 acc_train: 0.5044309701492538 loss_val: 0.9842402003705502 acc_val: 0.47802734375\n",
      "epoch 37 / 50 loss_train: 0.9549982583344873 acc_train: 0.5044309701492538 loss_val: 0.9842610508203506 acc_val: 0.47802734375\n",
      "epoch 38 / 50 loss_train: 0.9550011238055442 acc_train: 0.5044309701492538 loss_val: 0.9842801801860332 acc_val: 0.47802734375\n",
      "epoch 39 / 50 loss_train: 0.9550037277278616 acc_train: 0.5044309701492538 loss_val: 0.9842974580824375 acc_val: 0.47802734375\n",
      "epoch 40 / 50 loss_train: 0.955006098569329 acc_train: 0.5044309701492538 loss_val: 0.984313327819109 acc_val: 0.47802734375\n",
      "epoch 41 / 50 loss_train: 0.9550082621289723 acc_train: 0.5044309701492538 loss_val: 0.9843277148902416 acc_val: 0.47802734375\n",
      "epoch 42 / 50 loss_train: 0.9550102762321928 acc_train: 0.5044309701492538 loss_val: 0.9843408465385437 acc_val: 0.47802734375\n",
      "epoch 43 / 50 loss_train: 0.9550120892809398 acc_train: 0.5044309701492538 loss_val: 0.9843529313802719 acc_val: 0.47802734375\n",
      "epoch 44 / 50 loss_train: 0.9550137688864523 acc_train: 0.5044309701492538 loss_val: 0.9843640029430389 acc_val: 0.47802734375\n",
      "epoch 45 / 50 loss_train: 0.9550152821327323 acc_train: 0.5044309701492538 loss_val: 0.9843741059303284 acc_val: 0.47802734375\n",
      "epoch 46 / 50 loss_train: 0.9550166628253993 acc_train: 0.5044309701492538 loss_val: 0.9843832366168499 acc_val: 0.47802734375\n",
      "epoch 47 / 50 loss_train: 0.955017975017206 acc_train: 0.5044309701492538 loss_val: 0.9843917563557625 acc_val: 0.47802734375\n",
      "epoch 48 / 50 loss_train: 0.9550191377525898 acc_train: 0.5044309701492538 loss_val: 0.9843995161354542 acc_val: 0.47802734375\n",
      "epoch 49 / 50 loss_train: 0.9550202337663565 acc_train: 0.5044309701492538 loss_val: 0.9844065606594086 acc_val: 0.47802734375\n",
      "Validation accuracy for lr 0.1 bs 128 hl [100, 50, 25] : 0.47802734375\n",
      "epoch 0 / 50 loss_train: 0.30394653713247227 acc_train: 0.8754664179104478 loss_val: 0.20700241437286232 acc_val: 0.9165111940298507\n",
      "epoch 1 / 50 loss_train: 0.169983280092867 acc_train: 0.9321361940298507 loss_val: 0.17335568375155494 acc_val: 0.929570895522388\n",
      "epoch 2 / 50 loss_train: 0.1366860830579509 acc_train: 0.9433302238805971 loss_val: 0.15853312103913017 acc_val: 0.9402985074626866\n",
      "epoch 3 / 50 loss_train: 0.13294831751984781 acc_train: 0.9448460820895522 loss_val: 0.16566271580486092 acc_val: 0.9351679104477612\n",
      "epoch 4 / 50 loss_train: 0.1239356571589527 acc_train: 0.9507929104477612 loss_val: 0.16373650237255682 acc_val: 0.9347014925373134\n",
      "epoch 5 / 50 loss_train: 0.11964119651959514 acc_train: 0.9486940298507462 loss_val: 0.17656225485948945 acc_val: 0.9314365671641791\n",
      "epoch 6 / 50 loss_train: 0.10496997441696837 acc_train: 0.9537080223880597 loss_val: 0.14790870651356328 acc_val: 0.9468283582089553\n",
      "epoch 7 / 50 loss_train: 0.10315896219470966 acc_train: 0.9573227611940298 loss_val: 0.1696311956500912 acc_val: 0.9384328358208955\n",
      "epoch 8 / 50 loss_train: 0.146546570697987 acc_train: 0.9483442164179104 loss_val: 0.25919684432073387 acc_val: 0.9235074626865671\n",
      "epoch 9 / 50 loss_train: 0.11661243388158228 acc_train: 0.9527751865671642 loss_val: 0.1521247560333721 acc_val: 0.9472947761194029\n",
      "epoch 10 / 50 loss_train: 0.10010311330161266 acc_train: 0.956972947761194 loss_val: 0.15078750310540195 acc_val: 0.9514925373134329\n",
      "epoch 11 / 50 loss_train: 0.0916298014900661 acc_train: 0.961054104477612 loss_val: 0.1330487952778918 acc_val: 0.9538246268656716\n",
      "epoch 12 / 50 loss_train: 0.09207673957345507 acc_train: 0.9581389925373134 loss_val: 0.13608637047642796 acc_val: 0.9500932835820896\n",
      "epoch 13 / 50 loss_train: 0.1018310246950905 acc_train: 0.9594216417910447 loss_val: 0.1370150301504034 acc_val: 0.9524253731343284\n",
      "epoch 14 / 50 loss_train: 0.10118757113779603 acc_train: 0.9588386194029851 loss_val: 0.13609119446526965 acc_val: 0.945429104477612\n",
      "epoch 15 / 50 loss_train: 0.09214308353726502 acc_train: 0.9630363805970149 loss_val: 0.12546073038508587 acc_val: 0.9524253731343284\n",
      "epoch 16 / 50 loss_train: 0.0843646961955854 acc_train: 0.9645522388059702 loss_val: 0.12212418390839984 acc_val: 0.9556902985074627\n",
      "epoch 17 / 50 loss_train: 0.07680667639410717 acc_train: 0.9679337686567164 loss_val: 0.12255146407905661 acc_val: 0.9603544776119403\n",
      "epoch 18 / 50 loss_train: 0.0820179808847502 acc_train: 0.9658348880597015 loss_val: 0.16699861624929904 acc_val: 0.9486940298507462\n",
      "epoch 19 / 50 loss_train: 0.08651205026388252 acc_train: 0.964668843283582 loss_val: 0.18748573521568504 acc_val: 0.9440298507462687\n",
      "epoch 20 / 50 loss_train: 0.12555047542655165 acc_train: 0.9556902985074627 loss_val: 0.15008274569695187 acc_val: 0.9561567164179104\n",
      "epoch 21 / 50 loss_train: 0.08103942780307175 acc_train: 0.9643190298507462 loss_val: 0.12463145619081663 acc_val: 0.9575559701492538\n",
      "epoch 22 / 50 loss_train: 0.07557073526354612 acc_train: 0.9678171641791045 loss_val: 0.12712147142509644 acc_val: 0.9598880597014925\n",
      "epoch 23 / 50 loss_train: 0.07405836509860819 acc_train: 0.9680503731343284 loss_val: 0.14543241481253785 acc_val: 0.9538246268656716\n",
      "epoch 24 / 50 loss_train: 0.08061697690225547 acc_train: 0.9684001865671642 loss_val: 0.12944897383785747 acc_val: 0.9622201492537313\n",
      "epoch 25 / 50 loss_train: 0.0721897044794203 acc_train: 0.9697994402985075 loss_val: 0.13268895178843965 acc_val: 0.9575559701492538\n",
      "epoch 26 / 50 loss_train: 0.06973861718360064 acc_train: 0.9706156716417911 loss_val: 0.12895392583535625 acc_val: 0.9622201492537313\n",
      "epoch 27 / 50 loss_train: 0.09567584246857468 acc_train: 0.9614039179104478 loss_val: 0.1529520184260525 acc_val: 0.945429104477612\n",
      "epoch 28 / 50 loss_train: 0.08474308890457796 acc_train: 0.9654850746268657 loss_val: 0.15670064419999835 acc_val: 0.9538246268656716\n",
      "epoch 29 / 50 loss_train: 0.08183562583350855 acc_train: 0.9680503731343284 loss_val: 0.22773244221838282 acc_val: 0.9398320895522388\n",
      "epoch 30 / 50 loss_train: 0.09135669266486648 acc_train: 0.9666511194029851 loss_val: 0.14143573478884092 acc_val: 0.9514925373134329\n",
      "epoch 31 / 50 loss_train: 0.06155407277108327 acc_train: 0.972831156716418 loss_val: 0.14170873174830986 acc_val: 0.957089552238806\n",
      "epoch 32 / 50 loss_train: 0.054424491965293594 acc_train: 0.9753964552238806 loss_val: 0.16706815048763288 acc_val: 0.9566231343283582\n",
      "epoch 33 / 50 loss_train: 0.05793762135078002 acc_train: 0.9745802238805971 loss_val: 0.19562439814466306 acc_val: 0.9482276119402985\n",
      "epoch 34 / 50 loss_train: 0.08546554524334048 acc_train: 0.9694496268656716 loss_val: 0.15949628192811738 acc_val: 0.9500932835820896\n",
      "epoch 35 / 50 loss_train: 0.07185111874857841 acc_train: 0.9694496268656716 loss_val: 0.12363724289984099 acc_val: 0.9589552238805971\n",
      "epoch 36 / 50 loss_train: 0.06717924959240863 acc_train: 0.9735307835820896 loss_val: 0.18986527154932153 acc_val: 0.9584888059701493\n",
      "epoch 37 / 50 loss_train: 0.07135759150197142 acc_train: 0.9722481343283582 loss_val: 0.13943436053419653 acc_val: 0.9594216417910447\n",
      "epoch 38 / 50 loss_train: 0.062028208385635844 acc_train: 0.9748134328358209 loss_val: 0.1395626008704054 acc_val: 0.957089552238806\n",
      "epoch 39 / 50 loss_train: 0.05575107561140721 acc_train: 0.9751632462686567 loss_val: 0.16268144535218126 acc_val: 0.9538246268656716\n",
      "epoch 40 / 50 loss_train: 0.0512286249401103 acc_train: 0.9773787313432836 loss_val: 0.17712852382691102 acc_val: 0.9538246268656716\n",
      "epoch 41 / 50 loss_train: 0.09989230200878424 acc_train: 0.9659514925373134 loss_val: 0.1575249689916263 acc_val: 0.9533582089552238\n",
      "epoch 42 / 50 loss_train: 0.07421184633823208 acc_train: 0.9722481343283582 loss_val: 0.24204891615395932 acc_val: 0.9472947761194029\n",
      "epoch 43 / 50 loss_train: 0.06457786016096125 acc_train: 0.9757462686567164 loss_val: 0.14018579057304195 acc_val: 0.9603544776119403\n",
      "epoch 44 / 50 loss_train: 0.048487996143995224 acc_train: 0.9801772388059702 loss_val: 0.16472942128046583 acc_val: 0.9631529850746269\n",
      "epoch 45 / 50 loss_train: 0.04879909424198219 acc_train: 0.9780783582089553 loss_val: 0.16451604732273004 acc_val: 0.957089552238806\n",
      "epoch 46 / 50 loss_train: 0.06985544617240955 acc_train: 0.9741138059701493 loss_val: 0.16294350781977776 acc_val: 0.9524253731343284\n",
      "epoch 47 / 50 loss_train: 0.059767877826580745 acc_train: 0.9771455223880597 loss_val: 0.14254635020767073 acc_val: 0.9589552238805971\n",
      "epoch 48 / 50 loss_train: 0.06703289584179606 acc_train: 0.9748134328358209 loss_val: 0.14476071096921286 acc_val: 0.9575559701492538\n",
      "epoch 49 / 50 loss_train: 0.05287709323293888 acc_train: 0.9746968283582089 loss_val: 0.15058970767258292 acc_val: 0.9538246268656716\n",
      "Validation accuracy for lr 0.01 bs 32 hl [30, 15] : 0.9538246268656716\n",
      "epoch 0 / 50 loss_train: 0.2899522638059597 acc_train: 0.874883395522388 loss_val: 0.22234188955264447 acc_val: 0.9155783582089553\n",
      "epoch 1 / 50 loss_train: 0.16929470234786842 acc_train: 0.9289878731343284 loss_val: 0.20845295943948244 acc_val: 0.9244402985074627\n",
      "epoch 2 / 50 loss_train: 0.15383570765687235 acc_train: 0.9381996268656716 loss_val: 0.18756335942208538 acc_val: 0.9272388059701493\n",
      "epoch 3 / 50 loss_train: 0.12719418171014804 acc_train: 0.945195895522388 loss_val: 0.1537575391031467 acc_val: 0.9426305970149254\n",
      "epoch 4 / 50 loss_train: 0.13814066540025896 acc_train: 0.9443796641791045 loss_val: 0.17133206748259447 acc_val: 0.941231343283582\n",
      "epoch 5 / 50 loss_train: 0.12579607017991593 acc_train: 0.9472947761194029 loss_val: 0.14610003712265612 acc_val: 0.9426305970149254\n",
      "epoch 6 / 50 loss_train: 0.11200043880085309 acc_train: 0.9526585820895522 loss_val: 0.14871146637542143 acc_val: 0.9477611940298507\n",
      "epoch 7 / 50 loss_train: 0.10979700092844832 acc_train: 0.9545242537313433 loss_val: 0.1524078393630005 acc_val: 0.9449626865671642\n",
      "epoch 8 / 50 loss_train: 0.10423505555059928 acc_train: 0.9567397388059702 loss_val: 0.18329135304222982 acc_val: 0.9319029850746269\n",
      "epoch 9 / 50 loss_train: 0.1262177114301959 acc_train: 0.9512593283582089 loss_val: 0.16268156500712957 acc_val: 0.9430970149253731\n",
      "epoch 10 / 50 loss_train: 0.10595885636076442 acc_train: 0.9561567164179104 loss_val: 0.2004219450780565 acc_val: 0.9440298507462687\n",
      "epoch 11 / 50 loss_train: 0.09788835188460801 acc_train: 0.9593050373134329 loss_val: 0.15399250810300555 acc_val: 0.949160447761194\n",
      "epoch 12 / 50 loss_train: 0.1070907869458341 acc_train: 0.9566231343283582 loss_val: 0.15265245186771606 acc_val: 0.9505597014925373\n",
      "epoch 13 / 50 loss_train: 0.09041378182904627 acc_train: 0.9612873134328358 loss_val: 0.14524422999771397 acc_val: 0.9542910447761194\n",
      "epoch 14 / 50 loss_train: 0.09054687574728211 acc_train: 0.9625699626865671 loss_val: 0.14325598828633287 acc_val: 0.9533582089552238\n",
      "epoch 15 / 50 loss_train: 0.10755235186118611 acc_train: 0.9597714552238806 loss_val: 0.14876357279433092 acc_val: 0.9505597014925373\n",
      "epoch 16 / 50 loss_train: 0.08847924127241721 acc_train: 0.9631529850746269 loss_val: 0.14798185018148896 acc_val: 0.9542910447761194\n",
      "epoch 17 / 50 loss_train: 0.07769858085890492 acc_train: 0.9678171641791045 loss_val: 0.1548536462808972 acc_val: 0.9510261194029851\n",
      "epoch 18 / 50 loss_train: 0.08431861381492371 acc_train: 0.964902052238806 loss_val: 0.1541393130428765 acc_val: 0.9542910447761194\n",
      "epoch 19 / 50 loss_train: 0.09823662546950257 acc_train: 0.9623367537313433 loss_val: 0.200408011478498 acc_val: 0.9505597014925373\n",
      "epoch 20 / 50 loss_train: 0.08368083366532567 acc_train: 0.9668843283582089 loss_val: 0.13802172643653335 acc_val: 0.9542910447761194\n",
      "epoch 21 / 50 loss_train: 0.0764663025933694 acc_train: 0.9689832089552238 loss_val: 0.19887435241631846 acc_val: 0.9365671641791045\n",
      "epoch 22 / 50 loss_train: 0.08701027660805918 acc_train: 0.965018656716418 loss_val: 0.15288991476352667 acc_val: 0.9533582089552238\n",
      "epoch 23 / 50 loss_train: 0.08092689050953147 acc_train: 0.9685167910447762 loss_val: 0.15323033263545577 acc_val: 0.9584888059701493\n",
      "epoch 24 / 50 loss_train: 0.07997911187844897 acc_train: 0.9675839552238806 loss_val: 0.16431734398629608 acc_val: 0.9556902985074627\n",
      "epoch 25 / 50 loss_train: 0.08047765034084546 acc_train: 0.9706156716417911 loss_val: 0.14592327842838945 acc_val: 0.9594216417910447\n",
      "epoch 26 / 50 loss_train: 0.08802995168043165 acc_train: 0.9697994402985075 loss_val: 0.17347382846958204 acc_val: 0.9580223880597015\n",
      "epoch 27 / 50 loss_train: 0.06184042720125754 acc_train: 0.9738805970149254 loss_val: 0.1660761064664157 acc_val: 0.9575559701492538\n",
      "epoch 28 / 50 loss_train: 0.06346191050531853 acc_train: 0.9742304104477612 loss_val: 0.20456100708010627 acc_val: 0.9533582089552238\n",
      "epoch 29 / 50 loss_train: 0.08442368945749755 acc_train: 0.9670009328358209 loss_val: 0.15159318588046436 acc_val: 0.9542910447761194\n",
      "epoch 30 / 50 loss_train: 0.07649788034024226 acc_train: 0.9701492537313433 loss_val: 0.1586308582996297 acc_val: 0.9580223880597015\n",
      "epoch 31 / 50 loss_train: 0.061144775378912226 acc_train: 0.9755130597014925 loss_val: 0.16961475533544013 acc_val: 0.9552238805970149\n",
      "epoch 32 / 50 loss_train: 0.12115952643694751 acc_train: 0.9609375 loss_val: 0.15569569698317223 acc_val: 0.9561567164179104\n",
      "epoch 33 / 50 loss_train: 0.0747008401044356 acc_train: 0.9718983208955224 loss_val: 0.1387713864838476 acc_val: 0.9594216417910447\n",
      "epoch 34 / 50 loss_train: 0.05987094463788124 acc_train: 0.9763292910447762 loss_val: 0.15423360264597152 acc_val: 0.9584888059701493\n",
      "epoch 35 / 50 loss_train: 0.05750713672488873 acc_train: 0.9767957089552238 loss_val: 0.16069315482645943 acc_val: 0.9575559701492538\n",
      "epoch 36 / 50 loss_train: 0.0686366113392856 acc_train: 0.9730643656716418 loss_val: 0.15245462847925484 acc_val: 0.9598880597014925\n",
      "epoch 37 / 50 loss_train: 0.06791383188905076 acc_train: 0.9742304104477612 loss_val: 0.16517333776246673 acc_val: 0.9538246268656716\n",
      "epoch 38 / 50 loss_train: 0.05739309116774872 acc_train: 0.976679104477612 loss_val: 0.24434656874068628 acc_val: 0.9528917910447762\n",
      "epoch 39 / 50 loss_train: 0.059985273308379265 acc_train: 0.9757462686567164 loss_val: 0.17277164616867074 acc_val: 0.957089552238806\n",
      "epoch 40 / 50 loss_train: 0.10165200298949607 acc_train: 0.9708488805970149 loss_val: 0.15541106426106993 acc_val: 0.9510261194029851\n",
      "epoch 41 / 50 loss_train: 0.09428056760972045 acc_train: 0.9707322761194029 loss_val: 0.1653948083952068 acc_val: 0.9514925373134329\n",
      "epoch 42 / 50 loss_train: 0.06964816978185823 acc_train: 0.9716651119402985 loss_val: 0.20913269016743366 acc_val: 0.9482276119402985\n",
      "epoch 43 / 50 loss_train: 0.05228341932934709 acc_train: 0.9794776119402985 loss_val: 0.19917212496685222 acc_val: 0.9524253731343284\n",
      "epoch 44 / 50 loss_train: 0.0508193234433218 acc_train: 0.9778451492537313 loss_val: 0.238439217836843 acc_val: 0.949160447761194\n",
      "epoch 45 / 50 loss_train: 0.05299921804642274 acc_train: 0.9777285447761194 loss_val: 0.19638061793413833 acc_val: 0.9505597014925373\n",
      "epoch 46 / 50 loss_train: 0.048760474110330604 acc_train: 0.9798274253731343 loss_val: 0.1750425523369323 acc_val: 0.9528917910447762\n",
      "epoch 47 / 50 loss_train: 0.05948693208098229 acc_train: 0.9763292910447762 loss_val: 0.19473007108135307 acc_val: 0.9552238805970149\n",
      "epoch 48 / 50 loss_train: 0.04873581341275691 acc_train: 0.980293843283582 loss_val: 0.24659580497010208 acc_val: 0.9552238805970149\n",
      "epoch 49 / 50 loss_train: 0.04734517637678152 acc_train: 0.9801772388059702 loss_val: 0.2601402126209177 acc_val: 0.9482276119402985\n",
      "Validation accuracy for lr 0.01 bs 32 hl [50, 25] : 0.9482276119402985\n",
      "epoch 0 / 50 loss_train: 0.30153753244276366 acc_train: 0.8753498134328358 loss_val: 0.1929155674992235 acc_val: 0.9160447761194029\n",
      "epoch 1 / 50 loss_train: 0.21738651638794967 acc_train: 0.9155783582089553 loss_val: 0.3232470157296838 acc_val: 0.8745335820895522\n",
      "epoch 2 / 50 loss_train: 0.17094401821299499 acc_train: 0.929570895522388 loss_val: 0.16972264889257382 acc_val: 0.9375\n",
      "epoch 3 / 50 loss_train: 0.13695758324700283 acc_train: 0.9428638059701493 loss_val: 0.1688350337936118 acc_val: 0.9375\n",
      "epoch 4 / 50 loss_train: 0.1423650297098237 acc_train: 0.9411147388059702 loss_val: 0.1761401993492452 acc_val: 0.9407649253731343\n",
      "epoch 5 / 50 loss_train: 0.1261363900353465 acc_train: 0.9442630597014925 loss_val: 0.16028365670227862 acc_val: 0.945429104477612\n",
      "epoch 6 / 50 loss_train: 0.13281338286600006 acc_train: 0.9457789179104478 loss_val: 0.18791567529731412 acc_val: 0.9370335820895522\n",
      "epoch 7 / 50 loss_train: 0.12376105126841411 acc_train: 0.9465951492537313 loss_val: 0.1578322158098613 acc_val: 0.9402985074626866\n",
      "epoch 8 / 50 loss_train: 0.11168665199022768 acc_train: 0.9517257462686567 loss_val: 0.2011519587512069 acc_val: 0.9375\n",
      "epoch 9 / 50 loss_train: 0.11481520144873535 acc_train: 0.9538246268656716 loss_val: 0.15536343168092387 acc_val: 0.9468283582089553\n",
      "epoch 10 / 50 loss_train: 0.09819859611052922 acc_train: 0.956972947761194 loss_val: 0.14513567585581497 acc_val: 0.9449626865671642\n",
      "epoch 11 / 50 loss_train: 0.0968708740860057 acc_train: 0.9587220149253731 loss_val: 0.1821764557801055 acc_val: 0.9482276119402985\n",
      "epoch 12 / 50 loss_train: 0.10554355638859265 acc_train: 0.9574393656716418 loss_val: 0.17531734231284243 acc_val: 0.9444962686567164\n",
      "epoch 13 / 50 loss_train: 0.14699555162962444 acc_train: 0.9436800373134329 loss_val: 0.17750634936035797 acc_val: 0.9444962686567164\n",
      "epoch 14 / 50 loss_train: 0.1058239422385831 acc_train: 0.9575559701492538 loss_val: 0.14531261598953712 acc_val: 0.9477611940298507\n",
      "epoch 15 / 50 loss_train: 0.1138108764347662 acc_train: 0.956972947761194 loss_val: 0.15652526746964837 acc_val: 0.9421641791044776\n",
      "epoch 16 / 50 loss_train: 0.08527179027854555 acc_train: 0.9658348880597015 loss_val: 0.14511765967920418 acc_val: 0.9472947761194029\n",
      "epoch 17 / 50 loss_train: 0.0788540913715373 acc_train: 0.9665345149253731 loss_val: 0.14756625915749702 acc_val: 0.9463619402985075\n",
      "epoch 18 / 50 loss_train: 0.08430725695213925 acc_train: 0.9665345149253731 loss_val: 0.15706684767455817 acc_val: 0.9528917910447762\n",
      "epoch 19 / 50 loss_train: 0.08717033386857287 acc_train: 0.964785447761194 loss_val: 0.13424908619377512 acc_val: 0.9472947761194029\n",
      "epoch 20 / 50 loss_train: 0.11265634761952947 acc_train: 0.9614039179104478 loss_val: 0.1663519773974379 acc_val: 0.9477611940298507\n",
      "epoch 21 / 50 loss_train: 0.09349792844686682 acc_train: 0.961054104477612 loss_val: 0.17538545902522004 acc_val: 0.9472947761194029\n",
      "epoch 22 / 50 loss_train: 0.09566728203295063 acc_train: 0.961054104477612 loss_val: 0.18955971035097274 acc_val: 0.9500932835820896\n",
      "epoch 23 / 50 loss_train: 0.09570088567089619 acc_train: 0.9594216417910447 loss_val: 0.19336476087532173 acc_val: 0.9556902985074627\n",
      "epoch 24 / 50 loss_train: 0.0953039909704717 acc_train: 0.9624533582089553 loss_val: 0.17449735087791343 acc_val: 0.9528917910447762\n",
      "epoch 25 / 50 loss_train: 0.09271188805110227 acc_train: 0.9645522388059702 loss_val: 0.19802153035438555 acc_val: 0.9407649253731343\n",
      "epoch 26 / 50 loss_train: 0.12065125153371334 acc_train: 0.9584888059701493 loss_val: 0.18795373737578416 acc_val: 0.9505597014925373\n",
      "epoch 27 / 50 loss_train: 0.11314439040143043 acc_train: 0.953008395522388 loss_val: 0.2989560813258193 acc_val: 0.8959888059701493\n",
      "epoch 28 / 50 loss_train: 0.08795385807804193 acc_train: 0.9622201492537313 loss_val: 0.16376723454830064 acc_val: 0.9528917910447762\n",
      "epoch 29 / 50 loss_train: 0.06037431046972225 acc_train: 0.9735307835820896 loss_val: 0.21147444048118022 acc_val: 0.9538246268656716\n",
      "epoch 30 / 50 loss_train: 0.06800601152807853 acc_train: 0.972831156716418 loss_val: 0.2916676541836655 acc_val: 0.941231343283582\n",
      "epoch 31 / 50 loss_train: 0.08778090472390825 acc_train: 0.9678171641791045 loss_val: 0.21512194387959097 acc_val: 0.9584888059701493\n",
      "epoch 32 / 50 loss_train: 0.08148548766474784 acc_train: 0.9696828358208955 loss_val: 0.19158650175333958 acc_val: 0.9552238805970149\n",
      "epoch 33 / 50 loss_train: 0.06178069612636123 acc_train: 0.9737639925373134 loss_val: 0.1511867749207123 acc_val: 0.9542910447761194\n",
      "epoch 34 / 50 loss_train: 0.052645490661519426 acc_train: 0.9757462686567164 loss_val: 0.2580961800204277 acc_val: 0.9496268656716418\n",
      "epoch 35 / 50 loss_train: 0.052554336826511616 acc_train: 0.9778451492537313 loss_val: 0.26706797129143306 acc_val: 0.9542910447761194\n",
      "epoch 36 / 50 loss_train: 0.0506516545593418 acc_train: 0.9801772388059702 loss_val: 0.2077830726051872 acc_val: 0.9482276119402985\n",
      "epoch 37 / 50 loss_train: 0.09316220520361354 acc_train: 0.9693330223880597 loss_val: 0.19097071193673257 acc_val: 0.9482276119402985\n",
      "epoch 38 / 50 loss_train: 0.12530508245569097 acc_train: 0.9626865671641791 loss_val: 0.23641808362896624 acc_val: 0.9514925373134329\n",
      "epoch 39 / 50 loss_train: 0.10340810685858144 acc_train: 0.9651352611940298 loss_val: 0.20624944906324263 acc_val: 0.9552238805970149\n",
      "epoch 40 / 50 loss_train: 0.07739011384671049 acc_train: 0.9731809701492538 loss_val: 0.27454272554030107 acc_val: 0.9556902985074627\n",
      "epoch 41 / 50 loss_train: 0.1971093033581152 acc_train: 0.9432136194029851 loss_val: 0.21874375612787506 acc_val: 0.949160447761194\n",
      "epoch 42 / 50 loss_train: 0.07727510808770115 acc_train: 0.9703824626865671 loss_val: 0.2206988740173997 acc_val: 0.9594216417910447\n",
      "epoch 43 / 50 loss_train: 0.06560029342865924 acc_train: 0.9763292910447762 loss_val: 0.19874284198508602 acc_val: 0.9575559701492538\n",
      "epoch 44 / 50 loss_train: 0.05920544005849519 acc_train: 0.9770289179104478 loss_val: 0.19996334644397024 acc_val: 0.9524253731343284\n",
      "epoch 45 / 50 loss_train: 0.04402486664461756 acc_train: 0.980410447761194 loss_val: 0.18623612462164932 acc_val: 0.9594216417910447\n",
      "epoch 46 / 50 loss_train: 0.047070690138460276 acc_train: 0.9820429104477612 loss_val: 0.1655112058681179 acc_val: 0.9556902985074627\n",
      "epoch 47 / 50 loss_train: 0.048558469636421915 acc_train: 0.9797108208955224 loss_val: 0.20196731603682636 acc_val: 0.9566231343283582\n",
      "epoch 48 / 50 loss_train: 0.03846954867288047 acc_train: 0.9837919776119403 loss_val: 0.20874392960772198 acc_val: 0.9538246268656716\n",
      "epoch 49 / 50 loss_train: 0.06530906928008932 acc_train: 0.9708488805970149 loss_val: 0.257270054666704 acc_val: 0.9589552238805971\n",
      "Validation accuracy for lr 0.01 bs 32 hl [100, 50, 25] : 0.9589552238805971\n",
      "epoch 0 / 50 loss_train: 0.30865520883851977 acc_train: 0.8726679104477612 loss_val: 0.18673185191988578 acc_val: 0.9237689393939394\n",
      "epoch 1 / 50 loss_train: 0.16861553787629105 acc_train: 0.9300373134328358 loss_val: 0.16323011252683445 acc_val: 0.9360795454545454\n",
      "epoch 2 / 50 loss_train: 0.13886253691431302 acc_train: 0.941347947761194 loss_val: 0.15532893120555127 acc_val: 0.9370265151515151\n",
      "epoch 3 / 50 loss_train: 0.13553434486653823 acc_train: 0.9421641791044776 loss_val: 0.15626381437795195 acc_val: 0.9375\n",
      "epoch 4 / 50 loss_train: 0.11915314692392279 acc_train: 0.9502098880597015 loss_val: 0.14676132873687425 acc_val: 0.9431818181818182\n",
      "epoch 5 / 50 loss_train: 0.11306175388006577 acc_train: 0.9520755597014925 loss_val: 0.13829184296143163 acc_val: 0.946969696969697\n",
      "epoch 6 / 50 loss_train: 0.10893352196287753 acc_train: 0.953008395522388 loss_val: 0.14192996529351204 acc_val: 0.9483901515151515\n",
      "epoch 7 / 50 loss_train: 0.10650102520333742 acc_train: 0.9537080223880597 loss_val: 0.1668021284803093 acc_val: 0.9384469696969697\n",
      "epoch 8 / 50 loss_train: 0.10535689768617723 acc_train: 0.9549906716417911 loss_val: 0.14595828074208045 acc_val: 0.9460227272727273\n",
      "epoch 9 / 50 loss_train: 0.09369153480754415 acc_train: 0.9582555970149254 loss_val: 0.14295203819324914 acc_val: 0.9517045454545454\n",
      "epoch 10 / 50 loss_train: 0.0907884846205142 acc_train: 0.9600046641791045 loss_val: 0.1575114386121806 acc_val: 0.9493371212121212\n",
      "epoch 11 / 50 loss_train: 0.09195955466034252 acc_train: 0.9600046641791045 loss_val: 0.1452396148642637 acc_val: 0.9512310606060606\n",
      "epoch 12 / 50 loss_train: 0.08986707234671756 acc_train: 0.9597714552238806 loss_val: 0.14842935011085612 acc_val: 0.9507575757575758\n",
      "epoch 13 / 50 loss_train: 0.08396859125081282 acc_train: 0.9635027985074627 loss_val: 0.13963857813566813 acc_val: 0.9517045454545454\n",
      "epoch 14 / 50 loss_train: 0.12219918740273857 acc_train: 0.9540578358208955 loss_val: 0.1511523762773405 acc_val: 0.9389204545454546\n",
      "epoch 15 / 50 loss_train: 0.09167946850074761 acc_train: 0.9611707089552238 loss_val: 0.15920256735426902 acc_val: 0.9455492424242424\n",
      "epoch 16 / 50 loss_train: 0.07727024655566732 acc_train: 0.9670009328358209 loss_val: 0.1529862281053213 acc_val: 0.9488636363636364\n",
      "epoch 17 / 50 loss_train: 0.08013879068529428 acc_train: 0.9656016791044776 loss_val: 0.16150940969182007 acc_val: 0.946969696969697\n",
      "epoch 18 / 50 loss_train: 0.07848769805825023 acc_train: 0.964902052238806 loss_val: 0.15463416118732173 acc_val: 0.9479166666666666\n",
      "epoch 19 / 50 loss_train: 0.07866442548250084 acc_train: 0.9645522388059702 loss_val: 0.12361995233087245 acc_val: 0.9564393939393939\n",
      "epoch 20 / 50 loss_train: 0.06955639582905751 acc_train: 0.9685167910447762 loss_val: 0.12858689874111184 acc_val: 0.9554924242424242\n",
      "epoch 21 / 50 loss_train: 0.07431819014001026 acc_train: 0.9664179104477612 loss_val: 0.14028714090364955 acc_val: 0.9569128787878788\n",
      "epoch 22 / 50 loss_train: 0.0709791556556723 acc_train: 0.9678171641791045 loss_val: 0.13857222568538308 acc_val: 0.9493371212121212\n",
      "epoch 23 / 50 loss_train: 0.07605425711137367 acc_train: 0.9679337686567164 loss_val: 0.17605131925523493 acc_val: 0.9474431818181818\n",
      "epoch 24 / 50 loss_train: 0.07588140710965911 acc_train: 0.9682835820895522 loss_val: 0.1389710718232438 acc_val: 0.9540719696969697\n",
      "epoch 25 / 50 loss_train: 0.06268131170672045 acc_train: 0.972831156716418 loss_val: 0.1438062798050933 acc_val: 0.9554924242424242\n",
      "epoch 26 / 50 loss_train: 0.0708230123994177 acc_train: 0.9708488805970149 loss_val: 0.17709333220299212 acc_val: 0.9488636363636364\n",
      "epoch 27 / 50 loss_train: 0.06821497479008871 acc_train: 0.9697994402985075 loss_val: 0.15217458234398387 acc_val: 0.9540719696969697\n",
      "epoch 28 / 50 loss_train: 0.09914991168527683 acc_train: 0.9621035447761194 loss_val: 0.14738148113247007 acc_val: 0.9488636363636364\n",
      "epoch 29 / 50 loss_train: 0.07246785171663583 acc_train: 0.9716651119402985 loss_val: 0.15224565443305904 acc_val: 0.9564393939393939\n",
      "epoch 30 / 50 loss_train: 0.061986277640950105 acc_train: 0.9739972014925373 loss_val: 0.1507444643934281 acc_val: 0.9521780303030303\n",
      "epoch 31 / 50 loss_train: 0.06147540169907039 acc_train: 0.9736473880597015 loss_val: 0.15158376086026065 acc_val: 0.9545454545454546\n",
      "epoch 32 / 50 loss_train: 0.06899521289282103 acc_train: 0.9730643656716418 loss_val: 0.16303270202333103 acc_val: 0.9517045454545454\n",
      "epoch 33 / 50 loss_train: 0.061281131118285787 acc_train: 0.9736473880597015 loss_val: 0.18023043191992932 acc_val: 0.9526515151515151\n",
      "epoch 34 / 50 loss_train: 0.05737182748879292 acc_train: 0.9753964552238806 loss_val: 0.17689864408703754 acc_val: 0.9573863636363636\n",
      "epoch 35 / 50 loss_train: 0.06157770911370641 acc_train: 0.9744636194029851 loss_val: 0.17004903013272119 acc_val: 0.959280303030303\n",
      "epoch 36 / 50 loss_train: 0.06445955225169214 acc_train: 0.9729477611940298 loss_val: 0.17364780912491065 acc_val: 0.9517045454545454\n",
      "epoch 37 / 50 loss_train: 0.048950889543977694 acc_train: 0.9773787313432836 loss_val: 0.16192507215338758 acc_val: 0.9526515151515151\n",
      "epoch 38 / 50 loss_train: 0.06135392562771188 acc_train: 0.9745802238805971 loss_val: 0.1892897208083675 acc_val: 0.9502840909090909\n",
      "epoch 39 / 50 loss_train: 0.07282499348008033 acc_train: 0.9697994402985075 loss_val: 0.18350001830769921 acc_val: 0.9526515151515151\n",
      "epoch 40 / 50 loss_train: 0.060755505207439525 acc_train: 0.9753964552238806 loss_val: 0.18228551439784035 acc_val: 0.9578598484848485\n",
      "epoch 41 / 50 loss_train: 0.051583553977143856 acc_train: 0.976679104477612 loss_val: 0.20113862661949525 acc_val: 0.9540719696969697\n",
      "epoch 42 / 50 loss_train: 0.04773733257698312 acc_train: 0.9772621268656716 loss_val: 0.19788029322562856 acc_val: 0.9569128787878788\n",
      "epoch 43 / 50 loss_train: 0.056304101519453434 acc_train: 0.9771455223880597 loss_val: 0.1941199514894696 acc_val: 0.9502840909090909\n",
      "epoch 44 / 50 loss_train: 0.04793694212601812 acc_train: 0.980410447761194 loss_val: 0.23000786669682016 acc_val: 0.9483901515151515\n",
      "epoch 45 / 50 loss_train: 0.04717229866186407 acc_train: 0.9792444029850746 loss_val: 0.2072738955814037 acc_val: 0.9545454545454546\n",
      "epoch 46 / 50 loss_train: 0.04272547088660745 acc_train: 0.9807602611940298 loss_val: 0.17939583683712756 acc_val: 0.9588068181818182\n",
      "epoch 47 / 50 loss_train: 0.042609009068153465 acc_train: 0.9825093283582089 loss_val: 0.18421797326884526 acc_val: 0.9564393939393939\n",
      "epoch 48 / 50 loss_train: 0.043621757141151116 acc_train: 0.980293843283582 loss_val: 0.18739949072044676 acc_val: 0.9621212121212122\n",
      "epoch 49 / 50 loss_train: 0.04567635429279406 acc_train: 0.980527052238806 loss_val: 0.18373133448999673 acc_val: 0.9616477272727273\n",
      "Validation accuracy for lr 0.01 bs 64 hl [30, 15] : 0.9616477272727273\n",
      "epoch 0 / 50 loss_train: 0.32871249698992094 acc_train: 0.8622901119402985 loss_val: 0.1957956209115571 acc_val: 0.9214015151515151\n",
      "epoch 1 / 50 loss_train: 0.1727156436154202 acc_train: 0.9293376865671642 loss_val: 0.18007971860159463 acc_val: 0.9270833333333334\n",
      "epoch 2 / 50 loss_train: 0.13427418909633337 acc_train: 0.9446128731343284 loss_val: 0.14715988862427007 acc_val: 0.946969696969697\n",
      "epoch 3 / 50 loss_train: 0.13599425192866751 acc_train: 0.9444962686567164 loss_val: 0.16680013319869016 acc_val: 0.9341856060606061\n",
      "epoch 4 / 50 loss_train: 0.13294493028921867 acc_train: 0.9467117537313433 loss_val: 0.14180111227247064 acc_val: 0.9450757575757576\n",
      "epoch 5 / 50 loss_train: 0.10671959986993626 acc_train: 0.9544076492537313 loss_val: 0.14041079494000813 acc_val: 0.9450757575757576\n",
      "epoch 6 / 50 loss_train: 0.104233920935597 acc_train: 0.9562733208955224 loss_val: 0.14979173548393443 acc_val: 0.9488636363636364\n",
      "epoch 7 / 50 loss_train: 0.10651764466858177 acc_train: 0.9583722014925373 loss_val: 0.16356558493465523 acc_val: 0.9422348484848485\n",
      "epoch 8 / 50 loss_train: 0.09889269074939414 acc_train: 0.9586054104477612 loss_val: 0.14049979078436964 acc_val: 0.9455492424242424\n",
      "epoch 9 / 50 loss_train: 0.09853178289121212 acc_train: 0.9558069029850746 loss_val: 0.13728633906981325 acc_val: 0.9512310606060606\n",
      "epoch 10 / 50 loss_train: 0.10001521047427138 acc_train: 0.9558069029850746 loss_val: 0.1299183939685623 acc_val: 0.9517045454545454\n",
      "epoch 11 / 50 loss_train: 0.09153244944303227 acc_train: 0.9609375 loss_val: 0.14146106193496052 acc_val: 0.9488636363636364\n",
      "epoch 12 / 50 loss_train: 0.08577886273834243 acc_train: 0.9630363805970149 loss_val: 0.15418701928296077 acc_val: 0.9521780303030303\n",
      "epoch 13 / 50 loss_train: 0.08391664646315708 acc_train: 0.9605876865671642 loss_val: 0.16146039718106556 acc_val: 0.9460227272727273\n",
      "epoch 14 / 50 loss_train: 0.08486341012280378 acc_train: 0.9639692164179104 loss_val: 0.13298626974849895 acc_val: 0.9507575757575758\n",
      "epoch 15 / 50 loss_train: 0.07238757895277952 acc_train: 0.968633395522388 loss_val: 0.15842942760512166 acc_val: 0.9464962121212122\n",
      "epoch 16 / 50 loss_train: 0.0685944721283419 acc_train: 0.9709654850746269 loss_val: 0.14661450150439728 acc_val: 0.9479166666666666\n",
      "epoch 17 / 50 loss_train: 0.0752110262609907 acc_train: 0.9684001865671642 loss_val: 0.14458377464213548 acc_val: 0.9526515151515151\n",
      "epoch 18 / 50 loss_train: 0.10769347707504656 acc_train: 0.9614039179104478 loss_val: 0.14163235971390156 acc_val: 0.9550189393939394\n",
      "epoch 19 / 50 loss_train: 0.0912392113510686 acc_train: 0.9622201492537313 loss_val: 0.15364059834477797 acc_val: 0.9502840909090909\n",
      "epoch 20 / 50 loss_train: 0.07573301674547925 acc_train: 0.968633395522388 loss_val: 0.15912935423098548 acc_val: 0.9493371212121212\n",
      "epoch 21 / 50 loss_train: 0.07026866899886683 acc_train: 0.9703824626865671 loss_val: 0.14753622617740114 acc_val: 0.9512310606060606\n",
      "epoch 22 / 50 loss_train: 0.06159282419761059 acc_train: 0.972714552238806 loss_val: 0.1660497628484915 acc_val: 0.9493371212121212\n",
      "epoch 23 / 50 loss_train: 0.0569983123238685 acc_train: 0.9746968283582089 loss_val: 0.1841419452346489 acc_val: 0.9535984848484849\n",
      "epoch 24 / 50 loss_train: 0.05747702926633629 acc_train: 0.9751632462686567 loss_val: 0.18356981446788379 acc_val: 0.9550189393939394\n",
      "epoch 25 / 50 loss_train: 0.06756000042851291 acc_train: 0.9730643656716418 loss_val: 0.19472931278351374 acc_val: 0.9545454545454546\n",
      "epoch 26 / 50 loss_train: 0.07474580877321536 acc_train: 0.9703824626865671 loss_val: 0.22836959614086388 acc_val: 0.9341856060606061\n",
      "epoch 27 / 50 loss_train: 0.08112690000405619 acc_train: 0.9657182835820896 loss_val: 0.18990234464438976 acc_val: 0.9488636363636364\n",
      "epoch 28 / 50 loss_train: 0.06562411688518391 acc_train: 0.972481343283582 loss_val: 0.19391533229852279 acc_val: 0.9427083333333334\n",
      "epoch 29 / 50 loss_train: 0.05924650332056431 acc_train: 0.9758628731343284 loss_val: 0.18133197015545607 acc_val: 0.9526515151515151\n",
      "epoch 30 / 50 loss_train: 0.04921878475188486 acc_train: 0.9787779850746269 loss_val: 0.17765185856018376 acc_val: 0.9507575757575758\n",
      "epoch 31 / 50 loss_train: 0.04769310138167452 acc_train: 0.9777285447761194 loss_val: 0.19788433512108536 acc_val: 0.9526515151515151\n",
      "epoch 32 / 50 loss_train: 0.06857673791862691 acc_train: 0.9716651119402985 loss_val: 0.20410197466139146 acc_val: 0.9408143939393939\n",
      "epoch 33 / 50 loss_train: 0.06659429993066909 acc_train: 0.9752798507462687 loss_val: 0.19458195749735305 acc_val: 0.9521780303030303\n",
      "epoch 34 / 50 loss_train: 0.06557913462339497 acc_train: 0.9739972014925373 loss_val: 0.16439171656538445 acc_val: 0.9550189393939394\n",
      "epoch 35 / 50 loss_train: 0.049177229407924544 acc_train: 0.9798274253731343 loss_val: 0.22728498043364861 acc_val: 0.9455492424242424\n",
      "epoch 36 / 50 loss_train: 0.0511735415697765 acc_train: 0.9799440298507462 loss_val: 0.20326299170648612 acc_val: 0.9526515151515151\n",
      "epoch 37 / 50 loss_train: 0.06386776651957754 acc_train: 0.9798274253731343 loss_val: 0.22880879566963053 acc_val: 0.9483901515151515\n",
      "epoch 38 / 50 loss_train: 0.05559736715261338 acc_train: 0.9772621268656716 loss_val: 0.16439708871511163 acc_val: 0.9554924242424242\n",
      "epoch 39 / 50 loss_train: 0.08090408161671749 acc_train: 0.9699160447761194 loss_val: 0.18876239844986525 acc_val: 0.9474431818181818\n",
      "epoch 40 / 50 loss_train: 0.057857911529909094 acc_train: 0.976445895522388 loss_val: 0.1627469616128351 acc_val: 0.9583333333333334\n",
      "epoch 41 / 50 loss_train: 0.03925774747908894 acc_train: 0.9829757462686567 loss_val: 0.16359237306127652 acc_val: 0.9550189393939394\n",
      "epoch 42 / 50 loss_train: 0.03940704630673932 acc_train: 0.9851912313432836 loss_val: 0.1732988073505167 acc_val: 0.9569128787878788\n",
      "epoch 43 / 50 loss_train: 0.03659514639936665 acc_train: 0.984375 loss_val: 0.1812170965921235 acc_val: 0.9559659090909091\n",
      "epoch 44 / 50 loss_train: 0.06185603628643969 acc_train: 0.9777285447761194 loss_val: 0.20710724065164918 acc_val: 0.9535984848484849\n",
      "epoch 45 / 50 loss_train: 0.06572925351706069 acc_train: 0.9749300373134329 loss_val: 0.19108713385216877 acc_val: 0.9441287878787878\n",
      "epoch 46 / 50 loss_train: 0.05294842223390075 acc_train: 0.9791277985074627 loss_val: 0.19581023130508685 acc_val: 0.9521780303030303\n",
      "epoch 47 / 50 loss_train: 0.06007990486521983 acc_train: 0.9774953358208955 loss_val: 0.16491559804434128 acc_val: 0.9550189393939394\n",
      "epoch 48 / 50 loss_train: 0.04314374061363902 acc_train: 0.9808768656716418 loss_val: 0.2076086076681375 acc_val: 0.9498106060606061\n",
      "epoch 49 / 50 loss_train: 0.04220631735142209 acc_train: 0.9821595149253731 loss_val: 0.226012877074302 acc_val: 0.9554924242424242\n",
      "Validation accuracy for lr 0.01 bs 64 hl [50, 25] : 0.9554924242424242\n",
      "epoch 0 / 50 loss_train: 0.3166332978810837 acc_train: 0.8652052238805971 loss_val: 0.22909295719793596 acc_val: 0.9067234848484849\n",
      "epoch 1 / 50 loss_train: 0.1816168556462473 acc_train: 0.9235074626865671 loss_val: 0.16725629114091217 acc_val: 0.9370265151515151\n",
      "epoch 2 / 50 loss_train: 0.1519541048458708 acc_train: 0.9385494402985075 loss_val: 0.15874476665912476 acc_val: 0.9412878787878788\n",
      "epoch 3 / 50 loss_train: 0.12861497591791757 acc_train: 0.945429104477612 loss_val: 0.15195771096462793 acc_val: 0.9464962121212122\n",
      "epoch 4 / 50 loss_train: 0.13295671976268736 acc_train: 0.9426305970149254 loss_val: 0.15432617794519474 acc_val: 0.9412878787878788\n",
      "epoch 5 / 50 loss_train: 0.1118927574369 acc_train: 0.9516091417910447 loss_val: 0.16268320443573758 acc_val: 0.9427083333333334\n",
      "epoch 6 / 50 loss_train: 0.10136160034853131 acc_train: 0.9556902985074627 loss_val: 0.1590703801232019 acc_val: 0.9427083333333334\n",
      "epoch 7 / 50 loss_train: 0.10896986058510061 acc_train: 0.9540578358208955 loss_val: 0.14667941505562598 acc_val: 0.9464962121212122\n",
      "epoch 8 / 50 loss_train: 0.10042952654410654 acc_train: 0.957206156716418 loss_val: 0.15443444408441737 acc_val: 0.9517045454545454\n",
      "epoch 9 / 50 loss_train: 0.10584376864969286 acc_train: 0.9554570895522388 loss_val: 0.12793250776380635 acc_val: 0.9540719696969697\n",
      "epoch 10 / 50 loss_train: 0.09134954325298765 acc_train: 0.9611707089552238 loss_val: 0.12919683738437016 acc_val: 0.9535984848484849\n",
      "epoch 11 / 50 loss_train: 0.08453925841474044 acc_train: 0.9616371268656716 loss_val: 0.12672839791401092 acc_val: 0.9488636363636364\n",
      "epoch 12 / 50 loss_train: 0.08774677041306425 acc_train: 0.9622201492537313 loss_val: 0.15024493503645345 acc_val: 0.9446022727272727\n",
      "epoch 13 / 50 loss_train: 0.08910649789691862 acc_train: 0.9623367537313433 loss_val: 0.1360666260060856 acc_val: 0.946969696969697\n",
      "epoch 14 / 50 loss_train: 0.07469002637011346 acc_train: 0.9681669776119403 loss_val: 0.13366983688912581 acc_val: 0.9517045454545454\n",
      "epoch 15 / 50 loss_train: 0.07113048674733337 acc_train: 0.9679337686567164 loss_val: 0.12853801578956225 acc_val: 0.9573863636363636\n",
      "epoch 16 / 50 loss_train: 0.07582262963918385 acc_train: 0.9679337686567164 loss_val: 0.13579181429513731 acc_val: 0.9540719696969697\n",
      "epoch 17 / 50 loss_train: 0.07295401182734589 acc_train: 0.9695662313432836 loss_val: 0.2258952986772761 acc_val: 0.9275568181818182\n",
      "epoch 18 / 50 loss_train: 0.0832742491942038 acc_train: 0.9643190298507462 loss_val: 0.16325409256387502 acc_val: 0.9526515151515151\n",
      "epoch 19 / 50 loss_train: 0.06855995608528437 acc_train: 0.9701492537313433 loss_val: 0.14934198855903422 acc_val: 0.9483901515151515\n",
      "epoch 20 / 50 loss_train: 0.11484485032704117 acc_train: 0.9594216417910447 loss_val: 0.14757159494823127 acc_val: 0.953125\n",
      "epoch 21 / 50 loss_train: 0.07333771853165617 acc_train: 0.9678171641791045 loss_val: 0.1594679235704737 acc_val: 0.9460227272727273\n",
      "epoch 22 / 50 loss_train: 0.07055968078159129 acc_train: 0.9690998134328358 loss_val: 0.17605159170331275 acc_val: 0.9412878787878788\n",
      "epoch 23 / 50 loss_train: 0.08663994068307664 acc_train: 0.9663013059701493 loss_val: 0.13443569018425128 acc_val: 0.9512310606060606\n",
      "epoch 24 / 50 loss_train: 0.07734318152452305 acc_train: 0.9681669776119403 loss_val: 0.1642446547134155 acc_val: 0.9474431818181818\n",
      "epoch 25 / 50 loss_train: 0.055970839446292386 acc_train: 0.976679104477612 loss_val: 0.15332820183637136 acc_val: 0.9569128787878788\n",
      "epoch 26 / 50 loss_train: 0.05777534739454902 acc_train: 0.9780783582089553 loss_val: 0.14862045890608 acc_val: 0.9540719696969697\n",
      "epoch 27 / 50 loss_train: 0.057514850912031845 acc_train: 0.9756296641791045 loss_val: 0.12583868147846783 acc_val: 0.9559659090909091\n",
      "epoch 28 / 50 loss_train: 0.05528144765586189 acc_train: 0.9773787313432836 loss_val: 0.13668368508355702 acc_val: 0.9545454545454546\n",
      "epoch 29 / 50 loss_train: 0.05346638837363571 acc_train: 0.9777285447761194 loss_val: 0.1673230595664362 acc_val: 0.9498106060606061\n",
      "epoch 30 / 50 loss_train: 0.04960240944941987 acc_train: 0.9794776119402985 loss_val: 0.177322280719333 acc_val: 0.9526515151515151\n",
      "epoch 31 / 50 loss_train: 0.06680593256592758 acc_train: 0.9777285447761194 loss_val: 0.1705224711323822 acc_val: 0.9483901515151515\n",
      "epoch 32 / 50 loss_train: 0.049229255048193234 acc_train: 0.9799440298507462 loss_val: 0.16642523632860198 acc_val: 0.9493371212121212\n",
      "epoch 33 / 50 loss_train: 0.05289443321143791 acc_train: 0.9788945895522388 loss_val: 0.1542213878385541 acc_val: 0.9517045454545454\n",
      "epoch 34 / 50 loss_train: 0.05684420619674249 acc_train: 0.9787779850746269 loss_val: 0.15576638828673767 acc_val: 0.9554924242424242\n",
      "epoch 35 / 50 loss_train: 0.06397547936171237 acc_train: 0.9770289179104478 loss_val: 0.1724629085969575 acc_val: 0.9498106060606061\n",
      "epoch 36 / 50 loss_train: 0.07510936343191024 acc_train: 0.9739972014925373 loss_val: 0.15759473032964447 acc_val: 0.9493371212121212\n",
      "epoch 37 / 50 loss_train: 0.05546120865750057 acc_train: 0.9771455223880597 loss_val: 0.17111783209524714 acc_val: 0.9588068181818182\n",
      "epoch 38 / 50 loss_train: 0.048639869164943055 acc_train: 0.9816930970149254 loss_val: 0.14824527062448783 acc_val: 0.959280303030303\n",
      "epoch 39 / 50 loss_train: 0.06970763419504597 acc_train: 0.9816930970149254 loss_val: 0.21821059103152066 acc_val: 0.9384469696969697\n",
      "epoch 40 / 50 loss_train: 0.063257871950636 acc_train: 0.9770289179104478 loss_val: 0.13276656252324975 acc_val: 0.9616477272727273\n",
      "epoch 41 / 50 loss_train: 0.04219958708281003 acc_train: 0.984258395522388 loss_val: 0.15161837074353543 acc_val: 0.9569128787878788\n",
      "epoch 42 / 50 loss_train: 0.04250507749807795 acc_train: 0.9850746268656716 loss_val: 0.1721257849624648 acc_val: 0.9588068181818182\n",
      "epoch 43 / 50 loss_train: 0.03784567713733144 acc_train: 0.9857742537313433 loss_val: 0.16642387042012766 acc_val: 0.9588068181818182\n",
      "epoch 44 / 50 loss_train: 0.04453183265938076 acc_train: 0.9830923507462687 loss_val: 0.157461397062317 acc_val: 0.9512310606060606\n",
      "epoch 45 / 50 loss_train: 0.051253841162803446 acc_train: 0.9811100746268657 loss_val: 0.1590115231758153 acc_val: 0.9550189393939394\n",
      "epoch 46 / 50 loss_train: 0.03220733898125394 acc_train: 0.9879897388059702 loss_val: 0.16709830592056582 acc_val: 0.9564393939393939\n",
      "epoch 47 / 50 loss_train: 0.026520296610690264 acc_train: 0.988456156716418 loss_val: 0.18844122057789442 acc_val: 0.9559659090909091\n",
      "epoch 48 / 50 loss_train: 0.025695022429629975 acc_train: 0.9905550373134329 loss_val: 0.16126868811833941 acc_val: 0.9573863636363636\n",
      "epoch 49 / 50 loss_train: 0.037220078346722606 acc_train: 0.9857742537313433 loss_val: 0.15609932800404033 acc_val: 0.9578598484848485\n",
      "Validation accuracy for lr 0.01 bs 64 hl [100, 50, 25] : 0.9578598484848485\n",
      "epoch 0 / 50 loss_train: 0.4127657738194537 acc_train: 0.8063199626865671 loss_val: 0.2314375353671494 acc_val: 0.90673828125\n",
      "epoch 1 / 50 loss_train: 0.18925326773479803 acc_train: 0.9226912313432836 loss_val: 0.1790102460035996 acc_val: 0.9287109375\n",
      "epoch 2 / 50 loss_train: 0.1512314268012545 acc_train: 0.9364505597014925 loss_val: 0.15961945113667753 acc_val: 0.94091796875\n",
      "epoch 3 / 50 loss_train: 0.13746608504608496 acc_train: 0.9429804104477612 loss_val: 0.15187703262199648 acc_val: 0.9423828125\n",
      "epoch 4 / 50 loss_train: 0.1198802572577747 acc_train: 0.9511427238805971 loss_val: 0.14739316867780872 acc_val: 0.943359375\n",
      "epoch 5 / 50 loss_train: 0.1215576841982443 acc_train: 0.9498600746268657 loss_val: 0.13648878551612142 acc_val: 0.94775390625\n",
      "epoch 6 / 50 loss_train: 0.10266830977886471 acc_train: 0.9567397388059702 loss_val: 0.13261204448645003 acc_val: 0.9482421875\n",
      "epoch 7 / 50 loss_train: 0.09604283133104666 acc_train: 0.9587220149253731 loss_val: 0.1264575835949664 acc_val: 0.94970703125\n",
      "epoch 8 / 50 loss_train: 0.0887705791018792 acc_train: 0.9633861940298507 loss_val: 0.1409754021151457 acc_val: 0.94580078125\n",
      "epoch 9 / 50 loss_train: 0.0909941463884133 acc_train: 0.9604710820895522 loss_val: 0.13654901119298302 acc_val: 0.94775390625\n",
      "epoch 10 / 50 loss_train: 0.08349707033207167 acc_train: 0.9660680970149254 loss_val: 0.14790741595788842 acc_val: 0.9482421875\n",
      "epoch 11 / 50 loss_train: 0.08466052302896086 acc_train: 0.9660680970149254 loss_val: 0.14919108495814726 acc_val: 0.95458984375\n",
      "epoch 12 / 50 loss_train: 0.07943394207465115 acc_train: 0.9642024253731343 loss_val: 0.1390272003918236 acc_val: 0.9521484375\n",
      "epoch 13 / 50 loss_train: 0.07141574372106524 acc_train: 0.9692164179104478 loss_val: 0.13743402629188473 acc_val: 0.9521484375\n",
      "epoch 14 / 50 loss_train: 0.07188847444173116 acc_train: 0.9681669776119403 loss_val: 0.15733802152681164 acc_val: 0.9521484375\n",
      "epoch 15 / 50 loss_train: 0.0646405339574636 acc_train: 0.9701492537313433 loss_val: 0.14892602794861887 acc_val: 0.951171875\n",
      "epoch 16 / 50 loss_train: 0.06437757286006834 acc_train: 0.9689832089552238 loss_val: 0.16318602529645432 acc_val: 0.9443359375\n",
      "epoch 17 / 50 loss_train: 0.06317052906796114 acc_train: 0.9730643656716418 loss_val: 0.13871656873652682 acc_val: 0.94921875\n",
      "epoch 18 / 50 loss_train: 0.06103186356598762 acc_train: 0.9737639925373134 loss_val: 0.15064411715138704 acc_val: 0.951171875\n",
      "epoch 19 / 50 loss_train: 0.07193572177037375 acc_train: 0.9701492537313433 loss_val: 0.17826336491998518 acc_val: 0.94580078125\n",
      "epoch 20 / 50 loss_train: 0.06724208832454326 acc_train: 0.9717817164179104 loss_val: 0.14430638656995143 acc_val: 0.95166015625\n",
      "epoch 21 / 50 loss_train: 0.07205362712491804 acc_train: 0.9708488805970149 loss_val: 0.14985709849315754 acc_val: 0.94287109375\n",
      "epoch 22 / 50 loss_train: 0.06323426216840744 acc_train: 0.9729477611940298 loss_val: 0.14760507868777495 acc_val: 0.94921875\n",
      "epoch 23 / 50 loss_train: 0.056603034013020456 acc_train: 0.9742304104477612 loss_val: 0.1469426766052493 acc_val: 0.9482421875\n",
      "epoch 24 / 50 loss_train: 0.05528952095157175 acc_train: 0.9748134328358209 loss_val: 0.1459607009346655 acc_val: 0.9501953125\n",
      "epoch 25 / 50 loss_train: 0.04712568907372987 acc_train: 0.9788945895522388 loss_val: 0.16273152024314186 acc_val: 0.94677734375\n",
      "epoch 26 / 50 loss_train: 0.04669167004080851 acc_train: 0.9792444029850746 loss_val: 0.1849343343592409 acc_val: 0.939453125\n",
      "epoch 27 / 50 loss_train: 0.0487430775955097 acc_train: 0.9776119402985075 loss_val: 0.19503384644394828 acc_val: 0.94091796875\n",
      "epoch 28 / 50 loss_train: 0.059459291231721195 acc_train: 0.9756296641791045 loss_val: 0.15612383714415046 acc_val: 0.94921875\n",
      "epoch 29 / 50 loss_train: 0.05541514578873097 acc_train: 0.9773787313432836 loss_val: 0.1622417355501966 acc_val: 0.953125\n",
      "epoch 30 / 50 loss_train: 0.0561362682049399 acc_train: 0.9750466417910447 loss_val: 0.1493764645841793 acc_val: 0.94677734375\n",
      "epoch 31 / 50 loss_train: 0.0536991688353357 acc_train: 0.9771455223880597 loss_val: 0.134823935566601 acc_val: 0.95751953125\n",
      "epoch 32 / 50 loss_train: 0.0582173767346722 acc_train: 0.9753964552238806 loss_val: 0.14079957092690165 acc_val: 0.9521484375\n",
      "epoch 33 / 50 loss_train: 0.04950311888398519 acc_train: 0.9777285447761194 loss_val: 0.15242727219811059 acc_val: 0.95654296875\n",
      "epoch 34 / 50 loss_train: 0.06879101432303884 acc_train: 0.9713152985074627 loss_val: 0.1577655793555266 acc_val: 0.9501953125\n",
      "epoch 35 / 50 loss_train: 0.05025630374786569 acc_train: 0.9783115671641791 loss_val: 0.1619036068495916 acc_val: 0.94921875\n",
      "epoch 36 / 50 loss_train: 0.03743768219293943 acc_train: 0.984375 loss_val: 0.15953838118116437 acc_val: 0.94921875\n",
      "epoch 37 / 50 loss_train: 0.038620345197173195 acc_train: 0.9822761194029851 loss_val: 0.15346749430295858 acc_val: 0.9541015625\n",
      "epoch 38 / 50 loss_train: 0.039998224839123325 acc_train: 0.9826259328358209 loss_val: 0.16437437362532137 acc_val: 0.9501953125\n",
      "epoch 39 / 50 loss_train: 0.05779783633440288 acc_train: 0.9760960820895522 loss_val: 0.21736160328782717 acc_val: 0.9453125\n",
      "epoch 40 / 50 loss_train: 0.06943526078682782 acc_train: 0.9702658582089553 loss_val: 0.1756507087047794 acc_val: 0.9462890625\n",
      "epoch 41 / 50 loss_train: 0.05299917880946131 acc_train: 0.9783115671641791 loss_val: 0.1756824286085248 acc_val: 0.95068359375\n",
      "epoch 42 / 50 loss_train: 0.042537460437239105 acc_train: 0.980410447761194 loss_val: 0.1597720629433752 acc_val: 0.94970703125\n",
      "epoch 43 / 50 loss_train: 0.03755712833033124 acc_train: 0.9835587686567164 loss_val: 0.15717847202267876 acc_val: 0.951171875\n",
      "epoch 44 / 50 loss_train: 0.04046692505979271 acc_train: 0.9827425373134329 loss_val: 0.16565899736247047 acc_val: 0.95166015625\n",
      "epoch 45 / 50 loss_train: 0.04948078000000609 acc_train: 0.9779617537313433 loss_val: 0.14297798475300283 acc_val: 0.95703125\n",
      "epoch 46 / 50 loss_train: 0.04180548342862236 acc_train: 0.9807602611940298 loss_val: 0.16269244894465373 acc_val: 0.94873046875\n",
      "epoch 47 / 50 loss_train: 0.040100195528522356 acc_train: 0.9819263059701493 loss_val: 0.15305821995389124 acc_val: 0.9521484375\n",
      "epoch 48 / 50 loss_train: 0.03599149885295487 acc_train: 0.9840251865671642 loss_val: 0.1569581261401254 acc_val: 0.95458984375\n",
      "epoch 49 / 50 loss_train: 0.03873302770861939 acc_train: 0.984491604477612 loss_val: 0.1994721843402658 acc_val: 0.95361328125\n",
      "Validation accuracy for lr 0.01 bs 128 hl [30, 15] : 0.95361328125\n",
      "epoch 0 / 50 loss_train: 0.34756645204415965 acc_train: 0.8540111940298507 loss_val: 0.20520353207393782 acc_val: 0.91748046875\n",
      "epoch 1 / 50 loss_train: 0.16986047999182743 acc_train: 0.9301539179104478 loss_val: 0.16751633152671275 acc_val: 0.9287109375\n",
      "epoch 2 / 50 loss_train: 0.12892742699651577 acc_train: 0.9441464552238806 loss_val: 0.1311104687401894 acc_val: 0.947265625\n",
      "epoch 3 / 50 loss_train: 0.11646305586197483 acc_train: 0.9496268656716418 loss_val: 0.1373767254999505 acc_val: 0.9462890625\n",
      "epoch 4 / 50 loss_train: 0.12770718413947232 acc_train: 0.9461287313432836 loss_val: 0.202520322665805 acc_val: 0.93212890625\n",
      "epoch 5 / 50 loss_train: 0.12094465064913479 acc_train: 0.9474113805970149 loss_val: 0.13595190658816136 acc_val: 0.94873046875\n",
      "epoch 6 / 50 loss_train: 0.10586309660948924 acc_train: 0.9538246268656716 loss_val: 0.13591503373754676 acc_val: 0.94873046875\n",
      "epoch 7 / 50 loss_train: 0.10093229553147928 acc_train: 0.9554570895522388 loss_val: 0.13340097501350084 acc_val: 0.951171875\n",
      "epoch 8 / 50 loss_train: 0.0853862899508494 acc_train: 0.961054104477612 loss_val: 0.1275170099106484 acc_val: 0.9541015625\n",
      "epoch 9 / 50 loss_train: 0.07364835631825141 acc_train: 0.9673507462686567 loss_val: 0.13089147835087012 acc_val: 0.94775390625\n",
      "epoch 10 / 50 loss_train: 0.0736391047543999 acc_train: 0.9680503731343284 loss_val: 0.12742673730097476 acc_val: 0.95361328125\n",
      "epoch 11 / 50 loss_train: 0.08102727370030845 acc_train: 0.9644356343283582 loss_val: 0.14007787391710735 acc_val: 0.95556640625\n",
      "epoch 12 / 50 loss_train: 0.08705387984527581 acc_train: 0.9614039179104478 loss_val: 0.13200876631248093 acc_val: 0.95458984375\n",
      "epoch 13 / 50 loss_train: 0.08819306319329276 acc_train: 0.9617537313432836 loss_val: 0.141284108547012 acc_val: 0.94775390625\n",
      "epoch 14 / 50 loss_train: 0.08049539715718868 acc_train: 0.9681669776119403 loss_val: 0.12672692395790364 acc_val: 0.95654296875\n",
      "epoch 15 / 50 loss_train: 0.06640533869391058 acc_train: 0.9722481343283582 loss_val: 0.15015021408089524 acc_val: 0.9541015625\n",
      "epoch 16 / 50 loss_train: 0.06571301182648584 acc_train: 0.9732975746268657 loss_val: 0.1509180015491438 acc_val: 0.95654296875\n",
      "epoch 17 / 50 loss_train: 0.06525129824876785 acc_train: 0.972481343283582 loss_val: 0.15474331920992945 acc_val: 0.95458984375\n",
      "epoch 18 / 50 loss_train: 0.07574910662178673 acc_train: 0.968633395522388 loss_val: 0.15788720204727724 acc_val: 0.947265625\n",
      "epoch 19 / 50 loss_train: 0.07721684439413583 acc_train: 0.9667677238805971 loss_val: 0.17374469368951395 acc_val: 0.9482421875\n",
      "epoch 20 / 50 loss_train: 0.07070214515412922 acc_train: 0.9708488805970149 loss_val: 0.15502597969273424 acc_val: 0.947265625\n",
      "epoch 21 / 50 loss_train: 0.058181300370106055 acc_train: 0.9760960820895522 loss_val: 0.1543681230978109 acc_val: 0.951171875\n",
      "epoch 22 / 50 loss_train: 0.0555569255835752 acc_train: 0.9774953358208955 loss_val: 0.14361651792381736 acc_val: 0.95849609375\n",
      "epoch 23 / 50 loss_train: 0.04580809552667301 acc_train: 0.9822761194029851 loss_val: 0.159034755713094 acc_val: 0.9501953125\n",
      "epoch 24 / 50 loss_train: 0.044595153460195706 acc_train: 0.9814598880597015 loss_val: 0.1477454632865829 acc_val: 0.9560546875\n",
      "epoch 25 / 50 loss_train: 0.05408259060011426 acc_train: 0.9769123134328358 loss_val: 0.17899653529093484 acc_val: 0.94873046875\n",
      "epoch 26 / 50 loss_train: 0.061101610633308316 acc_train: 0.9763292910447762 loss_val: 0.1543097347166622 acc_val: 0.95263671875\n",
      "epoch 27 / 50 loss_train: 0.06307845055334159 acc_train: 0.9742304104477612 loss_val: 0.1609454802237451 acc_val: 0.95068359375\n",
      "epoch 28 / 50 loss_train: 0.057951702889221815 acc_train: 0.9750466417910447 loss_val: 0.16842786879567484 acc_val: 0.9560546875\n",
      "epoch 29 / 50 loss_train: 0.07061303615458865 acc_train: 0.972831156716418 loss_val: 0.19706428622885142 acc_val: 0.955078125\n",
      "epoch 30 / 50 loss_train: 0.06083221085814398 acc_train: 0.9752798507462687 loss_val: 0.1583537398473469 acc_val: 0.9541015625\n",
      "epoch 31 / 50 loss_train: 0.06274486869684796 acc_train: 0.9737639925373134 loss_val: 0.1894148931669406 acc_val: 0.95068359375\n",
      "epoch 32 / 50 loss_train: 0.07264105961727563 acc_train: 0.9748134328358209 loss_val: 0.15495244622434257 acc_val: 0.9501953125\n",
      "epoch 33 / 50 loss_train: 0.050781428688831294 acc_train: 0.9769123134328358 loss_val: 0.1447568808143842 acc_val: 0.9619140625\n",
      "epoch 34 / 50 loss_train: 0.04177096217815111 acc_train: 0.9821595149253731 loss_val: 0.1413572623014261 acc_val: 0.95751953125\n",
      "epoch 35 / 50 loss_train: 0.03882100454656713 acc_train: 0.9833255597014925 loss_val: 0.1862272482278513 acc_val: 0.95751953125\n",
      "epoch 36 / 50 loss_train: 0.03897543765826902 acc_train: 0.9829757462686567 loss_val: 0.16463949304591097 acc_val: 0.9609375\n",
      "epoch 37 / 50 loss_train: 0.03610640501520082 acc_train: 0.984375 loss_val: 0.1715499070135138 acc_val: 0.95849609375\n",
      "epoch 38 / 50 loss_train: 0.04236442205120823 acc_train: 0.9812266791044776 loss_val: 0.1688939207670046 acc_val: 0.9560546875\n",
      "epoch 39 / 50 loss_train: 0.07773737589230956 acc_train: 0.9711986940298507 loss_val: 0.20586884408839978 acc_val: 0.9423828125\n",
      "epoch 40 / 50 loss_train: 0.08101787042817962 acc_train: 0.9715485074626866 loss_val: 0.15613656694677047 acc_val: 0.95654296875\n",
      "epoch 41 / 50 loss_train: 0.06093406447890534 acc_train: 0.9765625 loss_val: 0.15205978437370504 acc_val: 0.953125\n",
      "epoch 42 / 50 loss_train: 0.049644001592784676 acc_train: 0.9813432835820896 loss_val: 0.16562079903724214 acc_val: 0.9560546875\n",
      "epoch 43 / 50 loss_train: 0.037855780196946055 acc_train: 0.9833255597014925 loss_val: 0.17903631179768809 acc_val: 0.95654296875\n",
      "epoch 44 / 50 loss_train: 0.0362796601978367 acc_train: 0.9841417910447762 loss_val: 0.1757461563288416 acc_val: 0.9560546875\n",
      "epoch 45 / 50 loss_train: 0.03642431054431111 acc_train: 0.9840251865671642 loss_val: 0.17469530182097515 acc_val: 0.9560546875\n",
      "epoch 46 / 50 loss_train: 0.038855407470420225 acc_train: 0.9816930970149254 loss_val: 0.16572115737653803 acc_val: 0.95458984375\n",
      "epoch 47 / 50 loss_train: 0.03814878232721517 acc_train: 0.9846082089552238 loss_val: 0.18606049510765388 acc_val: 0.96142578125\n",
      "epoch 48 / 50 loss_train: 0.0324355906097969 acc_train: 0.9864738805970149 loss_val: 0.18357300546779243 acc_val: 0.95751953125\n",
      "epoch 49 / 50 loss_train: 0.03749392065567089 acc_train: 0.9834421641791045 loss_val: 0.23155680269689324 acc_val: 0.953125\n",
      "Validation accuracy for lr 0.01 bs 128 hl [50, 25] : 0.953125\n",
      "epoch 0 / 50 loss_train: 0.37194452623822793 acc_train: 0.8264925373134329 loss_val: 0.2095512240791777 acc_val: 0.91748046875\n",
      "epoch 1 / 50 loss_train: 0.19204668589492344 acc_train: 0.921991604477612 loss_val: 0.1779913997161202 acc_val: 0.9296875\n",
      "epoch 2 / 50 loss_train: 0.14772025039836542 acc_train: 0.9364505597014925 loss_val: 0.1528486501701991 acc_val: 0.93798828125\n",
      "epoch 3 / 50 loss_train: 0.1729262636891052 acc_train: 0.9322527985074627 loss_val: 0.20732364295417938 acc_val: 0.9111328125\n",
      "epoch 4 / 50 loss_train: 0.13960711924887415 acc_train: 0.9406483208955224 loss_val: 0.14327362794801957 acc_val: 0.94482421875\n",
      "epoch 5 / 50 loss_train: 0.11832997530921181 acc_train: 0.949160447761194 loss_val: 0.14446797527461008 acc_val: 0.94580078125\n",
      "epoch 6 / 50 loss_train: 0.11082822473636314 acc_train: 0.9514925373134329 loss_val: 0.13123640169214923 acc_val: 0.94970703125\n",
      "epoch 7 / 50 loss_train: 0.10007416923989111 acc_train: 0.953241604477612 loss_val: 0.1264532830718963 acc_val: 0.953125\n",
      "epoch 8 / 50 loss_train: 0.08902547658601803 acc_train: 0.9607042910447762 loss_val: 0.1282912842434598 acc_val: 0.951171875\n",
      "epoch 9 / 50 loss_train: 0.08279650424843404 acc_train: 0.9605876865671642 loss_val: 0.1489683693725965 acc_val: 0.95166015625\n",
      "epoch 10 / 50 loss_train: 0.08757268109205943 acc_train: 0.9615205223880597 loss_val: 0.13401015337149147 acc_val: 0.95263671875\n",
      "epoch 11 / 50 loss_train: 0.0972459882275382 acc_train: 0.9575559701492538 loss_val: 0.23684319800668163 acc_val: 0.9208984375\n",
      "epoch 12 / 50 loss_train: 0.10640795698472813 acc_train: 0.9514925373134329 loss_val: 0.13275420493980583 acc_val: 0.9521484375\n",
      "epoch 13 / 50 loss_train: 0.08407391293613768 acc_train: 0.9635027985074627 loss_val: 0.13209658926643386 acc_val: 0.94482421875\n",
      "epoch 14 / 50 loss_train: 0.07174952636792589 acc_train: 0.9674673507462687 loss_val: 0.12409595769713633 acc_val: 0.953125\n",
      "epoch 15 / 50 loss_train: 0.06458644488298182 acc_train: 0.9682835820895522 loss_val: 0.12257232728370582 acc_val: 0.95458984375\n",
      "epoch 16 / 50 loss_train: 0.06950379527215637 acc_train: 0.9685167910447762 loss_val: 0.12820197279961576 acc_val: 0.95263671875\n",
      "epoch 17 / 50 loss_train: 0.07473548012438105 acc_train: 0.96875 loss_val: 0.1995782457055384 acc_val: 0.939453125\n",
      "epoch 18 / 50 loss_train: 0.09306187407850329 acc_train: 0.9604710820895522 loss_val: 0.1424764487892386 acc_val: 0.9462890625\n",
      "epoch 19 / 50 loss_train: 0.07110512765374646 acc_train: 0.964785447761194 loss_val: 0.11728510541642123 acc_val: 0.96044921875\n",
      "epoch 20 / 50 loss_train: 0.05658273454477538 acc_train: 0.9739972014925373 loss_val: 0.14201805680932011 acc_val: 0.953125\n",
      "epoch 21 / 50 loss_train: 0.05605083881919064 acc_train: 0.9741138059701493 loss_val: 0.20385049443939351 acc_val: 0.935546875\n",
      "epoch 22 / 50 loss_train: 0.06650215075976813 acc_train: 0.9713152985074627 loss_val: 0.15364633413628326 acc_val: 0.9501953125\n",
      "epoch 23 / 50 loss_train: 0.05008612888684468 acc_train: 0.9783115671641791 loss_val: 0.14345122419399559 acc_val: 0.95703125\n",
      "epoch 24 / 50 loss_train: 0.057328966460121214 acc_train: 0.9745802238805971 loss_val: 0.15481145842750266 acc_val: 0.9482421875\n",
      "epoch 25 / 50 loss_train: 0.056101484516107324 acc_train: 0.9757462686567164 loss_val: 0.16182074834794946 acc_val: 0.943359375\n",
      "epoch 26 / 50 loss_train: 0.05243190672637811 acc_train: 0.9763292910447762 loss_val: 0.17514913580089342 acc_val: 0.94921875\n",
      "epoch 27 / 50 loss_train: 0.06168572956334744 acc_train: 0.9744636194029851 loss_val: 0.14259528106776997 acc_val: 0.94482421875\n",
      "epoch 28 / 50 loss_train: 0.050534107041225504 acc_train: 0.9790111940298507 loss_val: 0.16185509782906138 acc_val: 0.95166015625\n",
      "epoch 29 / 50 loss_train: 0.05964505999112752 acc_train: 0.9731809701492538 loss_val: 0.15905766651417252 acc_val: 0.95458984375\n",
      "epoch 30 / 50 loss_train: 0.04886693795169912 acc_train: 0.9776119402985075 loss_val: 0.15146608193208522 acc_val: 0.9541015625\n",
      "epoch 31 / 50 loss_train: 0.04848987914955438 acc_train: 0.9779617537313433 loss_val: 0.1516896859939152 acc_val: 0.95361328125\n",
      "epoch 32 / 50 loss_train: 0.046122246852784016 acc_train: 0.9812266791044776 loss_val: 0.18645778325299034 acc_val: 0.958984375\n",
      "epoch 33 / 50 loss_train: 0.03636890762388261 acc_train: 0.9840251865671642 loss_val: 0.21257085566469414 acc_val: 0.94970703125\n",
      "epoch 34 / 50 loss_train: 0.05163407248597759 acc_train: 0.9812266791044776 loss_val: 0.17525044007900625 acc_val: 0.94970703125\n",
      "epoch 35 / 50 loss_train: 0.05173000124797447 acc_train: 0.9787779850746269 loss_val: 0.1781171048235053 acc_val: 0.9501953125\n",
      "epoch 36 / 50 loss_train: 0.033918047296022304 acc_train: 0.9860074626865671 loss_val: 0.1725140779000327 acc_val: 0.9560546875\n",
      "epoch 37 / 50 loss_train: 0.03662108045432772 acc_train: 0.9853078358208955 loss_val: 0.18391031552528148 acc_val: 0.95166015625\n",
      "epoch 38 / 50 loss_train: 0.045598333663265427 acc_train: 0.9823927238805971 loss_val: 0.17574124773636868 acc_val: 0.95166015625\n",
      "epoch 39 / 50 loss_train: 0.04928161109338945 acc_train: 0.9808768656716418 loss_val: 0.2228340433468361 acc_val: 0.947265625\n",
      "epoch 40 / 50 loss_train: 0.043984741903841496 acc_train: 0.9827425373134329 loss_val: 0.19224644803933444 acc_val: 0.943359375\n",
      "epoch 41 / 50 loss_train: 0.038350894428622814 acc_train: 0.984258395522388 loss_val: 0.24669473871973926 acc_val: 0.94580078125\n",
      "epoch 42 / 50 loss_train: 0.050980373560242466 acc_train: 0.9786613805970149 loss_val: 0.2353807117717679 acc_val: 0.94091796875\n",
      "epoch 43 / 50 loss_train: 0.04357703312286245 acc_train: 0.9827425373134329 loss_val: 0.23230963629748613 acc_val: 0.94921875\n",
      "epoch 44 / 50 loss_train: 0.059951369052947456 acc_train: 0.9750466417910447 loss_val: 0.19576499194954522 acc_val: 0.9501953125\n",
      "epoch 45 / 50 loss_train: 0.05638933618928292 acc_train: 0.9784281716417911 loss_val: 0.2003267139225422 acc_val: 0.94482421875\n",
      "epoch 46 / 50 loss_train: 0.04867910428334083 acc_train: 0.9815764925373134 loss_val: 0.2025481531763944 acc_val: 0.94580078125\n",
      "epoch 47 / 50 loss_train: 0.04491253981519657 acc_train: 0.980527052238806 loss_val: 0.22667387084219115 acc_val: 0.9443359375\n",
      "epoch 48 / 50 loss_train: 0.054227427454359496 acc_train: 0.9778451492537313 loss_val: 0.17402312179314094 acc_val: 0.95654296875\n",
      "epoch 49 / 50 loss_train: 0.03662593538330784 acc_train: 0.9828591417910447 loss_val: 0.1934792110947381 acc_val: 0.95263671875\n",
      "Validation accuracy for lr 0.01 bs 128 hl [100, 50, 25] : 0.95263671875\n",
      "epoch 0 / 50 loss_train: 0.5953377560893102 acc_train: 0.7197994402985075 loss_val: 0.2808765746486276 acc_val: 0.8969216417910447\n",
      "epoch 1 / 50 loss_train: 0.242934106932536 acc_train: 0.9086986940298507 loss_val: 0.20018019874357798 acc_val: 0.9225746268656716\n",
      "epoch 2 / 50 loss_train: 0.188959093875627 acc_train: 0.925839552238806 loss_val: 0.16874513828626542 acc_val: 0.9365671641791045\n",
      "epoch 3 / 50 loss_train: 0.16120442364420465 acc_train: 0.9361007462686567 loss_val: 0.15441260060187734 acc_val: 0.9416977611940298\n",
      "epoch 4 / 50 loss_train: 0.1445875294270244 acc_train: 0.941347947761194 loss_val: 0.1467013782476259 acc_val: 0.9440298507462687\n",
      "epoch 5 / 50 loss_train: 0.13242318241078574 acc_train: 0.9471781716417911 loss_val: 0.14088007092613591 acc_val: 0.9435634328358209\n",
      "epoch 6 / 50 loss_train: 0.1227817384546984 acc_train: 0.9499766791044776 loss_val: 0.1359710051809957 acc_val: 0.9496268656716418\n",
      "epoch 7 / 50 loss_train: 0.11460144964477686 acc_train: 0.9538246268656716 loss_val: 0.13252809235716234 acc_val: 0.9514925373134329\n",
      "epoch 8 / 50 loss_train: 0.10769622970143083 acc_train: 0.9573227611940298 loss_val: 0.12987500731914725 acc_val: 0.9524253731343284\n",
      "epoch 9 / 50 loss_train: 0.10150778895320454 acc_train: 0.9587220149253731 loss_val: 0.12794973924992656 acc_val: 0.9538246268656716\n",
      "epoch 10 / 50 loss_train: 0.09617431406904735 acc_train: 0.9615205223880597 loss_val: 0.12520808688523954 acc_val: 0.9528917910447762\n",
      "epoch 11 / 50 loss_train: 0.09041082923154611 acc_train: 0.9625699626865671 loss_val: 0.12347437326331293 acc_val: 0.9547574626865671\n",
      "epoch 12 / 50 loss_train: 0.08590232754220019 acc_train: 0.9656016791044776 loss_val: 0.12499432184596844 acc_val: 0.9547574626865671\n",
      "epoch 13 / 50 loss_train: 0.08207148148803346 acc_train: 0.9672341417910447 loss_val: 0.1239341760132818 acc_val: 0.9575559701492538\n",
      "epoch 14 / 50 loss_train: 0.07860498410587047 acc_train: 0.9680503731343284 loss_val: 0.1245689658963188 acc_val: 0.9575559701492538\n",
      "epoch 15 / 50 loss_train: 0.075279862063923 acc_train: 0.9704990671641791 loss_val: 0.1255410253693162 acc_val: 0.9528917910447762\n",
      "epoch 16 / 50 loss_train: 0.07223557179390605 acc_train: 0.9718983208955224 loss_val: 0.12389634085928099 acc_val: 0.9542910447761194\n",
      "epoch 17 / 50 loss_train: 0.06930052841328946 acc_train: 0.9723647388059702 loss_val: 0.12470365349651714 acc_val: 0.9533582089552238\n",
      "epoch 18 / 50 loss_train: 0.06691835558001967 acc_train: 0.9736473880597015 loss_val: 0.1264020004424211 acc_val: 0.9547574626865671\n",
      "epoch 19 / 50 loss_train: 0.06487681938086831 acc_train: 0.9742304104477612 loss_val: 0.12747098378042346 acc_val: 0.9538246268656716\n",
      "epoch 20 / 50 loss_train: 0.06318448510050509 acc_train: 0.9745802238805971 loss_val: 0.12701183551116732 acc_val: 0.9538246268656716\n",
      "epoch 21 / 50 loss_train: 0.06028771213579005 acc_train: 0.976679104477612 loss_val: 0.12729357988574427 acc_val: 0.9533582089552238\n",
      "epoch 22 / 50 loss_train: 0.057967420548388264 acc_train: 0.9769123134328358 loss_val: 0.12967771161682204 acc_val: 0.9542910447761194\n",
      "epoch 23 / 50 loss_train: 0.05642726983564016 acc_train: 0.9772621268656716 loss_val: 0.12970141758168832 acc_val: 0.9519589552238806\n",
      "epoch 24 / 50 loss_train: 0.05515669463045979 acc_train: 0.9779617537313433 loss_val: 0.133509001245191 acc_val: 0.9510261194029851\n",
      "epoch 25 / 50 loss_train: 0.053179313734941766 acc_train: 0.9800606343283582 loss_val: 0.1313896303270662 acc_val: 0.9552238805970149\n",
      "epoch 26 / 50 loss_train: 0.050901703617088984 acc_train: 0.9801772388059702 loss_val: 0.13318256090278255 acc_val: 0.9519589552238806\n",
      "epoch 27 / 50 loss_train: 0.04884763857664248 acc_train: 0.9808768656716418 loss_val: 0.13574042265119834 acc_val: 0.9533582089552238\n",
      "epoch 28 / 50 loss_train: 0.04793077845316651 acc_train: 0.9812266791044776 loss_val: 0.13654382174216823 acc_val: 0.9524253731343284\n",
      "epoch 29 / 50 loss_train: 0.047845265448637264 acc_train: 0.980527052238806 loss_val: 0.13897672791857987 acc_val: 0.9547574626865671\n",
      "epoch 30 / 50 loss_train: 0.04523203781056183 acc_train: 0.9820429104477612 loss_val: 0.14494379141479496 acc_val: 0.9538246268656716\n",
      "epoch 31 / 50 loss_train: 0.051554420746822927 acc_train: 0.9788945895522388 loss_val: 0.15571042152320966 acc_val: 0.9528917910447762\n",
      "epoch 32 / 50 loss_train: 0.04916812117631137 acc_train: 0.9797108208955224 loss_val: 0.1413204241733507 acc_val: 0.9552238805970149\n",
      "epoch 33 / 50 loss_train: 0.04255642054894411 acc_train: 0.9836753731343284 loss_val: 0.1498225502751484 acc_val: 0.9542910447761194\n",
      "epoch 34 / 50 loss_train: 0.040777082531539194 acc_train: 0.9829757462686567 loss_val: 0.15827077967292663 acc_val: 0.9514925373134329\n",
      "epoch 35 / 50 loss_train: 0.03867446541427391 acc_train: 0.9854244402985075 loss_val: 0.16120794474701747 acc_val: 0.9528917910447762\n",
      "epoch 36 / 50 loss_train: 0.038381602983151165 acc_train: 0.9858908582089553 loss_val: 0.1833336039296184 acc_val: 0.9482276119402985\n",
      "epoch 37 / 50 loss_train: 0.039065057325021385 acc_train: 0.9848414179104478 loss_val: 0.17251631122265237 acc_val: 0.9505597014925373\n",
      "epoch 38 / 50 loss_train: 0.03612561031207121 acc_train: 0.9872901119402985 loss_val: 0.17756461438889212 acc_val: 0.949160447761194\n",
      "epoch 39 / 50 loss_train: 0.03455346335482969 acc_train: 0.9868236940298507 loss_val: 0.1846207928103051 acc_val: 0.9463619402985075\n",
      "epoch 40 / 50 loss_train: 0.035020320790130455 acc_train: 0.9862406716417911 loss_val: 0.18495352483446167 acc_val: 0.9477611940298507\n",
      "epoch 41 / 50 loss_train: 0.03388147212226342 acc_train: 0.9870569029850746 loss_val: 0.19504586214465333 acc_val: 0.949160447761194\n",
      "epoch 42 / 50 loss_train: 0.034709779276454356 acc_train: 0.9865904850746269 loss_val: 0.20637349367460475 acc_val: 0.9440298507462687\n",
      "epoch 43 / 50 loss_train: 0.03292275698767122 acc_train: 0.9870569029850746 loss_val: 0.1991114689689609 acc_val: 0.9444962686567164\n",
      "epoch 44 / 50 loss_train: 0.03286659312690782 acc_train: 0.9879897388059702 loss_val: 0.20398315642067708 acc_val: 0.9482276119402985\n",
      "epoch 45 / 50 loss_train: 0.03242502217203289 acc_train: 0.9870569029850746 loss_val: 0.20141987751637594 acc_val: 0.9435634328358209\n",
      "epoch 46 / 50 loss_train: 0.031471439235382835 acc_train: 0.9872901119402985 loss_val: 0.20777368632716403 acc_val: 0.9477611940298507\n",
      "epoch 47 / 50 loss_train: 0.031131710898962547 acc_train: 0.9872901119402985 loss_val: 0.21141870688433081 acc_val: 0.9449626865671642\n",
      "epoch 48 / 50 loss_train: 0.029281877991204748 acc_train: 0.9893889925373134 loss_val: 0.22084986674734544 acc_val: 0.9421641791044776\n",
      "epoch 49 / 50 loss_train: 0.027683076097809366 acc_train: 0.9895055970149254 loss_val: 0.23381096264527673 acc_val: 0.941231343283582\n",
      "Validation accuracy for lr 0.001 bs 32 hl [30, 15] : 0.941231343283582\n",
      "epoch 0 / 50 loss_train: 0.47245899629570653 acc_train: 0.8229944029850746 loss_val: 0.2392852657210471 acc_val: 0.9095149253731343\n",
      "epoch 1 / 50 loss_train: 0.20849655770154588 acc_train: 0.9204757462686567 loss_val: 0.17782954239884197 acc_val: 0.9351679104477612\n",
      "epoch 2 / 50 loss_train: 0.16137413050868174 acc_train: 0.9371501865671642 loss_val: 0.15113899417728907 acc_val: 0.9402985074626866\n",
      "epoch 3 / 50 loss_train: 0.13651073284085785 acc_train: 0.9447294776119403 loss_val: 0.1352050026226376 acc_val: 0.9472947761194029\n",
      "epoch 4 / 50 loss_train: 0.12061452435037649 acc_train: 0.9500932835820896 loss_val: 0.12644785618841972 acc_val: 0.9528917910447762\n",
      "epoch 5 / 50 loss_train: 0.10927348592624402 acc_train: 0.9542910447761194 loss_val: 0.11991283793833557 acc_val: 0.9542910447761194\n",
      "epoch 6 / 50 loss_train: 0.10006785438458587 acc_train: 0.9580223880597015 loss_val: 0.11597467239358787 acc_val: 0.9538246268656716\n",
      "epoch 7 / 50 loss_train: 0.09237269788577374 acc_train: 0.9615205223880597 loss_val: 0.11319315027031919 acc_val: 0.9538246268656716\n",
      "epoch 8 / 50 loss_train: 0.08604921538507872 acc_train: 0.9637360074626866 loss_val: 0.1110811690091001 acc_val: 0.9547574626865671\n",
      "epoch 9 / 50 loss_train: 0.08083765183179292 acc_train: 0.9651352611940298 loss_val: 0.11086246240239327 acc_val: 0.9552238805970149\n",
      "epoch 10 / 50 loss_train: 0.07611442804718807 acc_train: 0.9684001865671642 loss_val: 0.10898509340938867 acc_val: 0.9556902985074627\n",
      "epoch 11 / 50 loss_train: 0.07146992468893695 acc_train: 0.9702658582089553 loss_val: 0.10766135625888601 acc_val: 0.9552238805970149\n",
      "epoch 12 / 50 loss_train: 0.06799821434029962 acc_train: 0.9730643656716418 loss_val: 0.10920058649369807 acc_val: 0.9561567164179104\n",
      "epoch 13 / 50 loss_train: 0.06401641147364098 acc_train: 0.9758628731343284 loss_val: 0.1123924868368573 acc_val: 0.9547574626865671\n",
      "epoch 14 / 50 loss_train: 0.060586496119026036 acc_train: 0.976445895522388 loss_val: 0.11033053109569217 acc_val: 0.9566231343283582\n",
      "epoch 15 / 50 loss_train: 0.05784527048031598 acc_train: 0.9770289179104478 loss_val: 0.11205982601366701 acc_val: 0.9561567164179104\n",
      "epoch 16 / 50 loss_train: 0.05480720806144067 acc_train: 0.9780783582089553 loss_val: 0.11236203081967645 acc_val: 0.9566231343283582\n",
      "epoch 17 / 50 loss_train: 0.05299342073595287 acc_train: 0.9786613805970149 loss_val: 0.1152924981327868 acc_val: 0.9556902985074627\n",
      "epoch 18 / 50 loss_train: 0.05308109130408466 acc_train: 0.9792444029850746 loss_val: 0.11359526743558952 acc_val: 0.9561567164179104\n",
      "epoch 19 / 50 loss_train: 0.048034689028303434 acc_train: 0.980410447761194 loss_val: 0.11708074439395108 acc_val: 0.9547574626865671\n",
      "epoch 20 / 50 loss_train: 0.04424752682261082 acc_train: 0.9819263059701493 loss_val: 0.11732641903547243 acc_val: 0.9566231343283582\n",
      "epoch 21 / 50 loss_train: 0.042170640399300324 acc_train: 0.9826259328358209 loss_val: 0.12060163721982292 acc_val: 0.9542910447761194\n",
      "epoch 22 / 50 loss_train: 0.0402859873172576 acc_train: 0.984258395522388 loss_val: 0.11951923901792635 acc_val: 0.957089552238806\n",
      "epoch 23 / 50 loss_train: 0.038148158941139494 acc_train: 0.9848414179104478 loss_val: 0.12698363721232286 acc_val: 0.957089552238806\n",
      "epoch 24 / 50 loss_train: 0.03610250055569497 acc_train: 0.9863572761194029 loss_val: 0.12441380678265747 acc_val: 0.9552238805970149\n",
      "epoch 25 / 50 loss_train: 0.043997709176424066 acc_train: 0.9833255597014925 loss_val: 0.14602207825688615 acc_val: 0.9533582089552238\n",
      "epoch 26 / 50 loss_train: 0.03533330758765979 acc_train: 0.9862406716417911 loss_val: 0.12369706637721405 acc_val: 0.9589552238805971\n",
      "epoch 27 / 50 loss_train: 0.03358352960520876 acc_train: 0.9871735074626866 loss_val: 0.1378823067521428 acc_val: 0.9575559701492538\n",
      "epoch 28 / 50 loss_train: 0.03046297152881135 acc_train: 0.9895055970149254 loss_val: 0.13027802057528237 acc_val: 0.9589552238805971\n",
      "epoch 29 / 50 loss_train: 0.027988276913511887 acc_train: 0.9902052238805971 loss_val: 0.13134512407242377 acc_val: 0.9589552238805971\n",
      "epoch 30 / 50 loss_train: 0.02756154820175951 acc_train: 0.9905550373134329 loss_val: 0.12710404444941759 acc_val: 0.960820895522388\n",
      "epoch 31 / 50 loss_train: 0.03352716634232019 acc_train: 0.9878731343283582 loss_val: 0.13581431161298782 acc_val: 0.9589552238805971\n",
      "epoch 32 / 50 loss_train: 0.030141148839033828 acc_train: 0.9890391791044776 loss_val: 0.15202038529012768 acc_val: 0.9580223880597015\n",
      "epoch 33 / 50 loss_train: 0.03733398306310357 acc_train: 0.984258395522388 loss_val: 0.15685580987265368 acc_val: 0.9538246268656716\n",
      "epoch 34 / 50 loss_train: 0.04467024556633173 acc_train: 0.9849580223880597 loss_val: 0.16141645331921833 acc_val: 0.9561567164179104\n",
      "epoch 35 / 50 loss_train: 0.0283027649281 acc_train: 0.9905550373134329 loss_val: 0.1784737326168535 acc_val: 0.9533582089552238\n",
      "epoch 36 / 50 loss_train: 0.02607964001397894 acc_train: 0.9906716417910447 loss_val: 0.18105125353351972 acc_val: 0.9528917910447762\n",
      "epoch 37 / 50 loss_train: 0.0238505861736702 acc_train: 0.9907882462686567 loss_val: 0.19886291117698238 acc_val: 0.9472947761194029\n",
      "epoch 38 / 50 loss_train: 0.022449013061046953 acc_train: 0.9916044776119403 loss_val: 0.21386237015445775 acc_val: 0.9458955223880597\n",
      "epoch 39 / 50 loss_train: 0.022868981033006188 acc_train: 0.9911380597014925 loss_val: 0.23349988386657053 acc_val: 0.9416977611940298\n",
      "epoch 40 / 50 loss_train: 0.02579346003515082 acc_train: 0.9899720149253731 loss_val: 0.21604584723776094 acc_val: 0.9477611940298507\n",
      "epoch 41 / 50 loss_train: 0.020339950963227865 acc_train: 0.9921875 loss_val: 0.216475368066564 acc_val: 0.9482276119402985\n",
      "epoch 42 / 50 loss_train: 0.020106183599989946 acc_train: 0.9918376865671642 loss_val: 0.2190599889023339 acc_val: 0.9426305970149254\n",
      "epoch 43 / 50 loss_train: 0.018767034557796835 acc_train: 0.9931203358208955 loss_val: 0.23039360970890657 acc_val: 0.9463619402985075\n",
      "epoch 44 / 50 loss_train: 0.022464570242313294 acc_train: 0.9916044776119403 loss_val: 0.2439382584627116 acc_val: 0.9440298507462687\n",
      "epoch 45 / 50 loss_train: 0.03052116165087967 acc_train: 0.9878731343283582 loss_val: 0.24131215277668575 acc_val: 0.9463619402985075\n",
      "epoch 46 / 50 loss_train: 0.02285706810943705 acc_train: 0.9909048507462687 loss_val: 0.2503693883906844 acc_val: 0.9463619402985075\n",
      "epoch 47 / 50 loss_train: 0.017535244179719064 acc_train: 0.9942863805970149 loss_val: 0.24912748897414388 acc_val: 0.9444962686567164\n",
      "epoch 48 / 50 loss_train: 0.016572398670279598 acc_train: 0.9940531716417911 loss_val: 0.26292288519543905 acc_val: 0.9435634328358209\n",
      "epoch 49 / 50 loss_train: 0.014972022501931644 acc_train: 0.9947527985074627 loss_val: 0.2540665823968888 acc_val: 0.9468283582089553\n",
      "Validation accuracy for lr 0.001 bs 32 hl [50, 25] : 0.9468283582089553\n",
      "epoch 0 / 50 loss_train: 0.39866057857037035 acc_train: 0.8288246268656716 loss_val: 0.20478759366852134 acc_val: 0.9183768656716418\n",
      "epoch 1 / 50 loss_train: 0.1714885724958644 acc_train: 0.9299207089552238 loss_val: 0.15587676698949018 acc_val: 0.9370335820895522\n",
      "epoch 2 / 50 loss_train: 0.13118405844112718 acc_train: 0.9456623134328358 loss_val: 0.14581301366202995 acc_val: 0.941231343283582\n",
      "epoch 3 / 50 loss_train: 0.11254851306343812 acc_train: 0.9541744402985075 loss_val: 0.13156051381773023 acc_val: 0.9472947761194029\n",
      "epoch 4 / 50 loss_train: 0.10060694207673643 acc_train: 0.9589552238805971 loss_val: 0.12088132008753108 acc_val: 0.9552238805970149\n",
      "epoch 5 / 50 loss_train: 0.0902793410637263 acc_train: 0.9628031716417911 loss_val: 0.12062799404395465 acc_val: 0.9566231343283582\n",
      "epoch 6 / 50 loss_train: 0.08303871582811281 acc_train: 0.9657182835820896 loss_val: 0.12570070170293995 acc_val: 0.9524253731343284\n",
      "epoch 7 / 50 loss_train: 0.07749001076032318 acc_train: 0.9668843283582089 loss_val: 0.11982211831947545 acc_val: 0.9533582089552238\n",
      "epoch 8 / 50 loss_train: 0.07928498348902299 acc_train: 0.9679337686567164 loss_val: 0.12417284562946977 acc_val: 0.9547574626865671\n",
      "epoch 9 / 50 loss_train: 0.06531036731616746 acc_train: 0.9734141791044776 loss_val: 0.11964976464210551 acc_val: 0.9552238805970149\n",
      "epoch 10 / 50 loss_train: 0.060166887784769425 acc_train: 0.9762126865671642 loss_val: 0.12364479346713003 acc_val: 0.9584888059701493\n",
      "epoch 11 / 50 loss_train: 0.056126740401212835 acc_train: 0.9771455223880597 loss_val: 0.1274558322266549 acc_val: 0.9542910447761194\n",
      "epoch 12 / 50 loss_train: 0.04934821674494253 acc_train: 0.980527052238806 loss_val: 0.1272920627867795 acc_val: 0.9575559701492538\n",
      "epoch 13 / 50 loss_train: 0.05834907241215158 acc_train: 0.9770289179104478 loss_val: 0.14403822461871849 acc_val: 0.9556902985074627\n",
      "epoch 14 / 50 loss_train: 0.05833583091057267 acc_train: 0.9770289179104478 loss_val: 0.11612496372669481 acc_val: 0.9594216417910447\n",
      "epoch 15 / 50 loss_train: 0.04701718499075184 acc_train: 0.9819263059701493 loss_val: 0.1322921890585878 acc_val: 0.9594216417910447\n",
      "epoch 16 / 50 loss_train: 0.04049853031191655 acc_train: 0.9847248134328358 loss_val: 0.13207296516072597 acc_val: 0.9589552238805971\n",
      "epoch 17 / 50 loss_train: 0.03698610342152238 acc_train: 0.9858908582089553 loss_val: 0.14727837216807213 acc_val: 0.9598880597014925\n",
      "epoch 18 / 50 loss_train: 0.03494800205649582 acc_train: 0.9863572761194029 loss_val: 0.13732573553491662 acc_val: 0.9594216417910447\n",
      "epoch 19 / 50 loss_train: 0.037618413550547694 acc_train: 0.9851912313432836 loss_val: 0.13730244221358637 acc_val: 0.9594216417910447\n",
      "epoch 20 / 50 loss_train: 0.03146061007341067 acc_train: 0.9875233208955224 loss_val: 0.1586743755329453 acc_val: 0.9584888059701493\n",
      "epoch 21 / 50 loss_train: 0.03949431327364708 acc_train: 0.9853078358208955 loss_val: 0.17756728245711026 acc_val: 0.9533582089552238\n",
      "epoch 22 / 50 loss_train: 0.04138987554894689 acc_train: 0.9850746268656716 loss_val: 0.15860699792398278 acc_val: 0.9566231343283582\n",
      "epoch 23 / 50 loss_train: 0.033944749387833935 acc_train: 0.9871735074626866 loss_val: 0.1842255683000852 acc_val: 0.9500932835820896\n",
      "epoch 24 / 50 loss_train: 0.032902964084273865 acc_train: 0.9900886194029851 loss_val: 0.1773067564941515 acc_val: 0.9510261194029851\n",
      "epoch 25 / 50 loss_train: 0.027125325756019468 acc_train: 0.9890391791044776 loss_val: 0.2067186845306643 acc_val: 0.9468283582089553\n",
      "epoch 26 / 50 loss_train: 0.02722126345053033 acc_train: 0.9898554104477612 loss_val: 0.16285852319704072 acc_val: 0.9575559701492538\n",
      "epoch 27 / 50 loss_train: 0.024766555731377538 acc_train: 0.9903218283582089 loss_val: 0.18338340784496143 acc_val: 0.9566231343283582\n",
      "epoch 28 / 50 loss_train: 0.029923943302315792 acc_train: 0.9888059701492538 loss_val: 0.19342566893537996 acc_val: 0.9542910447761194\n",
      "epoch 29 / 50 loss_train: 0.02583066663501357 acc_train: 0.9891557835820896 loss_val: 0.19498045185014473 acc_val: 0.9580223880597015\n",
      "epoch 30 / 50 loss_train: 0.027512694996661213 acc_train: 0.9897388059701493 loss_val: 0.20244245568413088 acc_val: 0.9524253731343284\n",
      "epoch 31 / 50 loss_train: 0.03716293438924707 acc_train: 0.988106343283582 loss_val: 0.1945750007701481 acc_val: 0.9547574626865671\n",
      "epoch 32 / 50 loss_train: 0.03030861395107205 acc_train: 0.988222947761194 loss_val: 0.1931218585170061 acc_val: 0.9528917910447762\n",
      "epoch 33 / 50 loss_train: 0.023751785660538392 acc_train: 0.9927705223880597 loss_val: 0.19247085882958462 acc_val: 0.9542910447761194\n",
      "epoch 34 / 50 loss_train: 0.017688017994619734 acc_train: 0.9932369402985075 loss_val: 0.20432358509291562 acc_val: 0.9500932835820896\n",
      "epoch 35 / 50 loss_train: 0.023279980549805665 acc_train: 0.9898554104477612 loss_val: 0.20773256148041574 acc_val: 0.9486940298507462\n",
      "epoch 36 / 50 loss_train: 0.01770320657718498 acc_train: 0.9927705223880597 loss_val: 0.24779144044720508 acc_val: 0.9482276119402985\n",
      "epoch 37 / 50 loss_train: 0.014994443947959702 acc_train: 0.9942863805970149 loss_val: 0.22273696573163443 acc_val: 0.9552238805970149\n",
      "epoch 38 / 50 loss_train: 0.025581391449669123 acc_train: 0.9890391791044776 loss_val: 0.18984935662868951 acc_val: 0.9584888059701493\n",
      "epoch 39 / 50 loss_train: 0.021539083759640264 acc_train: 0.9921875 loss_val: 0.2042274381446886 acc_val: 0.9566231343283582\n",
      "epoch 40 / 50 loss_train: 0.020376040433463336 acc_train: 0.9919542910447762 loss_val: 0.25112061020140697 acc_val: 0.9472947761194029\n",
      "epoch 41 / 50 loss_train: 0.026202203248436123 acc_train: 0.9904384328358209 loss_val: 0.1902236629453869 acc_val: 0.9561567164179104\n",
      "epoch 42 / 50 loss_train: 0.016520615089916646 acc_train: 0.9933535447761194 loss_val: 0.23275951796807262 acc_val: 0.9496268656716418\n",
      "epoch 43 / 50 loss_train: 0.02408988578475879 acc_train: 0.992304104477612 loss_val: 0.21328012417550943 acc_val: 0.9510261194029851\n",
      "epoch 44 / 50 loss_train: 0.020385551313553938 acc_train: 0.9930037313432836 loss_val: 0.24601779218678377 acc_val: 0.9468283582089553\n",
      "epoch 45 / 50 loss_train: 0.012895047635330219 acc_train: 0.9963852611940298 loss_val: 0.21149135577570558 acc_val: 0.9547574626865671\n",
      "epoch 46 / 50 loss_train: 0.015311297840418348 acc_train: 0.9941697761194029 loss_val: 0.21525328751792874 acc_val: 0.9561567164179104\n",
      "epoch 47 / 50 loss_train: 0.022076074185265634 acc_train: 0.9917210820895522 loss_val: 0.26000192530532845 acc_val: 0.9482276119402985\n",
      "epoch 48 / 50 loss_train: 0.016203627975976395 acc_train: 0.9938199626865671 loss_val: 0.21320502355137783 acc_val: 0.957089552238806\n",
      "epoch 49 / 50 loss_train: 0.016730277671911153 acc_train: 0.9948694029850746 loss_val: 0.25715536846162923 acc_val: 0.9449626865671642\n",
      "Validation accuracy for lr 0.001 bs 32 hl [100, 50, 25] : 0.9449626865671642\n",
      "epoch 0 / 50 loss_train: 0.7097604250285163 acc_train: 0.6673274253731343 loss_val: 0.39469434230616596 acc_val: 0.8636363636363636\n",
      "epoch 1 / 50 loss_train: 0.3099747679349202 acc_train: 0.8896921641791045 loss_val: 0.24539446706573168 acc_val: 0.9090909090909091\n",
      "epoch 2 / 50 loss_train: 0.2312683972404964 acc_train: 0.9134794776119403 loss_val: 0.2028814669924252 acc_val: 0.9214015151515151\n",
      "epoch 3 / 50 loss_train: 0.19506003440760855 acc_train: 0.9245569029850746 loss_val: 0.17918705607933755 acc_val: 0.9308712121212122\n",
      "epoch 4 / 50 loss_train: 0.17141299906061658 acc_train: 0.9333022388059702 loss_val: 0.16453994502609764 acc_val: 0.9403409090909091\n",
      "epoch 5 / 50 loss_train: 0.1543109056108923 acc_train: 0.9394822761194029 loss_val: 0.15411774438777656 acc_val: 0.9431818181818182\n",
      "epoch 6 / 50 loss_train: 0.1416282546219986 acc_train: 0.9443796641791045 loss_val: 0.1464490011620166 acc_val: 0.9450757575757576\n",
      "epoch 7 / 50 loss_train: 0.13167683424344703 acc_train: 0.949043843283582 loss_val: 0.140443087623377 acc_val: 0.9460227272727273\n",
      "epoch 8 / 50 loss_train: 0.12363308128803524 acc_train: 0.9505597014925373 loss_val: 0.13757662739865764 acc_val: 0.9479166666666666\n",
      "epoch 9 / 50 loss_train: 0.11633705561841602 acc_train: 0.9534748134328358 loss_val: 0.13312639631432388 acc_val: 0.9493371212121212\n",
      "epoch 10 / 50 loss_train: 0.11024861735528085 acc_train: 0.9554570895522388 loss_val: 0.12994297503521948 acc_val: 0.9521780303030303\n",
      "epoch 11 / 50 loss_train: 0.10443835669377846 acc_train: 0.9582555970149254 loss_val: 0.12828185266786904 acc_val: 0.9535984848484849\n",
      "epoch 12 / 50 loss_train: 0.09941001109945685 acc_train: 0.960820895522388 loss_val: 0.126568709050568 acc_val: 0.9535984848484849\n",
      "epoch 13 / 50 loss_train: 0.09465703756006351 acc_train: 0.9635027985074627 loss_val: 0.12647408546752448 acc_val: 0.9535984848484849\n",
      "epoch 14 / 50 loss_train: 0.09046155767543103 acc_train: 0.964785447761194 loss_val: 0.12564801109273863 acc_val: 0.9521780303030303\n",
      "epoch 15 / 50 loss_train: 0.0867576120532493 acc_train: 0.9660680970149254 loss_val: 0.1257331288255073 acc_val: 0.9507575757575758\n",
      "epoch 16 / 50 loss_train: 0.08339524826507515 acc_train: 0.9682835820895522 loss_val: 0.12576360522628724 acc_val: 0.9498106060606061\n",
      "epoch 17 / 50 loss_train: 0.08087472386880597 acc_train: 0.968633395522388 loss_val: 0.128035137579652 acc_val: 0.9479166666666666\n",
      "epoch 18 / 50 loss_train: 0.07775801584930785 acc_train: 0.96875 loss_val: 0.12949714665826445 acc_val: 0.9483901515151515\n",
      "epoch 19 / 50 loss_train: 0.07506820884074515 acc_train: 0.9700326492537313 loss_val: 0.13123434230557462 acc_val: 0.9460227272727273\n",
      "epoch 20 / 50 loss_train: 0.07226221085484348 acc_train: 0.9716651119402985 loss_val: 0.1323351606915408 acc_val: 0.9455492424242424\n",
      "epoch 21 / 50 loss_train: 0.06974968184897704 acc_train: 0.972597947761194 loss_val: 0.13324334060591742 acc_val: 0.9450757575757576\n",
      "epoch 22 / 50 loss_train: 0.06764381082116891 acc_train: 0.9730643656716418 loss_val: 0.13232361283181407 acc_val: 0.9450757575757576\n",
      "epoch 23 / 50 loss_train: 0.06542467134101178 acc_train: 0.9735307835820896 loss_val: 0.1326095384232944 acc_val: 0.9483901515151515\n",
      "epoch 24 / 50 loss_train: 0.06393738696351647 acc_train: 0.9739972014925373 loss_val: 0.1357231867521126 acc_val: 0.946969696969697\n",
      "epoch 25 / 50 loss_train: 0.061797131172645445 acc_train: 0.9751632462686567 loss_val: 0.13557632432811792 acc_val: 0.946969696969697\n",
      "epoch 26 / 50 loss_train: 0.060269580973284456 acc_train: 0.9758628731343284 loss_val: 0.13683703989266 acc_val: 0.9450757575757576\n",
      "epoch 27 / 50 loss_train: 0.05812130155347621 acc_train: 0.9767957089552238 loss_val: 0.13619366824615087 acc_val: 0.9474431818181818\n",
      "epoch 28 / 50 loss_train: 0.05678772455804161 acc_train: 0.9771455223880597 loss_val: 0.13392164005868046 acc_val: 0.9488636363636364\n",
      "epoch 29 / 50 loss_train: 0.05436191115000132 acc_train: 0.9790111940298507 loss_val: 0.1378858363925064 acc_val: 0.9512310606060606\n",
      "epoch 30 / 50 loss_train: 0.0537551902512561 acc_train: 0.9790111940298507 loss_val: 0.1377887941348139 acc_val: 0.9479166666666666\n",
      "epoch 31 / 50 loss_train: 0.055701466805454514 acc_train: 0.9773787313432836 loss_val: 0.13496545064745302 acc_val: 0.9512310606060606\n",
      "epoch 32 / 50 loss_train: 0.05029082273258202 acc_train: 0.980643656716418 loss_val: 0.13391652289828754 acc_val: 0.9517045454545454\n",
      "epoch 33 / 50 loss_train: 0.048145977175558234 acc_train: 0.9815764925373134 loss_val: 0.13480870524029986 acc_val: 0.9517045454545454\n",
      "epoch 34 / 50 loss_train: 0.046725457700998035 acc_train: 0.9821595149253731 loss_val: 0.13405798407528874 acc_val: 0.953125\n",
      "epoch 35 / 50 loss_train: 0.0453738454910023 acc_train: 0.9826259328358209 loss_val: 0.1353990016416242 acc_val: 0.9540719696969697\n",
      "epoch 36 / 50 loss_train: 0.044522094498596974 acc_train: 0.9829757462686567 loss_val: 0.1353787434327527 acc_val: 0.9535984848484849\n",
      "epoch 37 / 50 loss_train: 0.042496016471466024 acc_train: 0.9835587686567164 loss_val: 0.1371802592669112 acc_val: 0.9517045454545454\n",
      "epoch 38 / 50 loss_train: 0.04199345765718773 acc_train: 0.9839085820895522 loss_val: 0.14193235073302707 acc_val: 0.9498106060606061\n",
      "epoch 39 / 50 loss_train: 0.04082669902231489 acc_train: 0.984491604477612 loss_val: 0.13398362877672462 acc_val: 0.9526515151515151\n",
      "epoch 40 / 50 loss_train: 0.039935647002629825 acc_train: 0.9849580223880597 loss_val: 0.13575942101982072 acc_val: 0.9512310606060606\n",
      "epoch 41 / 50 loss_train: 0.03822732870397506 acc_train: 0.9861240671641791 loss_val: 0.13693479406442988 acc_val: 0.9517045454545454\n",
      "epoch 42 / 50 loss_train: 0.037332872012213095 acc_train: 0.9862406716417911 loss_val: 0.13533558391623327 acc_val: 0.953125\n",
      "epoch 43 / 50 loss_train: 0.03583130727647178 acc_train: 0.9876399253731343 loss_val: 0.13547603484060147 acc_val: 0.9540719696969697\n",
      "epoch 44 / 50 loss_train: 0.03478345772557294 acc_train: 0.988106343283582 loss_val: 0.13855854254969804 acc_val: 0.9526515151515151\n",
      "epoch 45 / 50 loss_train: 0.03413554805039025 acc_train: 0.9879897388059702 loss_val: 0.13791713629807623 acc_val: 0.9521780303030303\n",
      "epoch 46 / 50 loss_train: 0.03281298927517969 acc_train: 0.9895055970149254 loss_val: 0.13906716659620508 acc_val: 0.9526515151515151\n",
      "epoch 47 / 50 loss_train: 0.0318168714406219 acc_train: 0.9895055970149254 loss_val: 0.1377154010504276 acc_val: 0.9550189393939394\n",
      "epoch 48 / 50 loss_train: 0.031429952823682064 acc_train: 0.9892723880597015 loss_val: 0.14031036893167015 acc_val: 0.9540719696969697\n",
      "epoch 49 / 50 loss_train: 0.02991773989816099 acc_train: 0.9903218283582089 loss_val: 0.1424633163762916 acc_val: 0.9540719696969697\n",
      "Validation accuracy for lr 0.001 bs 64 hl [30, 15] : 0.9540719696969697\n",
      "epoch 0 / 50 loss_train: 0.5812003048720644 acc_train: 0.7482509328358209 loss_val: 0.2964577423126409 acc_val: 0.8896780303030303\n",
      "epoch 1 / 50 loss_train: 0.2535398623614169 acc_train: 0.9050839552238806 loss_val: 0.20619437657296658 acc_val: 0.9195075757575758\n",
      "epoch 2 / 50 loss_train: 0.18989925953879286 acc_train: 0.9266557835820896 loss_val: 0.17231845601010276 acc_val: 0.9322916666666666\n",
      "epoch 3 / 50 loss_train: 0.15889941089188875 acc_train: 0.9388992537313433 loss_val: 0.15336417045557138 acc_val: 0.9389204545454546\n",
      "epoch 4 / 50 loss_train: 0.13989598009346135 acc_train: 0.9458955223880597 loss_val: 0.1422074601410493 acc_val: 0.943655303030303\n",
      "epoch 5 / 50 loss_train: 0.12541798810794283 acc_train: 0.9500932835820896 loss_val: 0.13468995383961804 acc_val: 0.9488636363636364\n",
      "epoch 6 / 50 loss_train: 0.11435648102535685 acc_train: 0.9535914179104478 loss_val: 0.12901534827269093 acc_val: 0.9488636363636364\n",
      "epoch 7 / 50 loss_train: 0.10487641991852824 acc_train: 0.9588386194029851 loss_val: 0.12382393769142007 acc_val: 0.9502840909090909\n",
      "epoch 8 / 50 loss_train: 0.0976074373127142 acc_train: 0.9614039179104478 loss_val: 0.12115386260418083 acc_val: 0.9507575757575758\n",
      "epoch 9 / 50 loss_train: 0.09161255168103015 acc_train: 0.9640858208955224 loss_val: 0.12079428023450116 acc_val: 0.9502840909090909\n",
      "epoch 10 / 50 loss_train: 0.08671340332436028 acc_train: 0.9654850746268657 loss_val: 0.11882021114858314 acc_val: 0.953125\n",
      "epoch 11 / 50 loss_train: 0.08145747001546977 acc_train: 0.9660680970149254 loss_val: 0.11822342166700157 acc_val: 0.9526515151515151\n",
      "epoch 12 / 50 loss_train: 0.07746192299997184 acc_train: 0.9685167910447762 loss_val: 0.11771410367948026 acc_val: 0.9535984848484849\n",
      "epoch 13 / 50 loss_train: 0.07364738957761828 acc_train: 0.9700326492537313 loss_val: 0.1147413676165508 acc_val: 0.9545454545454546\n",
      "epoch 14 / 50 loss_train: 0.06912901366490927 acc_train: 0.9720149253731343 loss_val: 0.11390537664200648 acc_val: 0.9559659090909091\n",
      "epoch 15 / 50 loss_train: 0.06554403257514559 acc_train: 0.9743470149253731 loss_val: 0.1133910179036179 acc_val: 0.9564393939393939\n",
      "epoch 16 / 50 loss_train: 0.062112649242435376 acc_train: 0.9753964552238806 loss_val: 0.11323024785102206 acc_val: 0.959280303030303\n",
      "epoch 17 / 50 loss_train: 0.05916908593264533 acc_train: 0.9762126865671642 loss_val: 0.11277292596717599 acc_val: 0.9607007575757576\n",
      "epoch 18 / 50 loss_train: 0.056490819518968685 acc_train: 0.9777285447761194 loss_val: 0.11392907197427453 acc_val: 0.959280303030303\n",
      "epoch 19 / 50 loss_train: 0.05383603566034294 acc_train: 0.9780783582089553 loss_val: 0.11549572386359133 acc_val: 0.959280303030303\n",
      "epoch 20 / 50 loss_train: 0.05135361593452963 acc_train: 0.9799440298507462 loss_val: 0.12007056735128796 acc_val: 0.9588068181818182\n",
      "epoch 21 / 50 loss_train: 0.04902861884850731 acc_train: 0.980527052238806 loss_val: 0.1199127724978346 acc_val: 0.9583333333333334\n",
      "epoch 22 / 50 loss_train: 0.055459973273743225 acc_train: 0.9799440298507462 loss_val: 0.16470793187385646 acc_val: 0.9526515151515151\n",
      "epoch 23 / 50 loss_train: 0.05247530818277442 acc_train: 0.9788945895522388 loss_val: 0.118358942276005 acc_val: 0.9583333333333334\n",
      "epoch 24 / 50 loss_train: 0.04536035444948878 acc_train: 0.9832089552238806 loss_val: 0.11968881600565281 acc_val: 0.9569128787878788\n",
      "epoch 25 / 50 loss_train: 0.04236795982248636 acc_train: 0.9847248134328358 loss_val: 0.13111582489427898 acc_val: 0.953125\n",
      "epoch 26 / 50 loss_train: 0.040655233341255294 acc_train: 0.9848414179104478 loss_val: 0.12606438314896834 acc_val: 0.9588068181818182\n",
      "epoch 27 / 50 loss_train: 0.03770905335558884 acc_train: 0.9865904850746269 loss_val: 0.12558495023083702 acc_val: 0.9583333333333334\n",
      "epoch 28 / 50 loss_train: 0.0377434367268805 acc_train: 0.9857742537313433 loss_val: 0.13308507848550233 acc_val: 0.9569128787878788\n",
      "epoch 29 / 50 loss_train: 0.037554729788967255 acc_train: 0.9862406716417911 loss_val: 0.1395046762225916 acc_val: 0.9550189393939394\n",
      "epoch 30 / 50 loss_train: 0.03493549912097627 acc_train: 0.9872901119402985 loss_val: 0.1363819744714887 acc_val: 0.9573863636363636\n",
      "epoch 31 / 50 loss_train: 0.03204977389515391 acc_train: 0.988106343283582 loss_val: 0.1362542236479453 acc_val: 0.9588068181818182\n",
      "epoch 32 / 50 loss_train: 0.03164080192626857 acc_train: 0.9886893656716418 loss_val: 0.13441767108832234 acc_val: 0.9588068181818182\n",
      "epoch 33 / 50 loss_train: 0.02913333646042634 acc_train: 0.9900886194029851 loss_val: 0.13063331242216553 acc_val: 0.9616477272727273\n",
      "epoch 34 / 50 loss_train: 0.027294045709483603 acc_train: 0.9910214552238806 loss_val: 0.1304725424817734 acc_val: 0.9607007575757576\n",
      "epoch 35 / 50 loss_train: 0.026860039090431893 acc_train: 0.9906716417910447 loss_val: 0.1411447365561589 acc_val: 0.9607007575757576\n",
      "epoch 36 / 50 loss_train: 0.024895100425164313 acc_train: 0.9914878731343284 loss_val: 0.15256195106355902 acc_val: 0.9588068181818182\n",
      "epoch 37 / 50 loss_train: 0.024243312935195445 acc_train: 0.9914878731343284 loss_val: 0.150676105413604 acc_val: 0.9602272727272727\n",
      "epoch 38 / 50 loss_train: 0.02368306384661666 acc_train: 0.9916044776119403 loss_val: 0.16336668389266754 acc_val: 0.9578598484848485\n",
      "epoch 39 / 50 loss_train: 0.02346096992268533 acc_train: 0.9914878731343284 loss_val: 0.15698283347779934 acc_val: 0.9607007575757576\n",
      "epoch 40 / 50 loss_train: 0.023934942652356687 acc_train: 0.9921875 loss_val: 0.1763160753739823 acc_val: 0.9621212121212122\n",
      "epoch 41 / 50 loss_train: 0.025895627905091673 acc_train: 0.9906716417910447 loss_val: 0.16201937580896783 acc_val: 0.9597537878787878\n",
      "epoch 42 / 50 loss_train: 0.035565983216928096 acc_train: 0.9856576492537313 loss_val: 0.17030282911924366 acc_val: 0.9569128787878788\n",
      "epoch 43 / 50 loss_train: 0.031057454727298178 acc_train: 0.9892723880597015 loss_val: 0.1558142321516491 acc_val: 0.9616477272727273\n",
      "epoch 44 / 50 loss_train: 0.024501541563841416 acc_train: 0.9903218283582089 loss_val: 0.14495576120213394 acc_val: 0.9564393939393939\n",
      "epoch 45 / 50 loss_train: 0.02430716339423175 acc_train: 0.9912546641791045 loss_val: 0.14961868594657585 acc_val: 0.9554924242424242\n",
      "epoch 46 / 50 loss_train: 0.02079674885903638 acc_train: 0.9935867537313433 loss_val: 0.13884417589392056 acc_val: 0.9602272727272727\n",
      "epoch 47 / 50 loss_train: 0.020195666022308573 acc_train: 0.9940531716417911 loss_val: 0.15105918067315524 acc_val: 0.9569128787878788\n",
      "epoch 48 / 50 loss_train: 0.019038828646715508 acc_train: 0.9947527985074627 loss_val: 0.1520961465301463 acc_val: 0.9564393939393939\n",
      "epoch 49 / 50 loss_train: 0.017227496527423804 acc_train: 0.9951026119402985 loss_val: 0.15518014288720783 acc_val: 0.9583333333333334\n",
      "Validation accuracy for lr 0.001 bs 64 hl [50, 25] : 0.9583333333333334\n",
      "epoch 0 / 50 loss_train: 0.5450251357101682 acc_train: 0.7495335820895522 loss_val: 0.2522980107671835 acc_val: 0.8991477272727273\n",
      "epoch 1 / 50 loss_train: 0.2198786484216576 acc_train: 0.9110307835820896 loss_val: 0.1842164397698291 acc_val: 0.9247159090909091\n",
      "epoch 2 / 50 loss_train: 0.1603529762699088 acc_train: 0.9365671641791045 loss_val: 0.15134826508812804 acc_val: 0.9441287878787878\n",
      "epoch 3 / 50 loss_train: 0.13058430843277655 acc_train: 0.9468283582089553 loss_val: 0.14064142047782574 acc_val: 0.9455492424242424\n",
      "epoch 4 / 50 loss_train: 0.11459826394470769 acc_train: 0.9524253731343284 loss_val: 0.13464157543986596 acc_val: 0.9493371212121212\n",
      "epoch 5 / 50 loss_train: 0.10301925162715253 acc_train: 0.9562733208955224 loss_val: 0.1276516207097984 acc_val: 0.9521780303030303\n",
      "epoch 6 / 50 loss_train: 0.09397584398680214 acc_train: 0.9601212686567164 loss_val: 0.12437140086214311 acc_val: 0.9535984848484849\n",
      "epoch 7 / 50 loss_train: 0.08702904546160752 acc_train: 0.9635027985074627 loss_val: 0.12460193474146303 acc_val: 0.9554924242424242\n",
      "epoch 8 / 50 loss_train: 0.0795092375448613 acc_train: 0.9664179104477612 loss_val: 0.12158268548485597 acc_val: 0.9540719696969697\n",
      "epoch 9 / 50 loss_train: 0.07335366979380374 acc_train: 0.9704990671641791 loss_val: 0.1275140224510837 acc_val: 0.9535984848484849\n",
      "epoch 10 / 50 loss_train: 0.07911271001420804 acc_train: 0.9668843283582089 loss_val: 0.1273759884031139 acc_val: 0.9540719696969697\n",
      "epoch 11 / 50 loss_train: 0.06625889753227804 acc_train: 0.972831156716418 loss_val: 0.12952408108685448 acc_val: 0.9517045454545454\n",
      "epoch 12 / 50 loss_train: 0.06159096330042873 acc_train: 0.9749300373134329 loss_val: 0.13341663751104227 acc_val: 0.9512310606060606\n",
      "epoch 13 / 50 loss_train: 0.057989360559592716 acc_train: 0.9760960820895522 loss_val: 0.13191946616544373 acc_val: 0.9545454545454546\n",
      "epoch 14 / 50 loss_train: 0.05507293339026397 acc_train: 0.9773787313432836 loss_val: 0.13672927211748465 acc_val: 0.9535984848484849\n",
      "epoch 15 / 50 loss_train: 0.04848658405383354 acc_train: 0.9808768656716418 loss_val: 0.13299387736690038 acc_val: 0.9550189393939394\n",
      "epoch 16 / 50 loss_train: 0.044172925311627226 acc_train: 0.9825093283582089 loss_val: 0.13834826682445983 acc_val: 0.9540719696969697\n",
      "epoch 17 / 50 loss_train: 0.03975755698047578 acc_train: 0.9855410447761194 loss_val: 0.14332377170994492 acc_val: 0.9535984848484849\n",
      "epoch 18 / 50 loss_train: 0.03540136558246979 acc_train: 0.9872901119402985 loss_val: 0.1495253590647677 acc_val: 0.9550189393939394\n",
      "epoch 19 / 50 loss_train: 0.03215580396771209 acc_train: 0.988106343283582 loss_val: 0.15402340863600766 acc_val: 0.9521780303030303\n",
      "epoch 20 / 50 loss_train: 0.03660961498033756 acc_train: 0.984375 loss_val: 0.16288281381967504 acc_val: 0.9545454545454546\n",
      "epoch 21 / 50 loss_train: 0.0629860483728516 acc_train: 0.9748134328358209 loss_val: 0.14037312593755455 acc_val: 0.9550189393939394\n",
      "epoch 22 / 50 loss_train: 0.03690696371555912 acc_train: 0.9848414179104478 loss_val: 0.151815228144573 acc_val: 0.9540719696969697\n",
      "epoch 23 / 50 loss_train: 0.028724793103217745 acc_train: 0.9892723880597015 loss_val: 0.1505302179867599 acc_val: 0.9545454545454546\n",
      "epoch 24 / 50 loss_train: 0.02627928036007919 acc_train: 0.9895055970149254 loss_val: 0.1734765043325518 acc_val: 0.9517045454545454\n",
      "epoch 25 / 50 loss_train: 0.02579289759420651 acc_train: 0.9896222014925373 loss_val: 0.17999633439190307 acc_val: 0.9488636363636364\n",
      "epoch 26 / 50 loss_train: 0.02108227817468526 acc_train: 0.9913712686567164 loss_val: 0.1824779926451304 acc_val: 0.9526515151515151\n",
      "epoch 27 / 50 loss_train: 0.020867474030515653 acc_train: 0.9911380597014925 loss_val: 0.18520481630923408 acc_val: 0.9540719696969697\n",
      "epoch 28 / 50 loss_train: 0.026859908160551994 acc_train: 0.9890391791044776 loss_val: 0.15990752933786875 acc_val: 0.9540719696969697\n",
      "epoch 29 / 50 loss_train: 0.060146976682381816 acc_train: 0.9800606343283582 loss_val: 0.1972489788502461 acc_val: 0.9483901515151515\n",
      "epoch 30 / 50 loss_train: 0.04016119606424568 acc_train: 0.9820429104477612 loss_val: 0.16543935277753585 acc_val: 0.9588068181818182\n",
      "epoch 31 / 50 loss_train: 0.023581479536630073 acc_train: 0.9917210820895522 loss_val: 0.15920676642042805 acc_val: 0.9597537878787878\n",
      "epoch 32 / 50 loss_train: 0.014454628984442107 acc_train: 0.9941697761194029 loss_val: 0.16220659131178014 acc_val: 0.959280303030303\n",
      "epoch 33 / 50 loss_train: 0.010888428834298237 acc_train: 0.996152052238806 loss_val: 0.16324394019540536 acc_val: 0.9611742424242424\n",
      "epoch 34 / 50 loss_train: 0.009363355215073307 acc_train: 0.9970848880597015 loss_val: 0.1598792864807281 acc_val: 0.9607007575757576\n",
      "epoch 35 / 50 loss_train: 0.012128444580181412 acc_train: 0.996152052238806 loss_val: 0.19242803210557102 acc_val: 0.9550189393939394\n",
      "epoch 36 / 50 loss_train: 0.01774694623263577 acc_train: 0.9927705223880597 loss_val: 0.17504997190535437 acc_val: 0.9597537878787878\n",
      "epoch 37 / 50 loss_train: 0.012322288903115845 acc_train: 0.9955690298507462 loss_val: 0.16520951591472735 acc_val: 0.9602272727272727\n",
      "epoch 38 / 50 loss_train: 0.00889199705472574 acc_train: 0.9975513059701493 loss_val: 0.16353898270556905 acc_val: 0.962594696969697\n",
      "epoch 39 / 50 loss_train: 0.009774420066687118 acc_train: 0.9972014925373134 loss_val: 0.19358001411049913 acc_val: 0.9535984848484849\n",
      "epoch 40 / 50 loss_train: 0.020305063499027207 acc_train: 0.9931203358208955 loss_val: 0.18303905690178604 acc_val: 0.9564393939393939\n",
      "epoch 41 / 50 loss_train: 0.030479075176443264 acc_train: 0.9878731343283582 loss_val: 0.22556786525037287 acc_val: 0.9502840909090909\n",
      "epoch 42 / 50 loss_train: 0.01687867845623205 acc_train: 0.9934701492537313 loss_val: 0.17352935875496286 acc_val: 0.9588068181818182\n",
      "epoch 43 / 50 loss_train: 0.01291700078625364 acc_train: 0.996152052238806 loss_val: 0.17284177489423339 acc_val: 0.962594696969697\n",
      "epoch 44 / 50 loss_train: 0.007635274570224174 acc_train: 0.9979011194029851 loss_val: 0.18658823456009044 acc_val: 0.9588068181818182\n",
      "epoch 45 / 50 loss_train: 0.00540624198038131 acc_train: 0.9988339552238806 loss_val: 0.19618519536058487 acc_val: 0.9640151515151515\n",
      "epoch 46 / 50 loss_train: 0.0116211109011262 acc_train: 0.9955690298507462 loss_val: 0.1763541481103793 acc_val: 0.9635416666666666\n",
      "epoch 47 / 50 loss_train: 0.014695233679076273 acc_train: 0.9955690298507462 loss_val: 0.19360576407504568 acc_val: 0.9583333333333334\n",
      "epoch 48 / 50 loss_train: 0.03207645055465661 acc_train: 0.9877565298507462 loss_val: 0.2021686990597807 acc_val: 0.9540719696969697\n",
      "epoch 49 / 50 loss_train: 0.020271335868773038 acc_train: 0.9924207089552238 loss_val: 0.21222489859646326 acc_val: 0.9550189393939394\n",
      "Validation accuracy for lr 0.001 bs 64 hl [100, 50, 25] : 0.9550189393939394\n",
      "epoch 0 / 50 loss_train: 0.8203292275542644 acc_train: 0.6985774253731343 loss_val: 0.5445315968245268 acc_val: 0.82470703125\n",
      "epoch 1 / 50 loss_train: 0.4068451341408402 acc_train: 0.8590251865671642 loss_val: 0.3256590161472559 acc_val: 0.87646484375\n",
      "epoch 2 / 50 loss_train: 0.2943023196352062 acc_train: 0.8921408582089553 loss_val: 0.2671321651432663 acc_val: 0.90087890625\n",
      "epoch 3 / 50 loss_train: 0.2515277846980451 acc_train: 0.9052005597014925 loss_val: 0.23602442163974047 acc_val: 0.9072265625\n",
      "epoch 4 / 50 loss_train: 0.22340972076601057 acc_train: 0.9137126865671642 loss_val: 0.21288885589456186 acc_val: 0.9150390625\n",
      "epoch 5 / 50 loss_train: 0.20168512125513446 acc_train: 0.9211753731343284 loss_val: 0.19440054291044362 acc_val: 0.92431640625\n",
      "epoch 6 / 50 loss_train: 0.1827679954802812 acc_train: 0.9282882462686567 loss_val: 0.18061482580378652 acc_val: 0.93017578125\n",
      "epoch 7 / 50 loss_train: 0.1665781740822009 acc_train: 0.9344682835820896 loss_val: 0.16867125098360702 acc_val: 0.93310546875\n",
      "epoch 8 / 50 loss_train: 0.15332050766073055 acc_train: 0.9385494402985075 loss_val: 0.1601396318874322 acc_val: 0.9365234375\n",
      "epoch 9 / 50 loss_train: 0.14221261152580603 acc_train: 0.9433302238805971 loss_val: 0.15249902897630818 acc_val: 0.93994140625\n",
      "epoch 10 / 50 loss_train: 0.1331399734229294 acc_train: 0.9476445895522388 loss_val: 0.1469280381425051 acc_val: 0.9404296875\n",
      "epoch 11 / 50 loss_train: 0.12509654084248328 acc_train: 0.949277052238806 loss_val: 0.1420757216765196 acc_val: 0.94189453125\n",
      "epoch 12 / 50 loss_train: 0.11836099891520258 acc_train: 0.9520755597014925 loss_val: 0.13746517564868554 acc_val: 0.94384765625\n",
      "epoch 13 / 50 loss_train: 0.11260483019165139 acc_train: 0.9541744402985075 loss_val: 0.13406044242583448 acc_val: 0.9462890625\n",
      "epoch 14 / 50 loss_train: 0.1076848143294676 acc_train: 0.9558069029850746 loss_val: 0.13153644814155996 acc_val: 0.9482421875\n",
      "epoch 15 / 50 loss_train: 0.10343626864365678 acc_train: 0.956856343283582 loss_val: 0.12959978870276245 acc_val: 0.94580078125\n",
      "epoch 16 / 50 loss_train: 0.09941066367857491 acc_train: 0.9584888059701493 loss_val: 0.12828161519792047 acc_val: 0.9462890625\n",
      "epoch 17 / 50 loss_train: 0.09598294717829618 acc_train: 0.9602378731343284 loss_val: 0.12739421607875556 acc_val: 0.94775390625\n",
      "epoch 18 / 50 loss_train: 0.0929321884664137 acc_train: 0.9607042910447762 loss_val: 0.1265534251997451 acc_val: 0.95068359375\n",
      "epoch 19 / 50 loss_train: 0.09005656021077242 acc_train: 0.9625699626865671 loss_val: 0.12564617952921253 acc_val: 0.94970703125\n",
      "epoch 20 / 50 loss_train: 0.08741009263182754 acc_train: 0.9643190298507462 loss_val: 0.12503378196470294 acc_val: 0.95068359375\n",
      "epoch 21 / 50 loss_train: 0.0848142898905633 acc_train: 0.964668843283582 loss_val: 0.12424331264082866 acc_val: 0.9501953125\n",
      "epoch 22 / 50 loss_train: 0.08206678337570447 acc_train: 0.9663013059701493 loss_val: 0.12440888613673451 acc_val: 0.95068359375\n",
      "epoch 23 / 50 loss_train: 0.07971846023157461 acc_train: 0.9679337686567164 loss_val: 0.12268410321757983 acc_val: 0.95166015625\n",
      "epoch 24 / 50 loss_train: 0.07743558537826609 acc_train: 0.9678171641791045 loss_val: 0.12192499984485039 acc_val: 0.95263671875\n",
      "epoch 25 / 50 loss_train: 0.07551366691269092 acc_train: 0.9685167910447762 loss_val: 0.12147155226193718 acc_val: 0.953125\n",
      "epoch 26 / 50 loss_train: 0.07343004905243418 acc_train: 0.9697994402985075 loss_val: 0.12211250971449772 acc_val: 0.95263671875\n",
      "epoch 27 / 50 loss_train: 0.07123282632387395 acc_train: 0.9701492537313433 loss_val: 0.12182507072293447 acc_val: 0.95361328125\n",
      "epoch 28 / 50 loss_train: 0.06930850371162393 acc_train: 0.9714319029850746 loss_val: 0.12247143984814102 acc_val: 0.9521484375\n",
      "epoch 29 / 50 loss_train: 0.06725507479772638 acc_train: 0.9730643656716418 loss_val: 0.12219576699772006 acc_val: 0.95263671875\n",
      "epoch 30 / 50 loss_train: 0.06540933946397767 acc_train: 0.9734141791044776 loss_val: 0.12183409290037162 acc_val: 0.9521484375\n",
      "epoch 31 / 50 loss_train: 0.06330496584301565 acc_train: 0.9748134328358209 loss_val: 0.1221037294112648 acc_val: 0.953125\n",
      "epoch 32 / 50 loss_train: 0.061530567566627885 acc_train: 0.9758628731343284 loss_val: 0.12200415387019348 acc_val: 0.953125\n",
      "epoch 33 / 50 loss_train: 0.059897734630686136 acc_train: 0.976445895522388 loss_val: 0.12284204170032353 acc_val: 0.955078125\n",
      "epoch 34 / 50 loss_train: 0.05825397116479589 acc_train: 0.9774953358208955 loss_val: 0.12373970897829167 acc_val: 0.95458984375\n",
      "epoch 35 / 50 loss_train: 0.05662988192999541 acc_train: 0.9783115671641791 loss_val: 0.12404312870080503 acc_val: 0.95556640625\n",
      "epoch 36 / 50 loss_train: 0.055259463391197264 acc_train: 0.9794776119402985 loss_val: 0.12329052610004965 acc_val: 0.95654296875\n",
      "epoch 37 / 50 loss_train: 0.05382000649375702 acc_train: 0.9798274253731343 loss_val: 0.12386426893704083 acc_val: 0.9560546875\n",
      "epoch 38 / 50 loss_train: 0.05253587832757786 acc_train: 0.9797108208955224 loss_val: 0.12472542386410623 acc_val: 0.95751953125\n",
      "epoch 39 / 50 loss_train: 0.051167293910437554 acc_train: 0.9801772388059702 loss_val: 0.12638127458143344 acc_val: 0.95751953125\n",
      "epoch 40 / 50 loss_train: 0.04984316969318176 acc_train: 0.9807602611940298 loss_val: 0.12697425207983315 acc_val: 0.9560546875\n",
      "epoch 41 / 50 loss_train: 0.04844987393815571 acc_train: 0.9815764925373134 loss_val: 0.12798180525794578 acc_val: 0.9580078125\n",
      "epoch 42 / 50 loss_train: 0.04743504143361725 acc_train: 0.9828591417910447 loss_val: 0.12821260526266087 acc_val: 0.95556640625\n",
      "epoch 43 / 50 loss_train: 0.04619864378569286 acc_train: 0.9828591417910447 loss_val: 0.12948413062878217 acc_val: 0.95654296875\n",
      "epoch 44 / 50 loss_train: 0.045212346629531525 acc_train: 0.9836753731343284 loss_val: 0.13075695557176914 acc_val: 0.95458984375\n",
      "epoch 45 / 50 loss_train: 0.04399812170095853 acc_train: 0.9846082089552238 loss_val: 0.13253254232745348 acc_val: 0.9560546875\n",
      "epoch 46 / 50 loss_train: 0.04301906887219468 acc_train: 0.9849580223880597 loss_val: 0.13405808762942684 acc_val: 0.9560546875\n",
      "epoch 47 / 50 loss_train: 0.04193788516654897 acc_train: 0.9853078358208955 loss_val: 0.13335066590897782 acc_val: 0.95556640625\n",
      "epoch 48 / 50 loss_train: 0.040828214694203724 acc_train: 0.9854244402985075 loss_val: 0.13520054928167724 acc_val: 0.955078125\n",
      "epoch 49 / 50 loss_train: 0.03987536607171172 acc_train: 0.9856576492537313 loss_val: 0.13724398486928635 acc_val: 0.95556640625\n",
      "Validation accuracy for lr 0.001 bs 128 hl [30, 15] : 0.95556640625\n",
      "epoch 0 / 50 loss_train: 0.7868574733164773 acc_train: 0.6494869402985075 loss_val: 0.5020435582846403 acc_val: 0.81689453125\n",
      "epoch 1 / 50 loss_train: 0.365959003790101 acc_train: 0.8702192164179104 loss_val: 0.28440424450673163 acc_val: 0.89599609375\n",
      "epoch 2 / 50 loss_train: 0.2539039765720937 acc_train: 0.9060167910447762 loss_val: 0.22545658994931728 acc_val: 0.91552734375\n",
      "epoch 3 / 50 loss_train: 0.20828324144900734 acc_train: 0.9198927238805971 loss_val: 0.19425052497535944 acc_val: 0.92529296875\n",
      "epoch 4 / 50 loss_train: 0.17973761905485125 acc_train: 0.9310867537313433 loss_val: 0.17512237954360899 acc_val: 0.93310546875\n",
      "epoch 5 / 50 loss_train: 0.159245865336105 acc_train: 0.9379664179104478 loss_val: 0.1623582436441211 acc_val: 0.9365234375\n",
      "epoch 6 / 50 loss_train: 0.143539618422736 acc_train: 0.9435634328358209 loss_val: 0.1535747671368881 acc_val: 0.94140625\n",
      "epoch 7 / 50 loss_train: 0.13116153151686513 acc_train: 0.9460121268656716 loss_val: 0.14617306145009934 acc_val: 0.9443359375\n",
      "epoch 8 / 50 loss_train: 0.12119019131607084 acc_train: 0.9496268656716418 loss_val: 0.14001342495612334 acc_val: 0.94482421875\n",
      "epoch 9 / 50 loss_train: 0.11272923876322917 acc_train: 0.953008395522388 loss_val: 0.13570434126268083 acc_val: 0.9482421875\n",
      "epoch 10 / 50 loss_train: 0.1061150964293907 acc_train: 0.9566231343283582 loss_val: 0.1313245769651985 acc_val: 0.94921875\n",
      "epoch 11 / 50 loss_train: 0.10038380163596637 acc_train: 0.9587220149253731 loss_val: 0.12860044573244522 acc_val: 0.94970703125\n",
      "epoch 12 / 50 loss_train: 0.09530482102018684 acc_train: 0.9614039179104478 loss_val: 0.12607839876409344 acc_val: 0.9501953125\n",
      "epoch 13 / 50 loss_train: 0.09066919454220515 acc_train: 0.9631529850746269 loss_val: 0.12313782938554141 acc_val: 0.95263671875\n",
      "epoch 14 / 50 loss_train: 0.08670661020189968 acc_train: 0.965018656716418 loss_val: 0.12130571664351919 acc_val: 0.9501953125\n",
      "epoch 15 / 50 loss_train: 0.08264988466207661 acc_train: 0.9671175373134329 loss_val: 0.11993443012897842 acc_val: 0.95263671875\n",
      "epoch 16 / 50 loss_train: 0.07907801994414472 acc_train: 0.9678171641791045 loss_val: 0.11876593080944531 acc_val: 0.953125\n",
      "epoch 17 / 50 loss_train: 0.07616656395926405 acc_train: 0.9694496268656716 loss_val: 0.11703572215276381 acc_val: 0.9541015625\n",
      "epoch 18 / 50 loss_train: 0.07280511589748646 acc_train: 0.9714319029850746 loss_val: 0.11594205828282611 acc_val: 0.95751953125\n",
      "epoch 19 / 50 loss_train: 0.06974675317308796 acc_train: 0.972481343283582 loss_val: 0.11561677971633344 acc_val: 0.95654296875\n",
      "epoch 20 / 50 loss_train: 0.06681228498580741 acc_train: 0.9736473880597015 loss_val: 0.11385041047617506 acc_val: 0.95654296875\n",
      "epoch 21 / 50 loss_train: 0.06403549393611167 acc_train: 0.9743470149253731 loss_val: 0.11377722824914827 acc_val: 0.95703125\n",
      "epoch 22 / 50 loss_train: 0.06141402869860628 acc_train: 0.9752798507462687 loss_val: 0.11321557982336117 acc_val: 0.958984375\n",
      "epoch 23 / 50 loss_train: 0.058759742124534366 acc_train: 0.9765625 loss_val: 0.11318996287056393 acc_val: 0.9580078125\n",
      "epoch 24 / 50 loss_train: 0.056247927693288714 acc_train: 0.9772621268656716 loss_val: 0.11269997996893721 acc_val: 0.95703125\n",
      "epoch 25 / 50 loss_train: 0.0542536657025565 acc_train: 0.9779617537313433 loss_val: 0.11388423500562794 acc_val: 0.958984375\n",
      "epoch 26 / 50 loss_train: 0.052353741740112876 acc_train: 0.9784281716417911 loss_val: 0.11343356086001677 acc_val: 0.95703125\n",
      "epoch 27 / 50 loss_train: 0.050174843158517314 acc_train: 0.9800606343283582 loss_val: 0.11241036680175398 acc_val: 0.95947265625\n",
      "epoch 28 / 50 loss_train: 0.04868764466425376 acc_train: 0.9812266791044776 loss_val: 0.11488587435371755 acc_val: 0.95703125\n",
      "epoch 29 / 50 loss_train: 0.04685280527641524 acc_train: 0.9814598880597015 loss_val: 0.11357324858096263 acc_val: 0.95849609375\n",
      "epoch 30 / 50 loss_train: 0.044798099227360826 acc_train: 0.9835587686567164 loss_val: 0.11394243160714268 acc_val: 0.95849609375\n",
      "epoch 31 / 50 loss_train: 0.043199533624435536 acc_train: 0.9840251865671642 loss_val: 0.11380206861451825 acc_val: 0.96044921875\n",
      "epoch 32 / 50 loss_train: 0.04131922829173394 acc_train: 0.9849580223880597 loss_val: 0.11522616329352786 acc_val: 0.96142578125\n",
      "epoch 33 / 50 loss_train: 0.03995605972387008 acc_train: 0.9865904850746269 loss_val: 0.11584872523644663 acc_val: 0.96142578125\n",
      "epoch 34 / 50 loss_train: 0.038383272601597344 acc_train: 0.9872901119402985 loss_val: 0.1170594943996548 acc_val: 0.9619140625\n",
      "epoch 35 / 50 loss_train: 0.036971841021371424 acc_train: 0.9870569029850746 loss_val: 0.1207038242582712 acc_val: 0.95947265625\n",
      "epoch 36 / 50 loss_train: 0.03537814341374298 acc_train: 0.9878731343283582 loss_val: 0.12110007902356301 acc_val: 0.95849609375\n",
      "epoch 37 / 50 loss_train: 0.03422432073127868 acc_train: 0.988222947761194 loss_val: 0.1236748711544351 acc_val: 0.96142578125\n",
      "epoch 38 / 50 loss_train: 0.03246990031104034 acc_train: 0.9897388059701493 loss_val: 0.12226931808254449 acc_val: 0.9609375\n",
      "epoch 39 / 50 loss_train: 0.031441957926127445 acc_train: 0.9895055970149254 loss_val: 0.12397435175080318 acc_val: 0.96142578125\n",
      "epoch 40 / 50 loss_train: 0.030171319508730476 acc_train: 0.9904384328358209 loss_val: 0.12658744108557585 acc_val: 0.9619140625\n",
      "epoch 41 / 50 loss_train: 0.02913183107304929 acc_train: 0.9907882462686567 loss_val: 0.12722319994099962 acc_val: 0.9609375\n",
      "epoch 42 / 50 loss_train: 0.028201906719425723 acc_train: 0.9912546641791045 loss_val: 0.1302488150759018 acc_val: 0.9619140625\n",
      "epoch 43 / 50 loss_train: 0.027249065061002525 acc_train: 0.9913712686567164 loss_val: 0.13489307415329677 acc_val: 0.9599609375\n",
      "epoch 44 / 50 loss_train: 0.02645323269152597 acc_train: 0.9914878731343284 loss_val: 0.13191319021700565 acc_val: 0.9609375\n",
      "epoch 45 / 50 loss_train: 0.02500273618819331 acc_train: 0.9918376865671642 loss_val: 0.13571833272908407 acc_val: 0.96142578125\n",
      "epoch 46 / 50 loss_train: 0.02461194668187579 acc_train: 0.9925373134328358 loss_val: 0.1380147420868525 acc_val: 0.9619140625\n",
      "epoch 47 / 50 loss_train: 0.024391184588755246 acc_train: 0.9926539179104478 loss_val: 0.14190316975509631 acc_val: 0.9599609375\n",
      "epoch 48 / 50 loss_train: 0.02329156811891207 acc_train: 0.9926539179104478 loss_val: 0.14088852397253504 acc_val: 0.9609375\n",
      "epoch 49 / 50 loss_train: 0.02222762788548621 acc_train: 0.9935867537313433 loss_val: 0.14671112981886836 acc_val: 0.9599609375\n",
      "Validation accuracy for lr 0.001 bs 128 hl [50, 25] : 0.9599609375\n",
      "epoch 0 / 50 loss_train: 0.6434284951259841 acc_train: 0.6940298507462687 loss_val: 0.3462828721385449 acc_val: 0.87109375\n",
      "epoch 1 / 50 loss_train: 0.27832612306324406 acc_train: 0.8964552238805971 loss_val: 0.22354704693134408 acc_val: 0.91455078125\n",
      "epoch 2 / 50 loss_train: 0.2017154045291801 acc_train: 0.9242070895522388 loss_val: 0.1810383663396351 acc_val: 0.931640625\n",
      "epoch 3 / 50 loss_train: 0.16302446787481878 acc_train: 0.9359841417910447 loss_val: 0.1570393468355178 acc_val: 0.93798828125\n",
      "epoch 4 / 50 loss_train: 0.1393958699347368 acc_train: 0.9442630597014925 loss_val: 0.1450417866344651 acc_val: 0.94287109375\n",
      "epoch 5 / 50 loss_train: 0.1221463092005075 acc_train: 0.9507929104477612 loss_val: 0.1372750492735122 acc_val: 0.9453125\n",
      "epoch 6 / 50 loss_train: 0.10913036001929596 acc_train: 0.9555736940298507 loss_val: 0.13311821616207453 acc_val: 0.9453125\n",
      "epoch 7 / 50 loss_train: 0.09894140364963618 acc_train: 0.9582555970149254 loss_val: 0.1306473208483112 acc_val: 0.94921875\n",
      "epoch 8 / 50 loss_train: 0.09013634914560105 acc_train: 0.9623367537313433 loss_val: 0.13014389451927855 acc_val: 0.94970703125\n",
      "epoch 9 / 50 loss_train: 0.08215367221342984 acc_train: 0.964668843283582 loss_val: 0.13028422598131328 acc_val: 0.94970703125\n",
      "epoch 10 / 50 loss_train: 0.0754418219092177 acc_train: 0.9689832089552238 loss_val: 0.1274812915480652 acc_val: 0.95166015625\n",
      "epoch 11 / 50 loss_train: 0.06955664977431297 acc_train: 0.9718983208955224 loss_val: 0.1303599817731822 acc_val: 0.95166015625\n",
      "epoch 12 / 50 loss_train: 0.06609711798825371 acc_train: 0.9730643656716418 loss_val: 0.12803459179073684 acc_val: 0.95361328125\n",
      "epoch 13 / 50 loss_train: 0.06180283221513478 acc_train: 0.9755130597014925 loss_val: 0.1262985385668003 acc_val: 0.95458984375\n",
      "epoch 14 / 50 loss_train: 0.057525092235474445 acc_train: 0.9770289179104478 loss_val: 0.12433645122449022 acc_val: 0.95458984375\n",
      "epoch 15 / 50 loss_train: 0.0533739695909308 acc_train: 0.9793610074626866 loss_val: 0.15703509222566936 acc_val: 0.9482421875\n",
      "epoch 16 / 50 loss_train: 0.05507410050772909 acc_train: 0.9791277985074627 loss_val: 0.1278850897670054 acc_val: 0.95263671875\n",
      "epoch 17 / 50 loss_train: 0.05079570343134118 acc_train: 0.980293843283582 loss_val: 0.13359938410439975 acc_val: 0.95458984375\n",
      "epoch 18 / 50 loss_train: 0.04816639149533724 acc_train: 0.9822761194029851 loss_val: 0.1455023697926663 acc_val: 0.95263671875\n",
      "epoch 19 / 50 loss_train: 0.04853857889660259 acc_train: 0.9815764925373134 loss_val: 0.13650179858802858 acc_val: 0.9560546875\n",
      "epoch 20 / 50 loss_train: 0.04275202202096359 acc_train: 0.9839085820895522 loss_val: 0.13648345997535216 acc_val: 0.9541015625\n",
      "epoch 21 / 50 loss_train: 0.03764215324407638 acc_train: 0.9858908582089553 loss_val: 0.14029985279921675 acc_val: 0.95068359375\n",
      "epoch 22 / 50 loss_train: 0.03937237344876821 acc_train: 0.9841417910447762 loss_val: 0.13822451279159864 acc_val: 0.9560546875\n",
      "epoch 23 / 50 loss_train: 0.03480456114760531 acc_train: 0.9869402985074627 loss_val: 0.17159897366144605 acc_val: 0.95166015625\n",
      "epoch 24 / 50 loss_train: 0.0366190954792633 acc_train: 0.9864738805970149 loss_val: 0.22001972748282128 acc_val: 0.9423828125\n",
      "epoch 25 / 50 loss_train: 0.052376029540353745 acc_train: 0.9814598880597015 loss_val: 0.18632340009207837 acc_val: 0.947265625\n",
      "epoch 26 / 50 loss_train: 0.03759830278942167 acc_train: 0.9850746268656716 loss_val: 0.1681993724705535 acc_val: 0.951171875\n",
      "epoch 27 / 50 loss_train: 0.030663244815459893 acc_train: 0.9874067164179104 loss_val: 0.19727067000349052 acc_val: 0.9462890625\n",
      "epoch 28 / 50 loss_train: 0.030095048700528804 acc_train: 0.988106343283582 loss_val: 0.19761537566955667 acc_val: 0.9462890625\n",
      "epoch 29 / 50 loss_train: 0.026470253570700316 acc_train: 0.9897388059701493 loss_val: 0.18910557201888878 acc_val: 0.94677734375\n",
      "epoch 30 / 50 loss_train: 0.02528463145578975 acc_train: 0.9893889925373134 loss_val: 0.18672740749025252 acc_val: 0.947265625\n",
      "epoch 31 / 50 loss_train: 0.023493851729626977 acc_train: 0.9893889925373134 loss_val: 0.17723548492358532 acc_val: 0.95068359375\n",
      "epoch 32 / 50 loss_train: 0.023581325563032236 acc_train: 0.9904384328358209 loss_val: 0.15983759395385277 acc_val: 0.95458984375\n",
      "epoch 33 / 50 loss_train: 0.02586989210390333 acc_train: 0.9899720149253731 loss_val: 0.15389001591393026 acc_val: 0.955078125\n",
      "epoch 34 / 50 loss_train: 0.027948664688963943 acc_train: 0.9874067164179104 loss_val: 0.1700901960286103 acc_val: 0.95556640625\n",
      "epoch 35 / 50 loss_train: 0.028252163491865146 acc_train: 0.9885727611940298 loss_val: 0.17674025694576545 acc_val: 0.95166015625\n",
      "epoch 36 / 50 loss_train: 0.040615731214326045 acc_train: 0.9857742537313433 loss_val: 0.20254797426559312 acc_val: 0.9482421875\n",
      "epoch 37 / 50 loss_train: 0.03815399470335957 acc_train: 0.9848414179104478 loss_val: 0.1968102960345277 acc_val: 0.9443359375\n",
      "epoch 38 / 50 loss_train: 0.03450114373117685 acc_train: 0.9850746268656716 loss_val: 0.17924327632135828 acc_val: 0.953125\n",
      "epoch 39 / 50 loss_train: 0.026513052252885787 acc_train: 0.9895055970149254 loss_val: 0.17524961995877675 acc_val: 0.951171875\n",
      "epoch 40 / 50 loss_train: 0.024648080162926397 acc_train: 0.9899720149253731 loss_val: 0.1674177099212102 acc_val: 0.9541015625\n",
      "epoch 41 / 50 loss_train: 0.021658263831232576 acc_train: 0.9914878731343284 loss_val: 0.15711154171731323 acc_val: 0.9580078125\n",
      "epoch 42 / 50 loss_train: 0.016454341744920657 acc_train: 0.9945195895522388 loss_val: 0.15601057727326406 acc_val: 0.9580078125\n",
      "epoch 43 / 50 loss_train: 0.010335311356153506 acc_train: 0.9972014925373134 loss_val: 0.17140473746985663 acc_val: 0.95654296875\n",
      "epoch 44 / 50 loss_train: 0.008748608623951007 acc_train: 0.9979011194029851 loss_val: 0.17463752192270476 acc_val: 0.9560546875\n",
      "epoch 45 / 50 loss_train: 0.0076075110709600486 acc_train: 0.9987173507462687 loss_val: 0.17947201760762255 acc_val: 0.955078125\n",
      "epoch 46 / 50 loss_train: 0.0060963052769761475 acc_train: 0.9993003731343284 loss_val: 0.18876801912483643 acc_val: 0.9560546875\n",
      "epoch 47 / 50 loss_train: 0.005538001601799711 acc_train: 0.9989505597014925 loss_val: 0.1913107179061626 acc_val: 0.95654296875\n",
      "epoch 48 / 50 loss_train: 0.004781305912444229 acc_train: 0.9994169776119403 loss_val: 0.19934830633064848 acc_val: 0.9560546875\n",
      "epoch 49 / 50 loss_train: 0.004186634955156261 acc_train: 0.9996501865671642 loss_val: 0.20641311942927132 acc_val: 0.953125\n",
      "Validation accuracy for lr 0.001 bs 128 hl [100, 50, 25] : 0.953125\n",
      "epoch 0 / 50 loss_train: 0.9591624876901285 acc_train: 0.49288712686567165 loss_val: 0.8587667603990925 acc_val: 0.590018656716418\n",
      "epoch 1 / 50 loss_train: 0.7555842148279076 acc_train: 0.6791044776119403 loss_val: 0.655195922993902 acc_val: 0.7444029850746269\n",
      "epoch 2 / 50 loss_train: 0.5778914220742325 acc_train: 0.7938432835820896 loss_val: 0.5023129442289694 acc_val: 0.8278917910447762\n",
      "epoch 3 / 50 loss_train: 0.45803933579530287 acc_train: 0.8416511194029851 loss_val: 0.40874032075725386 acc_val: 0.8647388059701493\n",
      "epoch 4 / 50 loss_train: 0.3851568324129973 acc_train: 0.8625233208955224 loss_val: 0.35252186800561736 acc_val: 0.8726679104477612\n",
      "epoch 5 / 50 loss_train: 0.3395450629516324 acc_train: 0.8776819029850746 loss_val: 0.31484039995207713 acc_val: 0.8815298507462687\n",
      "epoch 6 / 50 loss_train: 0.3085092767072258 acc_train: 0.8858442164179104 loss_val: 0.2892258588725062 acc_val: 0.8931902985074627\n",
      "epoch 7 / 50 loss_train: 0.2866136835637822 acc_train: 0.8942397388059702 loss_val: 0.2701997847310198 acc_val: 0.8964552238805971\n",
      "epoch 8 / 50 loss_train: 0.269696563788092 acc_train: 0.8975046641791045 loss_val: 0.2554465955151106 acc_val: 0.9001865671641791\n",
      "epoch 9 / 50 loss_train: 0.2560477224526121 acc_train: 0.9014692164179104 loss_val: 0.24342597519228262 acc_val: 0.9057835820895522\n",
      "epoch 10 / 50 loss_train: 0.24452267250796753 acc_train: 0.9059001865671642 loss_val: 0.2334807945220773 acc_val: 0.9113805970149254\n",
      "epoch 11 / 50 loss_train: 0.23472146192259752 acc_train: 0.910097947761194 loss_val: 0.2251451060022992 acc_val: 0.9109141791044776\n",
      "epoch 12 / 50 loss_train: 0.22611234687379936 acc_train: 0.9134794776119403 loss_val: 0.2178995133683419 acc_val: 0.9165111940298507\n",
      "epoch 13 / 50 loss_train: 0.21841747790519425 acc_train: 0.9159281716417911 loss_val: 0.21133078356037166 acc_val: 0.9183768656716418\n",
      "epoch 14 / 50 loss_train: 0.21150277399305087 acc_train: 0.9182602611940298 loss_val: 0.2054732528848768 acc_val: 0.9221082089552238\n",
      "epoch 15 / 50 loss_train: 0.205180630852371 acc_train: 0.9212919776119403 loss_val: 0.20010115680938115 acc_val: 0.9225746268656716\n",
      "epoch 16 / 50 loss_train: 0.19939712266813017 acc_train: 0.9242070895522388 loss_val: 0.19535127503505506 acc_val: 0.9221082089552238\n",
      "epoch 17 / 50 loss_train: 0.19406773815793332 acc_train: 0.9253731343283582 loss_val: 0.1909900787182208 acc_val: 0.9267723880597015\n",
      "epoch 18 / 50 loss_train: 0.18910046883705836 acc_train: 0.9270055970149254 loss_val: 0.18697267533245204 acc_val: 0.9300373134328358\n",
      "epoch 19 / 50 loss_train: 0.18449011221250047 acc_train: 0.9287546641791045 loss_val: 0.1833851693698497 acc_val: 0.9314365671641791\n",
      "epoch 20 / 50 loss_train: 0.18025775224582027 acc_train: 0.929804104477612 loss_val: 0.17998726637825482 acc_val: 0.9328358208955224\n",
      "epoch 21 / 50 loss_train: 0.1762771937134328 acc_train: 0.9313199626865671 loss_val: 0.17681521924695612 acc_val: 0.9347014925373134\n",
      "epoch 22 / 50 loss_train: 0.17251312433144272 acc_train: 0.9323694029850746 loss_val: 0.17389690304876987 acc_val: 0.9351679104477612\n",
      "epoch 23 / 50 loss_train: 0.16889679984453676 acc_train: 0.9343516791044776 loss_val: 0.17123556805430537 acc_val: 0.9351679104477612\n",
      "epoch 24 / 50 loss_train: 0.1654796552316133 acc_train: 0.9359841417910447 loss_val: 0.16878911528524373 acc_val: 0.9347014925373134\n",
      "epoch 25 / 50 loss_train: 0.16221703032949078 acc_train: 0.9366837686567164 loss_val: 0.16651787948985794 acc_val: 0.9356343283582089\n",
      "epoch 26 / 50 loss_train: 0.15909823060119108 acc_train: 0.9386660447761194 loss_val: 0.16440277424234367 acc_val: 0.9370335820895522\n",
      "epoch 27 / 50 loss_train: 0.1560986616662634 acc_train: 0.9390158582089553 loss_val: 0.16230716337684417 acc_val: 0.9375\n",
      "epoch 28 / 50 loss_train: 0.15324501967780405 acc_train: 0.9399486940298507 loss_val: 0.16021898376135113 acc_val: 0.9384328358208955\n",
      "epoch 29 / 50 loss_train: 0.1504780408759504 acc_train: 0.9407649253731343 loss_val: 0.15830784933137665 acc_val: 0.9393656716417911\n",
      "epoch 30 / 50 loss_train: 0.14782976733743033 acc_train: 0.941231343283582 loss_val: 0.1564592989721968 acc_val: 0.9393656716417911\n",
      "epoch 31 / 50 loss_train: 0.14529782773426442 acc_train: 0.9421641791044776 loss_val: 0.1546494628390933 acc_val: 0.9402985074626866\n",
      "epoch 32 / 50 loss_train: 0.14294087335661942 acc_train: 0.9435634328358209 loss_val: 0.15305013464934908 acc_val: 0.941231343283582\n",
      "epoch 33 / 50 loss_train: 0.1406616268040084 acc_train: 0.9443796641791045 loss_val: 0.1515471256436231 acc_val: 0.9407649253731343\n",
      "epoch 34 / 50 loss_train: 0.13845663956616686 acc_train: 0.9448460820895522 loss_val: 0.15006064809967395 acc_val: 0.9426305970149254\n",
      "epoch 35 / 50 loss_train: 0.13638105893632702 acc_train: 0.9457789179104478 loss_val: 0.14864959221015295 acc_val: 0.9426305970149254\n",
      "epoch 36 / 50 loss_train: 0.13429822964913476 acc_train: 0.9463619402985075 loss_val: 0.147306683725667 acc_val: 0.9426305970149254\n",
      "epoch 37 / 50 loss_train: 0.13227196491615317 acc_train: 0.9477611940298507 loss_val: 0.14600834455458384 acc_val: 0.9430970149253731\n",
      "epoch 38 / 50 loss_train: 0.13032393270074877 acc_train: 0.949043843283582 loss_val: 0.14491838278029975 acc_val: 0.9444962686567164\n",
      "epoch 39 / 50 loss_train: 0.1283985413228676 acc_train: 0.9497434701492538 loss_val: 0.14380922371307875 acc_val: 0.9444962686567164\n",
      "epoch 40 / 50 loss_train: 0.12653654900643585 acc_train: 0.9505597014925373 loss_val: 0.14272647516451403 acc_val: 0.945429104477612\n",
      "epoch 41 / 50 loss_train: 0.12469348022297247 acc_train: 0.9509095149253731 loss_val: 0.14178801934215685 acc_val: 0.9458955223880597\n",
      "epoch 42 / 50 loss_train: 0.12291920772030838 acc_train: 0.9516091417910447 loss_val: 0.1408372926934094 acc_val: 0.9463619402985075\n",
      "epoch 43 / 50 loss_train: 0.12117360283120243 acc_train: 0.9521921641791045 loss_val: 0.13988606042620455 acc_val: 0.9463619402985075\n",
      "epoch 44 / 50 loss_train: 0.11951808732197579 acc_train: 0.9526585820895522 loss_val: 0.1390469111937642 acc_val: 0.9468283582089553\n",
      "epoch 45 / 50 loss_train: 0.11789968506252366 acc_train: 0.9533582089552238 loss_val: 0.13805068193703765 acc_val: 0.9472947761194029\n",
      "epoch 46 / 50 loss_train: 0.1163398349664605 acc_train: 0.9539412313432836 loss_val: 0.13712349415181418 acc_val: 0.9477611940298507\n",
      "epoch 47 / 50 loss_train: 0.11484002670509491 acc_train: 0.9546408582089553 loss_val: 0.13619171226190388 acc_val: 0.9486940298507462\n",
      "epoch 48 / 50 loss_train: 0.1134020537381241 acc_train: 0.9547574626865671 loss_val: 0.13540413373695304 acc_val: 0.9482276119402985\n",
      "epoch 49 / 50 loss_train: 0.11202311685739724 acc_train: 0.9553404850746269 loss_val: 0.1343912163278789 acc_val: 0.949160447761194\n",
      "Validation accuracy for lr 0.0001 bs 32 hl [30, 15] : 0.949160447761194\n",
      "epoch 0 / 50 loss_train: 0.9963533735986966 acc_train: 0.5096781716417911 loss_val: 0.8456018887349029 acc_val: 0.683768656716418\n",
      "epoch 1 / 50 loss_train: 0.7044287696480751 acc_train: 0.7171175373134329 loss_val: 0.5911514158569166 acc_val: 0.7607276119402985\n",
      "epoch 2 / 50 loss_train: 0.5135008924043001 acc_train: 0.816231343283582 loss_val: 0.439591210168689 acc_val: 0.8558768656716418\n",
      "epoch 3 / 50 loss_train: 0.3992527778913726 acc_train: 0.8666044776119403 loss_val: 0.35373447585239337 acc_val: 0.8759328358208955\n",
      "epoch 4 / 50 loss_train: 0.3366273591545091 acc_train: 0.8822294776119403 loss_val: 0.30753014075444707 acc_val: 0.8899253731343284\n",
      "epoch 5 / 50 loss_train: 0.2991064305959353 acc_train: 0.8919076492537313 loss_val: 0.27652461949124263 acc_val: 0.898320895522388\n",
      "epoch 6 / 50 loss_train: 0.2724122546557615 acc_train: 0.9003031716417911 loss_val: 0.25344933216362747 acc_val: 0.9076492537313433\n",
      "epoch 7 / 50 loss_train: 0.2516704663467496 acc_train: 0.9068330223880597 loss_val: 0.23574318807683328 acc_val: 0.914179104477612\n",
      "epoch 8 / 50 loss_train: 0.23481715823621002 acc_train: 0.9133628731343284 loss_val: 0.22153582013170445 acc_val: 0.9207089552238806\n",
      "epoch 9 / 50 loss_train: 0.2208270224561887 acc_train: 0.9166277985074627 loss_val: 0.20988356502059458 acc_val: 0.9253731343283582\n",
      "epoch 10 / 50 loss_train: 0.20888621255811027 acc_train: 0.9222248134328358 loss_val: 0.20012201753153064 acc_val: 0.929570895522388\n",
      "epoch 11 / 50 loss_train: 0.19862136116890766 acc_train: 0.9245569029850746 loss_val: 0.1918963075705818 acc_val: 0.9328358208955224\n",
      "epoch 12 / 50 loss_train: 0.18966351036649587 acc_train: 0.9284048507462687 loss_val: 0.18510189320572387 acc_val: 0.9351679104477612\n",
      "epoch 13 / 50 loss_train: 0.18178624470732105 acc_train: 0.9316697761194029 loss_val: 0.1792620666561398 acc_val: 0.9370335820895522\n",
      "epoch 14 / 50 loss_train: 0.17483032631006704 acc_train: 0.933652052238806 loss_val: 0.17398594474287898 acc_val: 0.9375\n",
      "epoch 15 / 50 loss_train: 0.168565283758816 acc_train: 0.9358675373134329 loss_val: 0.16932448297530525 acc_val: 0.9402985074626866\n",
      "epoch 16 / 50 loss_train: 0.1628919551983031 acc_train: 0.9385494402985075 loss_val: 0.16527605602698428 acc_val: 0.941231343283582\n",
      "epoch 17 / 50 loss_train: 0.15772115333533998 acc_train: 0.941231343283582 loss_val: 0.16158821090704886 acc_val: 0.9426305970149254\n",
      "epoch 18 / 50 loss_train: 0.1528915577748818 acc_train: 0.9429804104477612 loss_val: 0.158330085912351 acc_val: 0.9440298507462687\n",
      "epoch 19 / 50 loss_train: 0.14844369198153937 acc_train: 0.9447294776119403 loss_val: 0.15544436573408849 acc_val: 0.9449626865671642\n",
      "epoch 20 / 50 loss_train: 0.14436504513080886 acc_train: 0.9458955223880597 loss_val: 0.15277738390193293 acc_val: 0.945429104477612\n",
      "epoch 21 / 50 loss_train: 0.1406322404909045 acc_train: 0.9467117537313433 loss_val: 0.15037130500335913 acc_val: 0.9458955223880597\n",
      "epoch 22 / 50 loss_train: 0.1371384740306704 acc_train: 0.9478777985074627 loss_val: 0.14819877927101202 acc_val: 0.9458955223880597\n",
      "epoch 23 / 50 loss_train: 0.13385361421908906 acc_train: 0.9486940298507462 loss_val: 0.14618048097185943 acc_val: 0.9482276119402985\n",
      "epoch 24 / 50 loss_train: 0.13071755896697737 acc_train: 0.9503264925373134 loss_val: 0.14413889204091115 acc_val: 0.9486940298507462\n",
      "epoch 25 / 50 loss_train: 0.1277569150482652 acc_train: 0.9513759328358209 loss_val: 0.14230733270754295 acc_val: 0.9477611940298507\n",
      "epoch 26 / 50 loss_train: 0.1249165165015677 acc_train: 0.9518423507462687 loss_val: 0.14067160512482288 acc_val: 0.949160447761194\n",
      "epoch 27 / 50 loss_train: 0.1222242413283284 acc_train: 0.953008395522388 loss_val: 0.13900897542271876 acc_val: 0.9500932835820896\n",
      "epoch 28 / 50 loss_train: 0.11959345847257037 acc_train: 0.9538246268656716 loss_val: 0.13752462890586875 acc_val: 0.9496268656716418\n",
      "epoch 29 / 50 loss_train: 0.11708060366713177 acc_train: 0.9546408582089553 loss_val: 0.13609851630386782 acc_val: 0.9500932835820896\n",
      "epoch 30 / 50 loss_train: 0.114702652330135 acc_train: 0.9552238805970149 loss_val: 0.13470575247032032 acc_val: 0.9510261194029851\n",
      "epoch 31 / 50 loss_train: 0.11238082486149202 acc_train: 0.9556902985074627 loss_val: 0.13349072114280391 acc_val: 0.9510261194029851\n",
      "epoch 32 / 50 loss_train: 0.11020643906250818 acc_train: 0.9560401119402985 loss_val: 0.13214616113202907 acc_val: 0.9514925373134329\n",
      "epoch 33 / 50 loss_train: 0.10813636027985433 acc_train: 0.957089552238806 loss_val: 0.13107952465437467 acc_val: 0.9505597014925373\n",
      "epoch 34 / 50 loss_train: 0.10613078308124929 acc_train: 0.9580223880597015 loss_val: 0.1298068821548142 acc_val: 0.9519589552238806\n",
      "epoch 35 / 50 loss_train: 0.104219809230139 acc_train: 0.9581389925373134 loss_val: 0.12868615749718643 acc_val: 0.9538246268656716\n",
      "epoch 36 / 50 loss_train: 0.10230475737115563 acc_train: 0.9590718283582089 loss_val: 0.12771621256778512 acc_val: 0.9533582089552238\n",
      "epoch 37 / 50 loss_train: 0.10045400073529402 acc_train: 0.9605876865671642 loss_val: 0.12671013689507515 acc_val: 0.9542910447761194\n",
      "epoch 38 / 50 loss_train: 0.09871328350698659 acc_train: 0.9612873134328358 loss_val: 0.12598865109696397 acc_val: 0.9547574626865671\n",
      "epoch 39 / 50 loss_train: 0.09703680099015917 acc_train: 0.9618703358208955 loss_val: 0.12494364991791589 acc_val: 0.9566231343283582\n",
      "epoch 40 / 50 loss_train: 0.09542191187058811 acc_train: 0.9629197761194029 loss_val: 0.1243529242856583 acc_val: 0.957089552238806\n",
      "epoch 41 / 50 loss_train: 0.0938235713141178 acc_train: 0.9633861940298507 loss_val: 0.12334252722516183 acc_val: 0.957089552238806\n",
      "epoch 42 / 50 loss_train: 0.09234539455205981 acc_train: 0.9640858208955224 loss_val: 0.12261354947728631 acc_val: 0.957089552238806\n",
      "epoch 43 / 50 loss_train: 0.09085199366142946 acc_train: 0.964902052238806 loss_val: 0.12189456654392525 acc_val: 0.9575559701492538\n",
      "epoch 44 / 50 loss_train: 0.08943430532756914 acc_train: 0.9658348880597015 loss_val: 0.12104665308109347 acc_val: 0.9575559701492538\n",
      "epoch 45 / 50 loss_train: 0.08801987512322115 acc_train: 0.9668843283582089 loss_val: 0.12036443091926516 acc_val: 0.9580223880597015\n",
      "epoch 46 / 50 loss_train: 0.08668489798507306 acc_train: 0.9671175373134329 loss_val: 0.11977937526715106 acc_val: 0.9580223880597015\n",
      "epoch 47 / 50 loss_train: 0.08527999001888753 acc_train: 0.9677005597014925 loss_val: 0.11913652907838669 acc_val: 0.9580223880597015\n",
      "epoch 48 / 50 loss_train: 0.08403123082391528 acc_train: 0.9679337686567164 loss_val: 0.11834766205030105 acc_val: 0.9580223880597015\n",
      "epoch 49 / 50 loss_train: 0.08273225687722217 acc_train: 0.968633395522388 loss_val: 0.11779714323486573 acc_val: 0.9584888059701493\n",
      "Validation accuracy for lr 0.0001 bs 32 hl [50, 25] : 0.9584888059701493\n",
      "epoch 0 / 50 loss_train: 0.93139796915339 acc_train: 0.6226679104477612 loss_val: 0.71704703435969 acc_val: 0.6637126865671642\n",
      "epoch 1 / 50 loss_train: 0.5425459298291313 acc_train: 0.7854477611940298 loss_val: 0.4029431568820085 acc_val: 0.8572761194029851\n",
      "epoch 2 / 50 loss_train: 0.3530906880858229 acc_train: 0.8766324626865671 loss_val: 0.30251510052093816 acc_val: 0.8908582089552238\n",
      "epoch 3 / 50 loss_train: 0.28602597467712504 acc_train: 0.8975046641791045 loss_val: 0.25845502157097877 acc_val: 0.9043843283582089\n",
      "epoch 4 / 50 loss_train: 0.24941315595060587 acc_train: 0.9085820895522388 loss_val: 0.23094103657015003 acc_val: 0.9132462686567164\n",
      "epoch 5 / 50 loss_train: 0.2242935312244652 acc_train: 0.9156949626865671 loss_val: 0.2116909998976417 acc_val: 0.9183768656716418\n",
      "epoch 6 / 50 loss_train: 0.2054182207211852 acc_train: 0.9223414179104478 loss_val: 0.19719083835336088 acc_val: 0.9221082089552238\n",
      "epoch 7 / 50 loss_train: 0.19025541515325878 acc_train: 0.925722947761194 loss_val: 0.18625871845653086 acc_val: 0.9267723880597015\n",
      "epoch 8 / 50 loss_train: 0.17765448210455143 acc_train: 0.9313199626865671 loss_val: 0.177444131389858 acc_val: 0.9286380597014925\n",
      "epoch 9 / 50 loss_train: 0.16689165186748575 acc_train: 0.9338852611940298 loss_val: 0.1702933575151794 acc_val: 0.929570895522388\n",
      "epoch 10 / 50 loss_train: 0.15759584583592282 acc_train: 0.9380830223880597 loss_val: 0.1640802223970349 acc_val: 0.9365671641791045\n",
      "epoch 11 / 50 loss_train: 0.149536605062547 acc_train: 0.941464552238806 loss_val: 0.1589260494958526 acc_val: 0.9370335820895522\n",
      "epoch 12 / 50 loss_train: 0.14233871426349923 acc_train: 0.9441464552238806 loss_val: 0.15429525986425022 acc_val: 0.9388992537313433\n",
      "epoch 13 / 50 loss_train: 0.13577763285182082 acc_train: 0.9461287313432836 loss_val: 0.15004149095308203 acc_val: 0.941231343283582\n",
      "epoch 14 / 50 loss_train: 0.12975453498161663 acc_train: 0.9484608208955224 loss_val: 0.14619830778062365 acc_val: 0.9440298507462687\n",
      "epoch 15 / 50 loss_train: 0.12422103748948717 acc_train: 0.9506763059701493 loss_val: 0.14272218539054618 acc_val: 0.9430970149253731\n",
      "epoch 16 / 50 loss_train: 0.1192219543527923 acc_train: 0.9526585820895522 loss_val: 0.13991691219438399 acc_val: 0.9449626865671642\n",
      "epoch 17 / 50 loss_train: 0.11458518985422578 acc_train: 0.9541744402985075 loss_val: 0.1377242623631575 acc_val: 0.9444962686567164\n",
      "epoch 18 / 50 loss_train: 0.11028865118399818 acc_train: 0.9558069029850746 loss_val: 0.13574732377923854 acc_val: 0.945429104477612\n",
      "epoch 19 / 50 loss_train: 0.1063925345261484 acc_train: 0.9573227611940298 loss_val: 0.13395998397917633 acc_val: 0.9468283582089553\n",
      "epoch 20 / 50 loss_train: 0.10277597090479598 acc_train: 0.9587220149253731 loss_val: 0.1324536214162517 acc_val: 0.9486940298507462\n",
      "epoch 21 / 50 loss_train: 0.09938974864780903 acc_train: 0.9597714552238806 loss_val: 0.13105565479347558 acc_val: 0.9486940298507462\n",
      "epoch 22 / 50 loss_train: 0.09626628139139667 acc_train: 0.9612873134328358 loss_val: 0.1297767676863517 acc_val: 0.9505597014925373\n",
      "epoch 23 / 50 loss_train: 0.09326395342897957 acc_train: 0.9626865671641791 loss_val: 0.12874362991204913 acc_val: 0.9505597014925373\n",
      "epoch 24 / 50 loss_train: 0.09040276905862309 acc_train: 0.9637360074626866 loss_val: 0.12791097011277497 acc_val: 0.9519589552238806\n",
      "epoch 25 / 50 loss_train: 0.08775848749531572 acc_train: 0.9645522388059702 loss_val: 0.12728844928848834 acc_val: 0.9519589552238806\n",
      "epoch 26 / 50 loss_train: 0.08509471552939947 acc_train: 0.9659514925373134 loss_val: 0.12616785660892632 acc_val: 0.9528917910447762\n",
      "epoch 27 / 50 loss_train: 0.08261807782770093 acc_train: 0.9664179104477612 loss_val: 0.1257447511110133 acc_val: 0.9528917910447762\n",
      "epoch 28 / 50 loss_train: 0.08010617833334341 acc_train: 0.9681669776119403 loss_val: 0.12496664594853618 acc_val: 0.9524253731343284\n",
      "epoch 29 / 50 loss_train: 0.07769682036396533 acc_train: 0.9692164179104478 loss_val: 0.12426150505703885 acc_val: 0.9528917910447762\n",
      "epoch 30 / 50 loss_train: 0.07538765227707075 acc_train: 0.9703824626865671 loss_val: 0.12388701887281947 acc_val: 0.9533582089552238\n",
      "epoch 31 / 50 loss_train: 0.07313010698868963 acc_train: 0.9704990671641791 loss_val: 0.12355225403251355 acc_val: 0.9533582089552238\n",
      "epoch 32 / 50 loss_train: 0.07093981646154243 acc_train: 0.9714319029850746 loss_val: 0.12304750569137468 acc_val: 0.9538246268656716\n",
      "epoch 33 / 50 loss_train: 0.06883912123635468 acc_train: 0.9723647388059702 loss_val: 0.12277742823862049 acc_val: 0.9547574626865671\n",
      "epoch 34 / 50 loss_train: 0.0666676322015614 acc_train: 0.9736473880597015 loss_val: 0.12286271248031828 acc_val: 0.9556902985074627\n",
      "epoch 35 / 50 loss_train: 0.06474905614150382 acc_train: 0.9741138059701493 loss_val: 0.12303176285388757 acc_val: 0.9575559701492538\n",
      "epoch 36 / 50 loss_train: 0.06281943059241427 acc_train: 0.9755130597014925 loss_val: 0.12343425226378446 acc_val: 0.9580223880597015\n",
      "epoch 37 / 50 loss_train: 0.061006156950077016 acc_train: 0.9763292910447762 loss_val: 0.12376731722279023 acc_val: 0.9575559701492538\n",
      "epoch 38 / 50 loss_train: 0.05915743448213785 acc_train: 0.9771455223880597 loss_val: 0.12393402537219557 acc_val: 0.9584888059701493\n",
      "epoch 39 / 50 loss_train: 0.05738207582148042 acc_train: 0.9778451492537313 loss_val: 0.12411736628836646 acc_val: 0.9594216417910447\n",
      "epoch 40 / 50 loss_train: 0.05562792591352725 acc_train: 0.9784281716417911 loss_val: 0.1246706948409763 acc_val: 0.9589552238805971\n",
      "epoch 41 / 50 loss_train: 0.05401630258993871 acc_train: 0.9792444029850746 loss_val: 0.12512987288479535 acc_val: 0.9598880597014925\n",
      "epoch 42 / 50 loss_train: 0.05240358562215662 acc_train: 0.980293843283582 loss_val: 0.12564860670732825 acc_val: 0.9594216417910447\n",
      "epoch 43 / 50 loss_train: 0.05083887982521026 acc_train: 0.9813432835820896 loss_val: 0.12603906781194088 acc_val: 0.9598880597014925\n",
      "epoch 44 / 50 loss_train: 0.04933061731941025 acc_train: 0.9825093283582089 loss_val: 0.127002295609856 acc_val: 0.9584888059701493\n",
      "epoch 45 / 50 loss_train: 0.04790842750977634 acc_train: 0.9833255597014925 loss_val: 0.12732866518673136 acc_val: 0.9594216417910447\n",
      "epoch 46 / 50 loss_train: 0.046493909789558585 acc_train: 0.9840251865671642 loss_val: 0.12780573604582718 acc_val: 0.9594216417910447\n",
      "epoch 47 / 50 loss_train: 0.04502410757761108 acc_train: 0.984491604477612 loss_val: 0.12836721928231656 acc_val: 0.9598880597014925\n",
      "epoch 48 / 50 loss_train: 0.043748121207785 acc_train: 0.9847248134328358 loss_val: 0.12904617041732938 acc_val: 0.9580223880597015\n",
      "epoch 49 / 50 loss_train: 0.04244337431628625 acc_train: 0.9854244402985075 loss_val: 0.12956536249319212 acc_val: 0.9584888059701493\n",
      "Validation accuracy for lr 0.0001 bs 32 hl [100, 50, 25] : 0.9584888059701493\n",
      "epoch 0 / 50 loss_train: 1.1324468815504616 acc_train: -0.0078125 loss_val: 1.0925249619917436 acc_val: 0.2755681818181818\n",
      "epoch 1 / 50 loss_train: 1.0542163906702355 acc_train: 0.4103311567164179 loss_val: 0.9965939612099619 acc_val: 0.49573863636363635\n",
      "epoch 2 / 50 loss_train: 0.9385671731251389 acc_train: 0.5258861940298507 loss_val: 0.8659566080931461 acc_val: 0.5625\n",
      "epoch 3 / 50 loss_train: 0.8029683703806862 acc_train: 0.629081156716418 loss_val: 0.7325367241194753 acc_val: 0.7159090909090909\n",
      "epoch 4 / 50 loss_train: 0.6705033703526454 acc_train: 0.7842817164179104 loss_val: 0.6113942057797404 acc_val: 0.8229166666666666\n",
      "epoch 5 / 50 loss_train: 0.5598850392583591 acc_train: 0.8376865671641791 loss_val: 0.5177926366979425 acc_val: 0.8423295454545454\n",
      "epoch 6 / 50 loss_train: 0.4805927207665657 acc_train: 0.8530783582089553 loss_val: 0.45212479974284314 acc_val: 0.8527462121212122\n",
      "epoch 7 / 50 loss_train: 0.42564426384755033 acc_train: 0.8677705223880597 loss_val: 0.4056903751510562 acc_val: 0.8626893939393939\n",
      "epoch 8 / 50 loss_train: 0.3865241528446995 acc_train: 0.8741837686567164 loss_val: 0.3715577660636468 acc_val: 0.8716856060606061\n",
      "epoch 9 / 50 loss_train: 0.35749391466379166 acc_train: 0.8810634328358209 loss_val: 0.3455265819123297 acc_val: 0.8783143939393939\n",
      "epoch 10 / 50 loss_train: 0.3351308050662724 acc_train: 0.8856110074626866 loss_val: 0.32512374064235977 acc_val: 0.8849431818181818\n",
      "epoch 11 / 50 loss_train: 0.3172347580096615 acc_train: 0.890741604477612 loss_val: 0.30849927934733307 acc_val: 0.8863636363636364\n",
      "epoch 12 / 50 loss_train: 0.302359991220396 acc_train: 0.8937733208955224 loss_val: 0.29455708181767754 acc_val: 0.8877840909090909\n",
      "epoch 13 / 50 loss_train: 0.28973784553470894 acc_train: 0.8968050373134329 loss_val: 0.28268542637427646 acc_val: 0.8948863636363636\n",
      "epoch 14 / 50 loss_train: 0.27877732310722125 acc_train: 0.9001865671641791 loss_val: 0.27241507545113564 acc_val: 0.8977272727272727\n",
      "epoch 15 / 50 loss_train: 0.26907700533742335 acc_train: 0.9034514925373134 loss_val: 0.2633535912203969 acc_val: 0.9015151515151515\n",
      "epoch 16 / 50 loss_train: 0.2602761444538387 acc_train: 0.9054337686567164 loss_val: 0.2551610917417389 acc_val: 0.9034090909090909\n",
      "epoch 17 / 50 loss_train: 0.2521883609023557 acc_train: 0.9086986940298507 loss_val: 0.2477936049302419 acc_val: 0.9067234848484849\n",
      "epoch 18 / 50 loss_train: 0.24476356420721582 acc_train: 0.9116138059701493 loss_val: 0.2409776910349275 acc_val: 0.9109848484848485\n",
      "epoch 19 / 50 loss_train: 0.23791125505717833 acc_train: 0.9130130597014925 loss_val: 0.23481353583999656 acc_val: 0.9128787878787878\n",
      "epoch 20 / 50 loss_train: 0.23162052157654692 acc_train: 0.9127798507462687 loss_val: 0.2292770031331615 acc_val: 0.9147727272727273\n",
      "epoch 21 / 50 loss_train: 0.2258075848221779 acc_train: 0.9146455223880597 loss_val: 0.22420617607845503 acc_val: 0.9185606060606061\n",
      "epoch 22 / 50 loss_train: 0.22047650780695588 acc_train: 0.9175606343283582 loss_val: 0.21961418138534733 acc_val: 0.9199810606060606\n",
      "epoch 23 / 50 loss_train: 0.21557381004095078 acc_train: 0.9190764925373134 loss_val: 0.21539982113366327 acc_val: 0.921875\n",
      "epoch 24 / 50 loss_train: 0.21106027594920415 acc_train: 0.9203591417910447 loss_val: 0.2115983371199532 acc_val: 0.9223484848484849\n",
      "epoch 25 / 50 loss_train: 0.2068648679710146 acc_train: 0.9214085820895522 loss_val: 0.20807290328384348 acc_val: 0.9242424242424242\n",
      "epoch 26 / 50 loss_train: 0.2029344167059927 acc_train: 0.9238572761194029 loss_val: 0.20477850639233083 acc_val: 0.9266098484848485\n",
      "epoch 27 / 50 loss_train: 0.19919206457796382 acc_train: 0.9254897388059702 loss_val: 0.20177922914312643 acc_val: 0.9285037878787878\n",
      "epoch 28 / 50 loss_train: 0.19565644893628448 acc_train: 0.9261893656716418 loss_val: 0.19885994291469228 acc_val: 0.9299242424242424\n",
      "epoch 29 / 50 loss_train: 0.19225217557665128 acc_train: 0.9268889925373134 loss_val: 0.1961368860846216 acc_val: 0.9303977272727273\n",
      "epoch 30 / 50 loss_train: 0.189032893627882 acc_train: 0.9287546641791045 loss_val: 0.19357529654393368 acc_val: 0.9327651515151515\n",
      "epoch 31 / 50 loss_train: 0.18596513497073258 acc_train: 0.9302705223880597 loss_val: 0.19109532088887962 acc_val: 0.9322916666666666\n",
      "epoch 32 / 50 loss_train: 0.18299838850524888 acc_train: 0.9312033582089553 loss_val: 0.18873324242392273 acc_val: 0.9318181818181818\n",
      "epoch 33 / 50 loss_train: 0.18017146525098315 acc_train: 0.9322527985074627 loss_val: 0.18655200706879524 acc_val: 0.9327651515151515\n",
      "epoch 34 / 50 loss_train: 0.17746061787231646 acc_train: 0.9333022388059702 loss_val: 0.18444580326181356 acc_val: 0.9341856060606061\n",
      "epoch 35 / 50 loss_train: 0.17485972421605195 acc_train: 0.9340018656716418 loss_val: 0.1824427256925088 acc_val: 0.9346590909090909\n",
      "epoch 36 / 50 loss_train: 0.1723371860918714 acc_train: 0.9350513059701493 loss_val: 0.18051548049587643 acc_val: 0.9351325757575758\n",
      "epoch 37 / 50 loss_train: 0.16992896467224874 acc_train: 0.9365671641791045 loss_val: 0.1786615417980956 acc_val: 0.9360795454545454\n",
      "epoch 38 / 50 loss_train: 0.1676076126854811 acc_train: 0.9370335820895522 loss_val: 0.17693702978166667 acc_val: 0.9360795454545454\n",
      "epoch 39 / 50 loss_train: 0.16539136904166707 acc_train: 0.937383395522388 loss_val: 0.1752819074423645 acc_val: 0.9360795454545454\n",
      "epoch 40 / 50 loss_train: 0.16323819656425448 acc_train: 0.9383162313432836 loss_val: 0.17373765011275696 acc_val: 0.9365530303030303\n",
      "epoch 41 / 50 loss_train: 0.1611619216681861 acc_train: 0.9381996268656716 loss_val: 0.17224105125977518 acc_val: 0.9370265151515151\n",
      "epoch 42 / 50 loss_train: 0.1591710647039894 acc_train: 0.9390158582089553 loss_val: 0.17083491282502303 acc_val: 0.9379734848484849\n",
      "epoch 43 / 50 loss_train: 0.15724521088622398 acc_train: 0.9398320895522388 loss_val: 0.1694507165141686 acc_val: 0.9384469696969697\n",
      "epoch 44 / 50 loss_train: 0.15538047862920298 acc_train: 0.9406483208955224 loss_val: 0.16815742600187095 acc_val: 0.9393939393939394\n",
      "epoch 45 / 50 loss_train: 0.1535593912895046 acc_train: 0.941347947761194 loss_val: 0.16694441980816369 acc_val: 0.9398674242424242\n",
      "epoch 46 / 50 loss_train: 0.15182089130046653 acc_train: 0.9422807835820896 loss_val: 0.16578543123923187 acc_val: 0.9398674242424242\n",
      "epoch 47 / 50 loss_train: 0.15010071743224093 acc_train: 0.9422807835820896 loss_val: 0.1646113880860591 acc_val: 0.9408143939393939\n",
      "epoch 48 / 50 loss_train: 0.14843010413112925 acc_train: 0.9426305970149254 loss_val: 0.16354101142055835 acc_val: 0.9408143939393939\n",
      "epoch 49 / 50 loss_train: 0.14683276645616808 acc_train: 0.9430970149253731 loss_val: 0.16249324585431058 acc_val: 0.9412878787878788\n",
      "Validation accuracy for lr 0.0001 bs 64 hl [30, 15] : 0.9412878787878788\n",
      "epoch 0 / 50 loss_train: 0.9599983398594073 acc_train: 0.5753264925373134 loss_val: 0.8763125827818206 acc_val: 0.6889204545454546\n",
      "epoch 1 / 50 loss_train: 0.7886349545485938 acc_train: 0.734258395522388 loss_val: 0.7038472845698848 acc_val: 0.7755681818181818\n",
      "epoch 2 / 50 loss_train: 0.6286995408695135 acc_train: 0.8018889925373134 loss_val: 0.5587800416079435 acc_val: 0.8167613636363636\n",
      "epoch 3 / 50 loss_train: 0.5058341540062605 acc_train: 0.840018656716418 loss_val: 0.45603903405594104 acc_val: 0.8456439393939394\n",
      "epoch 4 / 50 loss_train: 0.423439825315084 acc_train: 0.863222947761194 loss_val: 0.3914145612806985 acc_val: 0.8669507575757576\n",
      "epoch 5 / 50 loss_train: 0.3708165692749308 acc_train: 0.8752332089552238 loss_val: 0.34884183515201916 acc_val: 0.8764204545454546\n",
      "epoch 6 / 50 loss_train: 0.3355071038674952 acc_train: 0.8842117537313433 loss_val: 0.3194414826505112 acc_val: 0.884469696969697\n",
      "epoch 7 / 50 loss_train: 0.31019687975075705 acc_train: 0.8884095149253731 loss_val: 0.2973865480021094 acc_val: 0.8915719696969697\n",
      "epoch 8 / 50 loss_train: 0.29088003071608826 acc_train: 0.8935401119402985 loss_val: 0.28027063399327523 acc_val: 0.896780303030303\n",
      "epoch 9 / 50 loss_train: 0.27542891264406605 acc_train: 0.9008861940298507 loss_val: 0.266447505321015 acc_val: 0.9024621212121212\n",
      "epoch 10 / 50 loss_train: 0.2623628136048566 acc_train: 0.9054337686567164 loss_val: 0.2548709387016116 acc_val: 0.90625\n",
      "epoch 11 / 50 loss_train: 0.25075703303315744 acc_train: 0.9098647388059702 loss_val: 0.24468413785551535 acc_val: 0.9114583333333334\n",
      "epoch 12 / 50 loss_train: 0.24006777251167083 acc_train: 0.9138292910447762 loss_val: 0.23534417951084446 acc_val: 0.9119318181818182\n",
      "epoch 13 / 50 loss_train: 0.2302086118108301 acc_train: 0.9168610074626866 loss_val: 0.2268138105670611 acc_val: 0.9161931818181818\n",
      "epoch 14 / 50 loss_train: 0.2213044278879664 acc_train: 0.9203591417910447 loss_val: 0.21921968540515413 acc_val: 0.9204545454545454\n",
      "epoch 15 / 50 loss_train: 0.21322676300335286 acc_train: 0.9226912313432836 loss_val: 0.21242111277851192 acc_val: 0.9209280303030303\n",
      "epoch 16 / 50 loss_train: 0.20591143229559286 acc_train: 0.9245569029850746 loss_val: 0.20622526978453 acc_val: 0.9223484848484849\n",
      "epoch 17 / 50 loss_train: 0.1991836549853211 acc_train: 0.9263059701492538 loss_val: 0.20055643348446625 acc_val: 0.9270833333333334\n",
      "epoch 18 / 50 loss_train: 0.1930177841168731 acc_train: 0.9277052238805971 loss_val: 0.1954509786711159 acc_val: 0.9294507575757576\n",
      "epoch 19 / 50 loss_train: 0.1874046783838699 acc_train: 0.9300373134328358 loss_val: 0.19086970391711502 acc_val: 0.9308712121212122\n",
      "epoch 20 / 50 loss_train: 0.1822170478972926 acc_train: 0.9307369402985075 loss_val: 0.1866726211526177 acc_val: 0.9308712121212122\n",
      "epoch 21 / 50 loss_train: 0.17735319832034074 acc_train: 0.9324860074626866 loss_val: 0.18275385575764108 acc_val: 0.9341856060606061\n",
      "epoch 22 / 50 loss_train: 0.17275457326044788 acc_train: 0.9340018656716418 loss_val: 0.1792935378757785 acc_val: 0.9360795454545454\n",
      "epoch 23 / 50 loss_train: 0.16848517709703587 acc_train: 0.9362173507462687 loss_val: 0.17613821560128903 acc_val: 0.9365530303030303\n",
      "epoch 24 / 50 loss_train: 0.16450005024671555 acc_train: 0.9370335820895522 loss_val: 0.17321295281484103 acc_val: 0.9389204545454546\n",
      "epoch 25 / 50 loss_train: 0.1607955823471742 acc_train: 0.9379664179104478 loss_val: 0.17052874386938516 acc_val: 0.9389204545454546\n",
      "epoch 26 / 50 loss_train: 0.15734774002165935 acc_train: 0.9387826492537313 loss_val: 0.16802931059448217 acc_val: 0.9393939393939394\n",
      "epoch 27 / 50 loss_train: 0.1541052206572312 acc_train: 0.9399486940298507 loss_val: 0.1656277697572174 acc_val: 0.9408143939393939\n",
      "epoch 28 / 50 loss_train: 0.15106723296330937 acc_train: 0.941347947761194 loss_val: 0.1633908252335845 acc_val: 0.9412878787878788\n",
      "epoch 29 / 50 loss_train: 0.14814606072631345 acc_train: 0.9426305970149254 loss_val: 0.16139078688028155 acc_val: 0.9427083333333334\n",
      "epoch 30 / 50 loss_train: 0.14537264845931708 acc_train: 0.9440298507462687 loss_val: 0.15946134191295697 acc_val: 0.9427083333333334\n",
      "epoch 31 / 50 loss_train: 0.14273681911403563 acc_train: 0.9446128731343284 loss_val: 0.15771865542311306 acc_val: 0.943655303030303\n",
      "epoch 32 / 50 loss_train: 0.1402372190200571 acc_train: 0.9450792910447762 loss_val: 0.15605800099773193 acc_val: 0.9441287878787878\n",
      "epoch 33 / 50 loss_train: 0.13785924601243504 acc_train: 0.9457789179104478 loss_val: 0.15449996060377805 acc_val: 0.9450757575757576\n",
      "epoch 34 / 50 loss_train: 0.13560370942438715 acc_train: 0.9462453358208955 loss_val: 0.15300860483407494 acc_val: 0.9450757575757576\n",
      "epoch 35 / 50 loss_train: 0.13345474015865752 acc_train: 0.9472947761194029 loss_val: 0.15153700006670656 acc_val: 0.9446022727272727\n",
      "epoch 36 / 50 loss_train: 0.13140334722711078 acc_train: 0.9483442164179104 loss_val: 0.15019506668349178 acc_val: 0.9464962121212122\n",
      "epoch 37 / 50 loss_train: 0.1294232994707218 acc_train: 0.949393656716418 loss_val: 0.14889131471246594 acc_val: 0.946969696969697\n",
      "epoch 38 / 50 loss_train: 0.12751200658950343 acc_train: 0.9498600746268657 loss_val: 0.14767967385396882 acc_val: 0.9479166666666666\n",
      "epoch 39 / 50 loss_train: 0.12568763743585615 acc_train: 0.9507929104477612 loss_val: 0.14650385579575828 acc_val: 0.9474431818181818\n",
      "epoch 40 / 50 loss_train: 0.12392896082975081 acc_train: 0.9511427238805971 loss_val: 0.14540730503645274 acc_val: 0.9474431818181818\n",
      "epoch 41 / 50 loss_train: 0.12222607789644555 acc_train: 0.9516091417910447 loss_val: 0.14429348305169365 acc_val: 0.9474431818181818\n",
      "epoch 42 / 50 loss_train: 0.12059646473724896 acc_train: 0.9525419776119403 loss_val: 0.14329247268809317 acc_val: 0.9474431818181818\n",
      "epoch 43 / 50 loss_train: 0.1190232397265621 acc_train: 0.9527751865671642 loss_val: 0.14223856493216855 acc_val: 0.946969696969697\n",
      "epoch 44 / 50 loss_train: 0.11748274203056275 acc_train: 0.9538246268656716 loss_val: 0.1412721527502237 acc_val: 0.9479166666666666\n",
      "epoch 45 / 50 loss_train: 0.11597557914957626 acc_train: 0.9545242537313433 loss_val: 0.1402905068207474 acc_val: 0.9483901515151515\n",
      "epoch 46 / 50 loss_train: 0.11451964650247524 acc_train: 0.9549906716417911 loss_val: 0.1392763859231226 acc_val: 0.9493371212121212\n",
      "epoch 47 / 50 loss_train: 0.11307803891710382 acc_train: 0.9554570895522388 loss_val: 0.13837765192015053 acc_val: 0.9502840909090909\n",
      "epoch 48 / 50 loss_train: 0.11169773087239088 acc_train: 0.9563899253731343 loss_val: 0.13744488785111267 acc_val: 0.9512310606060606\n",
      "epoch 49 / 50 loss_train: 0.11026766479237755 acc_train: 0.957206156716418 loss_val: 0.13668162302862832 acc_val: 0.9517045454545454\n",
      "Validation accuracy for lr 0.0001 bs 64 hl [50, 25] : 0.9517045454545454\n",
      "epoch 0 / 50 loss_train: 1.0321011787919856 acc_train: 0.531133395522388 loss_val: 0.939288590893601 acc_val: 0.5700757575757576\n",
      "epoch 1 / 50 loss_train: 0.7698337409033704 acc_train: 0.7210820895522388 loss_val: 0.6013094674457203 acc_val: 0.8053977272727273\n",
      "epoch 2 / 50 loss_train: 0.49392973398094747 acc_train: 0.8392024253731343 loss_val: 0.40731108233784186 acc_val: 0.8598484848484849\n",
      "epoch 3 / 50 loss_train: 0.3677462919879316 acc_train: 0.8731343283582089 loss_val: 0.32902741138682223 acc_val: 0.8839962121212122\n",
      "epoch 4 / 50 loss_train: 0.31101056266186844 acc_train: 0.8894589552238806 loss_val: 0.28841829345081793 acc_val: 0.8958333333333334\n",
      "epoch 5 / 50 loss_train: 0.2773828904575376 acc_train: 0.8971548507462687 loss_val: 0.2615816696462306 acc_val: 0.9038825757575758\n",
      "epoch 6 / 50 loss_train: 0.2533384661621122 acc_train: 0.9064832089552238 loss_val: 0.2419885860360933 acc_val: 0.9086174242424242\n",
      "epoch 7 / 50 loss_train: 0.23467671509776541 acc_train: 0.9127798507462687 loss_val: 0.22680148500666925 acc_val: 0.9147727272727273\n",
      "epoch 8 / 50 loss_train: 0.21931347879233645 acc_train: 0.918027052238806 loss_val: 0.21451381372400757 acc_val: 0.9185606060606061\n",
      "epoch 9 / 50 loss_train: 0.20612711220312474 acc_train: 0.9235074626865671 loss_val: 0.20455103486099027 acc_val: 0.9237689393939394\n",
      "epoch 10 / 50 loss_train: 0.19464704021811485 acc_train: 0.925956156716418 loss_val: 0.19608532534347792 acc_val: 0.9247159090909091\n",
      "epoch 11 / 50 loss_train: 0.1845229184672014 acc_train: 0.9299207089552238 loss_val: 0.18879715764937413 acc_val: 0.9285037878787878\n",
      "epoch 12 / 50 loss_train: 0.17568682189752807 acc_train: 0.9333022388059702 loss_val: 0.18246392684523016 acc_val: 0.9322916666666666\n",
      "epoch 13 / 50 loss_train: 0.16789560386938834 acc_train: 0.9370335820895522 loss_val: 0.17681778641770396 acc_val: 0.9332386363636364\n",
      "epoch 14 / 50 loss_train: 0.160893927842601 acc_train: 0.9394822761194029 loss_val: 0.17183692230327957 acc_val: 0.9337121212121212\n",
      "epoch 15 / 50 loss_train: 0.15455512830348156 acc_train: 0.9426305970149254 loss_val: 0.16722178418069344 acc_val: 0.9346590909090909\n",
      "epoch 16 / 50 loss_train: 0.14871578007491668 acc_train: 0.9439132462686567 loss_val: 0.16303773646976685 acc_val: 0.9346590909090909\n",
      "epoch 17 / 50 loss_train: 0.14347250342591486 acc_train: 0.9450792910447762 loss_val: 0.15944439841691885 acc_val: 0.9379734848484849\n",
      "epoch 18 / 50 loss_train: 0.13871101723678075 acc_train: 0.9469449626865671 loss_val: 0.1561145904195269 acc_val: 0.9384469696969697\n",
      "epoch 19 / 50 loss_train: 0.13437576191638834 acc_train: 0.949277052238806 loss_val: 0.15329355581850518 acc_val: 0.9379734848484849\n",
      "epoch 20 / 50 loss_train: 0.1304024170194544 acc_train: 0.9504430970149254 loss_val: 0.1508192949073749 acc_val: 0.9379734848484849\n",
      "epoch 21 / 50 loss_train: 0.12670923377484528 acc_train: 0.9514925373134329 loss_val: 0.14860357005531885 acc_val: 0.9375\n",
      "epoch 22 / 50 loss_train: 0.12328927833308924 acc_train: 0.953241604477612 loss_val: 0.14662872290522722 acc_val: 0.9389204545454546\n",
      "epoch 23 / 50 loss_train: 0.12000501022410037 acc_train: 0.9541744402985075 loss_val: 0.14490341639678014 acc_val: 0.9417613636363636\n",
      "epoch 24 / 50 loss_train: 0.11692054644783041 acc_train: 0.9551072761194029 loss_val: 0.14322038006855908 acc_val: 0.9431818181818182\n",
      "epoch 25 / 50 loss_train: 0.11391132783645125 acc_train: 0.9562733208955224 loss_val: 0.14179268495376385 acc_val: 0.943655303030303\n",
      "epoch 26 / 50 loss_train: 0.11104339432082515 acc_train: 0.9574393656716418 loss_val: 0.14041998358372826 acc_val: 0.9446022727272727\n",
      "epoch 27 / 50 loss_train: 0.10832951334652616 acc_train: 0.9583722014925373 loss_val: 0.13917325302276312 acc_val: 0.9450757575757576\n",
      "epoch 28 / 50 loss_train: 0.10574226512281752 acc_train: 0.9591884328358209 loss_val: 0.13798308851013039 acc_val: 0.9455492424242424\n",
      "epoch 29 / 50 loss_train: 0.10327026772020913 acc_train: 0.9602378731343284 loss_val: 0.13681629968998482 acc_val: 0.9464962121212122\n",
      "epoch 30 / 50 loss_train: 0.10088115054836023 acc_train: 0.9612873134328358 loss_val: 0.13578896152874664 acc_val: 0.9474431818181818\n",
      "epoch 31 / 50 loss_train: 0.09857387073449235 acc_train: 0.9617537313432836 loss_val: 0.1347273203057491 acc_val: 0.9479166666666666\n",
      "epoch 32 / 50 loss_train: 0.09632013574131389 acc_train: 0.9626865671641791 loss_val: 0.13380247925581926 acc_val: 0.9483901515151515\n",
      "epoch 33 / 50 loss_train: 0.0941352027454483 acc_train: 0.9631529850746269 loss_val: 0.13285344623788356 acc_val: 0.9498106060606061\n",
      "epoch 34 / 50 loss_train: 0.09202864136435647 acc_train: 0.9643190298507462 loss_val: 0.13195949466981718 acc_val: 0.9493371212121212\n",
      "epoch 35 / 50 loss_train: 0.08994336858558566 acc_train: 0.9651352611940298 loss_val: 0.13116548445456525 acc_val: 0.9498106060606061\n",
      "epoch 36 / 50 loss_train: 0.08792457176344608 acc_train: 0.9663013059701493 loss_val: 0.1304479816907119 acc_val: 0.9502840909090909\n",
      "epoch 37 / 50 loss_train: 0.0859547654139017 acc_train: 0.9672341417910447 loss_val: 0.12967835888899903 acc_val: 0.9517045454545454\n",
      "epoch 38 / 50 loss_train: 0.08403467100614043 acc_train: 0.9678171641791045 loss_val: 0.12882553258976148 acc_val: 0.953125\n",
      "epoch 39 / 50 loss_train: 0.08216676213292044 acc_train: 0.9685167910447762 loss_val: 0.12801888989636637 acc_val: 0.9526515151515151\n",
      "epoch 40 / 50 loss_train: 0.08038350586682114 acc_train: 0.9696828358208955 loss_val: 0.12729078497569857 acc_val: 0.953125\n",
      "epoch 41 / 50 loss_train: 0.0786911208293776 acc_train: 0.9702658582089553 loss_val: 0.12681937480706576 acc_val: 0.9535984848484849\n",
      "epoch 42 / 50 loss_train: 0.07695111984025631 acc_train: 0.9709654850746269 loss_val: 0.12625708405568564 acc_val: 0.9535984848484849\n",
      "epoch 43 / 50 loss_train: 0.07530089496259591 acc_train: 0.9721315298507462 loss_val: 0.12589345256737666 acc_val: 0.9535984848484849\n",
      "epoch 44 / 50 loss_train: 0.07371193982327162 acc_train: 0.9729477611940298 loss_val: 0.12521269195519147 acc_val: 0.9535984848484849\n",
      "epoch 45 / 50 loss_train: 0.07214025970298185 acc_train: 0.9734141791044776 loss_val: 0.12478326241357811 acc_val: 0.953125\n",
      "epoch 46 / 50 loss_train: 0.07056599346670642 acc_train: 0.9742304104477612 loss_val: 0.12450113174615622 acc_val: 0.9535984848484849\n",
      "epoch 47 / 50 loss_train: 0.06906669218537968 acc_train: 0.9746968283582089 loss_val: 0.12410344281373441 acc_val: 0.9540719696969697\n",
      "epoch 48 / 50 loss_train: 0.06759200439051684 acc_train: 0.9756296641791045 loss_val: 0.12384120864019775 acc_val: 0.9535984848484849\n",
      "epoch 49 / 50 loss_train: 0.06614340519977373 acc_train: 0.9758628731343284 loss_val: 0.12344854003479765 acc_val: 0.9535984848484849\n",
      "Validation accuracy for lr 0.0001 bs 64 hl [100, 50, 25] : 0.9535984848484849\n",
      "epoch 0 / 50 loss_train: 1.0821050601219064 acc_train: 0.27180503731343286 loss_val: 1.0583645924925804 acc_val: 0.453125\n",
      "epoch 1 / 50 loss_train: 1.0256758821544363 acc_train: 0.5326492537313433 loss_val: 1.0059097483754158 acc_val: 0.51171875\n",
      "epoch 2 / 50 loss_train: 0.968201205801608 acc_train: 0.5619169776119403 loss_val: 0.9504924528300762 acc_val: 0.54052734375\n",
      "epoch 3 / 50 loss_train: 0.9089515582839055 acc_train: 0.589902052238806 loss_val: 0.8938488177955151 acc_val: 0.57470703125\n",
      "epoch 4 / 50 loss_train: 0.8503768230552105 acc_train: 0.6178871268656716 loss_val: 0.836701437830925 acc_val: 0.60595703125\n",
      "epoch 5 / 50 loss_train: 0.7933013216773076 acc_train: 0.6518190298507462 loss_val: 0.779912244528532 acc_val: 0.64111328125\n",
      "epoch 6 / 50 loss_train: 0.7375731468200684 acc_train: 0.6948460820895522 loss_val: 0.7221261337399483 acc_val: 0.68505859375\n",
      "epoch 7 / 50 loss_train: 0.6820349880118868 acc_train: 0.7491837686567164 loss_val: 0.6653473284095526 acc_val: 0.7490234375\n",
      "epoch 8 / 50 loss_train: 0.6282736534502968 acc_train: 0.7980410447761194 loss_val: 0.6117712371051311 acc_val: 0.79833984375\n",
      "epoch 9 / 50 loss_train: 0.5780290236223989 acc_train: 0.8288246268656716 loss_val: 0.5626082345843315 acc_val: 0.82568359375\n",
      "epoch 10 / 50 loss_train: 0.5315902989302108 acc_train: 0.8461986940298507 loss_val: 0.5167859261855483 acc_val: 0.84423828125\n",
      "epoch 11 / 50 loss_train: 0.4872154575675281 acc_train: 0.8617070895522388 loss_val: 0.4728217711672187 acc_val: 0.85498046875\n",
      "epoch 12 / 50 loss_train: 0.4450273865194463 acc_train: 0.8680037313432836 loss_val: 0.43310241401195526 acc_val: 0.861328125\n",
      "epoch 13 / 50 loss_train: 0.410376544318982 acc_train: 0.8745335820895522 loss_val: 0.40291934460401535 acc_val: 0.86669921875\n",
      "epoch 14 / 50 loss_train: 0.3828571349827211 acc_train: 0.8793143656716418 loss_val: 0.3784700743854046 acc_val: 0.8720703125\n",
      "epoch 15 / 50 loss_train: 0.3604733610331123 acc_train: 0.8845615671641791 loss_val: 0.3582967589609325 acc_val: 0.87744140625\n",
      "epoch 16 / 50 loss_train: 0.341806379271977 acc_train: 0.8878264925373134 loss_val: 0.3411598443053663 acc_val: 0.8837890625\n",
      "epoch 17 / 50 loss_train: 0.3258132282922517 acc_train: 0.8915578358208955 loss_val: 0.3261100207455456 acc_val: 0.8857421875\n",
      "epoch 18 / 50 loss_train: 0.31201521785401587 acc_train: 0.8937733208955224 loss_val: 0.3134505362249911 acc_val: 0.88916015625\n",
      "epoch 19 / 50 loss_train: 0.30042854196100094 acc_train: 0.8958722014925373 loss_val: 0.3027927805669606 acc_val: 0.890625\n",
      "epoch 20 / 50 loss_train: 0.29034762951865123 acc_train: 0.8998367537313433 loss_val: 0.29340087017044425 acc_val: 0.8955078125\n",
      "epoch 21 / 50 loss_train: 0.28139875673536047 acc_train: 0.902402052238806 loss_val: 0.2849525627680123 acc_val: 0.89599609375\n",
      "epoch 22 / 50 loss_train: 0.2733827113215603 acc_train: 0.9038013059701493 loss_val: 0.2773771828506142 acc_val: 0.900390625\n",
      "epoch 23 / 50 loss_train: 0.2660853642581114 acc_train: 0.9056669776119403 loss_val: 0.2704362324438989 acc_val: 0.9033203125\n",
      "epoch 24 / 50 loss_train: 0.25938537294295294 acc_train: 0.9074160447761194 loss_val: 0.26412591873668134 acc_val: 0.9052734375\n",
      "epoch 25 / 50 loss_train: 0.25316623609457445 acc_train: 0.9086986940298507 loss_val: 0.2582801559474319 acc_val: 0.90576171875\n",
      "epoch 26 / 50 loss_train: 0.2473696594807639 acc_train: 0.9113805970149254 loss_val: 0.2529798748437315 acc_val: 0.90771484375\n",
      "epoch 27 / 50 loss_train: 0.24197840601650636 acc_train: 0.9128964552238806 loss_val: 0.24805697449482977 acc_val: 0.90869140625\n",
      "epoch 28 / 50 loss_train: 0.23693741257510967 acc_train: 0.9145289179104478 loss_val: 0.24353974719997495 acc_val: 0.9091796875\n",
      "epoch 29 / 50 loss_train: 0.23218606946183673 acc_train: 0.9165111940298507 loss_val: 0.239347786991857 acc_val: 0.9111328125\n",
      "epoch 30 / 50 loss_train: 0.2277118645942033 acc_train: 0.9182602611940298 loss_val: 0.23547739966306835 acc_val: 0.912109375\n",
      "epoch 31 / 50 loss_train: 0.2234753499280161 acc_train: 0.9195429104477612 loss_val: 0.231852060649544 acc_val: 0.91162109375\n",
      "epoch 32 / 50 loss_train: 0.21948165835729286 acc_train: 0.9201259328358209 loss_val: 0.2285119480220601 acc_val: 0.9150390625\n",
      "epoch 33 / 50 loss_train: 0.21572902442804023 acc_train: 0.9216417910447762 loss_val: 0.22540174459572881 acc_val: 0.91748046875\n",
      "epoch 34 / 50 loss_train: 0.21216049323331065 acc_train: 0.921875 loss_val: 0.22247254371177405 acc_val: 0.91845703125\n",
      "epoch 35 / 50 loss_train: 0.20879984057661313 acc_train: 0.9225746268656716 loss_val: 0.21973810566123575 acc_val: 0.92041015625\n",
      "epoch 36 / 50 loss_train: 0.20559776763417828 acc_train: 0.9230410447761194 loss_val: 0.21719591179862618 acc_val: 0.92041015625\n",
      "epoch 37 / 50 loss_train: 0.20255627405287616 acc_train: 0.9232742537313433 loss_val: 0.21481594000943005 acc_val: 0.92138671875\n",
      "epoch 38 / 50 loss_train: 0.19968864099303288 acc_train: 0.9236240671641791 loss_val: 0.21258271497208625 acc_val: 0.921875\n",
      "epoch 39 / 50 loss_train: 0.19694245095128443 acc_train: 0.9240904850746269 loss_val: 0.2104278173064813 acc_val: 0.92236328125\n",
      "epoch 40 / 50 loss_train: 0.1943065164916551 acc_train: 0.9242070895522388 loss_val: 0.20838426239788532 acc_val: 0.92333984375\n",
      "epoch 41 / 50 loss_train: 0.19179153353420655 acc_train: 0.9254897388059702 loss_val: 0.20643928414210677 acc_val: 0.92431640625\n",
      "epoch 42 / 50 loss_train: 0.18934442959169842 acc_train: 0.9274720149253731 loss_val: 0.20458813989534974 acc_val: 0.92431640625\n",
      "epoch 43 / 50 loss_train: 0.18700807141279108 acc_train: 0.9287546641791045 loss_val: 0.20277115376666188 acc_val: 0.92529296875\n",
      "epoch 44 / 50 loss_train: 0.18475305867284092 acc_train: 0.9299207089552238 loss_val: 0.2010326933232136 acc_val: 0.92626953125\n",
      "epoch 45 / 50 loss_train: 0.18259255843820857 acc_train: 0.9307369402985075 loss_val: 0.1993869910365902 acc_val: 0.92578125\n",
      "epoch 46 / 50 loss_train: 0.18051210557346914 acc_train: 0.9314365671641791 loss_val: 0.19780737231485546 acc_val: 0.92529296875\n",
      "epoch 47 / 50 loss_train: 0.17848033613678235 acc_train: 0.9317863805970149 loss_val: 0.19624225341249257 acc_val: 0.92529296875\n",
      "epoch 48 / 50 loss_train: 0.17654220492982153 acc_train: 0.9321361940298507 loss_val: 0.1947569597978145 acc_val: 0.92578125\n",
      "epoch 49 / 50 loss_train: 0.17466172086658763 acc_train: 0.9326026119402985 loss_val: 0.19333558736252598 acc_val: 0.92626953125\n",
      "Validation accuracy for lr 0.0001 bs 128 hl [30, 15] : 0.92626953125\n",
      "epoch 0 / 50 loss_train: 1.153669866163339 acc_train: -0.11205690298507463 loss_val: 1.1135207638144493 acc_val: 0.10009765625\n",
      "epoch 1 / 50 loss_train: 1.0766684297305436 acc_train: 0.3416511194029851 loss_val: 1.032174989581108 acc_val: 0.41259765625\n",
      "epoch 2 / 50 loss_train: 0.9760282093019628 acc_train: 0.4686333955223881 loss_val: 0.9240127205848694 acc_val: 0.57373046875\n",
      "epoch 3 / 50 loss_train: 0.8624059397782853 acc_train: 0.6535680970149254 loss_val: 0.8158664219081402 acc_val: 0.68408203125\n",
      "epoch 4 / 50 loss_train: 0.7545439796661263 acc_train: 0.7280783582089553 loss_val: 0.7154453061521053 acc_val: 0.73974609375\n",
      "epoch 5 / 50 loss_train: 0.6594800628832916 acc_train: 0.7714552238805971 loss_val: 0.6278117075562477 acc_val: 0.78076171875\n",
      "epoch 6 / 50 loss_train: 0.5793061149654104 acc_train: 0.8093516791044776 loss_val: 0.553734052926302 acc_val: 0.810546875\n",
      "epoch 7 / 50 loss_train: 0.5128596998862366 acc_train: 0.8387360074626866 loss_val: 0.4923489782959223 acc_val: 0.8359375\n",
      "epoch 8 / 50 loss_train: 0.458776119929641 acc_train: 0.8565764925373134 loss_val: 0.44333263020962477 acc_val: 0.84912109375\n",
      "epoch 9 / 50 loss_train: 0.4164678325403982 acc_train: 0.8678871268656716 loss_val: 0.4052762845531106 acc_val: 0.859375\n",
      "epoch 10 / 50 loss_train: 0.38370785873327684 acc_train: 0.8733675373134329 loss_val: 0.3756966427899897 acc_val: 0.86767578125\n",
      "epoch 11 / 50 loss_train: 0.35799319664044166 acc_train: 0.8797807835820896 loss_val: 0.35215615620836616 acc_val: 0.87451171875\n",
      "epoch 12 / 50 loss_train: 0.33721920024992813 acc_train: 0.8857276119402985 loss_val: 0.3327575037255883 acc_val: 0.87841796875\n",
      "epoch 13 / 50 loss_train: 0.3200812184098941 acc_train: 0.8901585820895522 loss_val: 0.31677069747820497 acc_val: 0.88427734375\n",
      "epoch 14 / 50 loss_train: 0.3057206895814013 acc_train: 0.894356343283582 loss_val: 0.303219866938889 acc_val: 0.890625\n",
      "epoch 15 / 50 loss_train: 0.2933392680403012 acc_train: 0.8982042910447762 loss_val: 0.29152065026573837 acc_val: 0.8955078125\n",
      "epoch 16 / 50 loss_train: 0.28251142599689427 acc_train: 0.9007695895522388 loss_val: 0.281425867928192 acc_val: 0.900390625\n",
      "epoch 17 / 50 loss_train: 0.2729450840113768 acc_train: 0.9035680970149254 loss_val: 0.2724343524314463 acc_val: 0.90283203125\n",
      "epoch 18 / 50 loss_train: 0.26438667725271253 acc_train: 0.9057835820895522 loss_val: 0.2644124865764752 acc_val: 0.90576171875\n",
      "epoch 19 / 50 loss_train: 0.25668899158933267 acc_train: 0.9081156716417911 loss_val: 0.2572609316557646 acc_val: 0.90869140625\n",
      "epoch 20 / 50 loss_train: 0.24969683076018717 acc_train: 0.9095149253731343 loss_val: 0.25071782094892114 acc_val: 0.9091796875\n",
      "epoch 21 / 50 loss_train: 0.24322516504508346 acc_train: 0.9113805970149254 loss_val: 0.24472849466837943 acc_val: 0.91064453125\n",
      "epoch 22 / 50 loss_train: 0.23720876716855746 acc_train: 0.9137126865671642 loss_val: 0.23923540150281042 acc_val: 0.91259765625\n",
      "epoch 23 / 50 loss_train: 0.2315914223888027 acc_train: 0.9155783582089553 loss_val: 0.23416976362932473 acc_val: 0.91259765625\n",
      "epoch 24 / 50 loss_train: 0.2262903861145475 acc_train: 0.9162779850746269 loss_val: 0.22941186546813697 acc_val: 0.9130859375\n",
      "epoch 25 / 50 loss_train: 0.22130598950741895 acc_train: 0.9176772388059702 loss_val: 0.224986125016585 acc_val: 0.91259765625\n",
      "epoch 26 / 50 loss_train: 0.21658529363461396 acc_train: 0.9190764925373134 loss_val: 0.22090726025635377 acc_val: 0.9140625\n",
      "epoch 27 / 50 loss_train: 0.21213178536785182 acc_train: 0.9193097014925373 loss_val: 0.21705898485379294 acc_val: 0.91650390625\n",
      "epoch 28 / 50 loss_train: 0.20790837771856963 acc_train: 0.9208255597014925 loss_val: 0.2135146201471798 acc_val: 0.91943359375\n",
      "epoch 29 / 50 loss_train: 0.2038975787251743 acc_train: 0.9212919776119403 loss_val: 0.2101516563561745 acc_val: 0.92236328125\n",
      "epoch 30 / 50 loss_train: 0.2000817698774053 acc_train: 0.921991604477612 loss_val: 0.20699665934080258 acc_val: 0.92236328125\n",
      "epoch 31 / 50 loss_train: 0.19643196196698431 acc_train: 0.9226912313432836 loss_val: 0.20395861665019765 acc_val: 0.92333984375\n",
      "epoch 32 / 50 loss_train: 0.192892606245048 acc_train: 0.9239738805970149 loss_val: 0.20103848379221745 acc_val: 0.92333984375\n",
      "epoch 33 / 50 loss_train: 0.18949090122286952 acc_train: 0.9253731343283582 loss_val: 0.1982247690029908 acc_val: 0.923828125\n",
      "epoch 34 / 50 loss_train: 0.1862389627677291 acc_train: 0.9261893656716418 loss_val: 0.19555511797079816 acc_val: 0.92431640625\n",
      "epoch 35 / 50 loss_train: 0.18308863032664827 acc_train: 0.9275886194029851 loss_val: 0.19301007883041166 acc_val: 0.92431640625\n",
      "epoch 36 / 50 loss_train: 0.18000129588059524 acc_train: 0.9285214552238806 loss_val: 0.19059683158411644 acc_val: 0.92431640625\n",
      "epoch 37 / 50 loss_train: 0.17701226606297848 acc_train: 0.9293376865671642 loss_val: 0.18826125530176796 acc_val: 0.92626953125\n",
      "epoch 38 / 50 loss_train: 0.1741480311351036 acc_train: 0.9305037313432836 loss_val: 0.18604144037817605 acc_val: 0.92724609375\n",
      "epoch 39 / 50 loss_train: 0.1714219636659124 acc_train: 0.9321361940298507 loss_val: 0.18398134119343013 acc_val: 0.92626953125\n",
      "epoch 40 / 50 loss_train: 0.1687958371728214 acc_train: 0.9338852611940298 loss_val: 0.18204498913837597 acc_val: 0.92626953125\n",
      "epoch 41 / 50 loss_train: 0.16626226023506763 acc_train: 0.9345848880597015 loss_val: 0.18014275745372288 acc_val: 0.92626953125\n",
      "epoch 42 / 50 loss_train: 0.16385718001358546 acc_train: 0.9357509328358209 loss_val: 0.1783684760594042 acc_val: 0.9267578125\n",
      "epoch 43 / 50 loss_train: 0.1615470125159221 acc_train: 0.9366837686567164 loss_val: 0.17660540240467526 acc_val: 0.9267578125\n",
      "epoch 44 / 50 loss_train: 0.15929359431142237 acc_train: 0.9385494402985075 loss_val: 0.1749407966126455 acc_val: 0.927734375\n",
      "epoch 45 / 50 loss_train: 0.1571328268567128 acc_train: 0.9388992537313433 loss_val: 0.17331018946424592 acc_val: 0.92919921875\n",
      "epoch 46 / 50 loss_train: 0.15506756094409457 acc_train: 0.9397154850746269 loss_val: 0.17178905512264464 acc_val: 0.9296875\n",
      "epoch 47 / 50 loss_train: 0.15306752608783208 acc_train: 0.9407649253731343 loss_val: 0.17024282984493766 acc_val: 0.93017578125\n",
      "epoch 48 / 50 loss_train: 0.15113138207303944 acc_train: 0.941347947761194 loss_val: 0.16887050874356646 acc_val: 0.93017578125\n",
      "epoch 49 / 50 loss_train: 0.14923896740621595 acc_train: 0.9426305970149254 loss_val: 0.1675606338394573 acc_val: 0.9296875\n",
      "Validation accuracy for lr 0.0001 bs 128 hl [50, 25] : 0.9296875\n",
      "epoch 0 / 50 loss_train: 1.0362370192114987 acc_train: 0.5109608208955224 loss_val: 1.013344693928957 acc_val: 0.47802734375\n",
      "epoch 1 / 50 loss_train: 0.9467865593397795 acc_train: 0.5114272388059702 loss_val: 0.8991817757487297 acc_val: 0.48046875\n",
      "epoch 2 / 50 loss_train: 0.804616085628965 acc_train: 0.6265158582089553 loss_val: 0.73555519990623 acc_val: 0.72900390625\n",
      "epoch 3 / 50 loss_train: 0.6376378803110835 acc_train: 0.788945895522388 loss_val: 0.5697109941393137 acc_val: 0.81787109375\n",
      "epoch 4 / 50 loss_train: 0.49972069930674423 acc_train: 0.8443330223880597 loss_val: 0.4524524286389351 acc_val: 0.84765625\n",
      "epoch 5 / 50 loss_train: 0.40884710025431503 acc_train: 0.8687033582089553 loss_val: 0.3782903654500842 acc_val: 0.87158203125\n",
      "epoch 6 / 50 loss_train: 0.3513338708165866 acc_train: 0.8850279850746269 loss_val: 0.33054354786872864 acc_val: 0.88525390625\n",
      "epoch 7 / 50 loss_train: 0.3127471566645067 acc_train: 0.8942397388059702 loss_val: 0.29761113971471786 acc_val: 0.89453125\n",
      "epoch 8 / 50 loss_train: 0.2847247730884979 acc_train: 0.9034514925373134 loss_val: 0.27361591276712716 acc_val: 0.9033203125\n",
      "epoch 9 / 50 loss_train: 0.2635279469080825 acc_train: 0.9081156716417911 loss_val: 0.2558507790090516 acc_val: 0.91015625\n",
      "epoch 10 / 50 loss_train: 0.246783903508044 acc_train: 0.913945895522388 loss_val: 0.24204818869475275 acc_val: 0.9150390625\n",
      "epoch 11 / 50 loss_train: 0.23306778918451337 acc_train: 0.917793843283582 loss_val: 0.23092983703827485 acc_val: 0.91943359375\n",
      "epoch 12 / 50 loss_train: 0.22139955056247426 acc_train: 0.9198927238805971 loss_val: 0.22162448172457516 acc_val: 0.9208984375\n",
      "epoch 13 / 50 loss_train: 0.21135396477001817 acc_train: 0.921991604477612 loss_val: 0.21368457842618227 acc_val: 0.92529296875\n",
      "epoch 14 / 50 loss_train: 0.202419086512345 acc_train: 0.925956156716418 loss_val: 0.206556701567024 acc_val: 0.92724609375\n",
      "epoch 15 / 50 loss_train: 0.19432607615616784 acc_train: 0.9272388059701493 loss_val: 0.20017903635744005 acc_val: 0.927734375\n",
      "epoch 16 / 50 loss_train: 0.18695659001371753 acc_train: 0.9287546641791045 loss_val: 0.1943668169551529 acc_val: 0.9287109375\n",
      "epoch 17 / 50 loss_train: 0.18021845105868667 acc_train: 0.9305037313432836 loss_val: 0.18916152435122058 acc_val: 0.9296875\n",
      "epoch 18 / 50 loss_train: 0.17401164291954752 acc_train: 0.9323694029850746 loss_val: 0.18428109990782104 acc_val: 0.931640625\n",
      "epoch 19 / 50 loss_train: 0.16834549550245057 acc_train: 0.9340018656716418 loss_val: 0.17971480661071837 acc_val: 0.931640625\n",
      "epoch 20 / 50 loss_train: 0.16302409734743745 acc_train: 0.9362173507462687 loss_val: 0.17544817391899414 acc_val: 0.9326171875\n",
      "epoch 21 / 50 loss_train: 0.15803889574399635 acc_train: 0.937616604477612 loss_val: 0.17161421080527361 acc_val: 0.93310546875\n",
      "epoch 22 / 50 loss_train: 0.1534425620712451 acc_train: 0.9395988805970149 loss_val: 0.16804897809925023 acc_val: 0.9345703125\n",
      "epoch 23 / 50 loss_train: 0.14909631569883716 acc_train: 0.9406483208955224 loss_val: 0.16476588054501917 acc_val: 0.935546875\n",
      "epoch 24 / 50 loss_train: 0.14500357622086113 acc_train: 0.9418143656716418 loss_val: 0.1617693170410348 acc_val: 0.93798828125\n",
      "epoch 25 / 50 loss_train: 0.14118532669633183 acc_train: 0.9434468283582089 loss_val: 0.15899896330665797 acc_val: 0.93896484375\n",
      "epoch 26 / 50 loss_train: 0.13755604913875238 acc_train: 0.9457789179104478 loss_val: 0.15646999676391715 acc_val: 0.939453125\n",
      "epoch 27 / 50 loss_train: 0.13416258067782247 acc_train: 0.9468283582089553 loss_val: 0.15411369573848788 acc_val: 0.94091796875\n",
      "epoch 28 / 50 loss_train: 0.13096708348437922 acc_train: 0.9484608208955224 loss_val: 0.15192863648553612 acc_val: 0.94287109375\n",
      "epoch 29 / 50 loss_train: 0.12793882091098757 acc_train: 0.949277052238806 loss_val: 0.1498555071593728 acc_val: 0.9443359375\n",
      "epoch 30 / 50 loss_train: 0.1250294016590759 acc_train: 0.9499766791044776 loss_val: 0.14793149784236448 acc_val: 0.9443359375\n",
      "epoch 31 / 50 loss_train: 0.12225817477525171 acc_train: 0.9511427238805971 loss_val: 0.1460510946199065 acc_val: 0.9443359375\n",
      "epoch 32 / 50 loss_train: 0.11958725483559851 acc_train: 0.9520755597014925 loss_val: 0.1443185535390512 acc_val: 0.94384765625\n",
      "epoch 33 / 50 loss_train: 0.11700214829240273 acc_train: 0.9534748134328358 loss_val: 0.14275278292552684 acc_val: 0.9443359375\n",
      "epoch 34 / 50 loss_train: 0.11456927351319968 acc_train: 0.9546408582089553 loss_val: 0.14131637049649726 acc_val: 0.94482421875\n",
      "epoch 35 / 50 loss_train: 0.11226196992975562 acc_train: 0.9556902985074627 loss_val: 0.13984346908546286 acc_val: 0.9462890625\n",
      "epoch 36 / 50 loss_train: 0.11001958837037656 acc_train: 0.9566231343283582 loss_val: 0.1384992979856179 acc_val: 0.94677734375\n",
      "epoch 37 / 50 loss_train: 0.10785450619548115 acc_train: 0.9577891791044776 loss_val: 0.13730333441344555 acc_val: 0.9482421875\n",
      "epoch 38 / 50 loss_train: 0.10576672373867746 acc_train: 0.9590718283582089 loss_val: 0.13613629315477738 acc_val: 0.94970703125\n",
      "epoch 39 / 50 loss_train: 0.10376649066361028 acc_train: 0.9597714552238806 loss_val: 0.13500257320811215 acc_val: 0.95068359375\n",
      "epoch 40 / 50 loss_train: 0.10181732486877869 acc_train: 0.9602378731343284 loss_val: 0.13393361254202318 acc_val: 0.951171875\n",
      "epoch 41 / 50 loss_train: 0.09991281613040326 acc_train: 0.9612873134328358 loss_val: 0.13293205250738538 acc_val: 0.95263671875\n",
      "epoch 42 / 50 loss_train: 0.09805590981867776 acc_train: 0.9616371268656716 loss_val: 0.13200031213636976 acc_val: 0.953125\n",
      "epoch 43 / 50 loss_train: 0.09628170595240237 acc_train: 0.9619869402985075 loss_val: 0.13111711732926778 acc_val: 0.955078125\n",
      "epoch 44 / 50 loss_train: 0.09454512879697245 acc_train: 0.9624533582089553 loss_val: 0.13026638204974006 acc_val: 0.95556640625\n",
      "epoch 45 / 50 loss_train: 0.09281306986266108 acc_train: 0.9632695895522388 loss_val: 0.12949244292303774 acc_val: 0.9560546875\n",
      "epoch 46 / 50 loss_train: 0.0911473537892548 acc_train: 0.9640858208955224 loss_val: 0.12867240175000916 acc_val: 0.95556640625\n",
      "epoch 47 / 50 loss_train: 0.08952080458402634 acc_train: 0.9652518656716418 loss_val: 0.12783541622911798 acc_val: 0.955078125\n",
      "epoch 48 / 50 loss_train: 0.08792308289835703 acc_train: 0.9657182835820896 loss_val: 0.12700886521997745 acc_val: 0.95556640625\n",
      "epoch 49 / 50 loss_train: 0.08633243453814023 acc_train: 0.9665345149253731 loss_val: 0.1262430593001227 acc_val: 0.95556640625\n",
      "Validation accuracy for lr 0.0001 bs 128 hl [100, 50, 25] : 0.95556640625\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.1, 0.01, 0.001, 0.0001]:\n",
    "    for batch_size in [32, 64, 128]:\n",
    "        for hidden_layers in [[30,15],[50, 25], [100, 50, 25]]:\n",
    "            writer = SummaryWriter(comment=f\"lr_{lr}_bs_{batch_size}_layers_{hidden_layers}\")\n",
    "            model = create_model(X.shape[1], hidden_layers, Y.shape[1])\n",
    "            optimizer = torch.optim.Adam( params = model.parameters(), lr=lr )\n",
    "            loss_fn   = torch.nn.CrossEntropyLoss()\n",
    "            \n",
    "            losses_train           = []\n",
    "            accuracies_train       = []\n",
    "            losses_validation      = []\n",
    "            accuracies_validation  = []\n",
    "            n_epochs = 50\n",
    "\n",
    "            for i in range (n_epochs):\n",
    "                metrics_train = train_one_epoch( i, indices_train, X, Y, optimizer, loss_fn, batch_size, writer )\n",
    "                losses_train.append(metrics_train[0])\n",
    "                accuracies_train.append(metrics_train[1])\n",
    "                metrics_val = validate_one_epoch( i, indices_validation, X, Y, model, loss_fn, batch_size, writer )\n",
    "                losses_validation.append(metrics_val[0])\n",
    "                accuracies_validation.append(metrics_val[1])\n",
    "                print(\"epoch\",i,\"/\",n_epochs,\"loss_train:\",metrics_train[0],\"acc_train:\",metrics_train[1],\"loss_val:\",metrics_val[0],\"acc_val:\",metrics_val[1])\n",
    "\n",
    "            print(\"Validation accuracy for lr\",lr,\"bs\",batch_size,\"hl\",hidden_layers,\":\",accuracies_validation[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 / 50 loss_train: 1.047047646149345 acc_train: -0.0695652274981789 loss_val: 0.976997305949529 acc_val: 0.29999999205271405\n",
      "epoch 1 / 50 loss_train: 0.9152113354724386 acc_train: 0.4956521625104158 loss_val: 0.8485518793265024 acc_val: 0.5666666602094969\n",
      "epoch 2 / 50 loss_train: 0.7387337710546411 acc_train: 0.7478260812552079 loss_val: 0.7074128886063894 acc_val: 0.6666666542490324\n",
      "epoch 3 / 50 loss_train: 0.5613305128138998 acc_train: 0.8260869532823563 loss_val: 0.5963465571403503 acc_val: 0.6666666567325592\n",
      "epoch 4 / 50 loss_train: 0.4280287986216338 acc_train: 0.8695652147997981 loss_val: 0.527994729578495 acc_val: 0.6666666567325592\n",
      "epoch 5 / 50 loss_train: 0.3421871901854225 acc_train: 0.8869565200546513 loss_val: 0.48523737117648125 acc_val: 0.7333333293596903\n",
      "epoch 6 / 50 loss_train: 0.28436223352732864 acc_train: 0.9130434769651165 loss_val: 0.4550500263770421 acc_val: 0.7666666631897291\n",
      "epoch 7 / 50 loss_train: 0.24079678159045137 acc_train: 0.9217391292686048 loss_val: 0.42869753018021584 acc_val: 0.7999999970197678\n",
      "epoch 8 / 50 loss_train: 0.20477587227588115 acc_train: 0.9304347815720931 loss_val: 0.4016030002385378 acc_val: 0.7999999970197678\n",
      "epoch 9 / 50 loss_train: 0.1727671690447175 acc_train: 0.9652173907860465 loss_val: 0.3750650243212779 acc_val: 0.8333333308498064\n",
      "epoch 10 / 50 loss_train: 0.14486975336204405 acc_train: 0.973913043089535 loss_val: 0.35091757277647656 acc_val: 0.7999999970197678\n",
      "epoch 11 / 50 loss_train: 0.12207081622403601 acc_train: 0.9826086953930233 loss_val: 0.33022981230169535 acc_val: 0.7999999970197678\n",
      "epoch 12 / 50 loss_train: 0.10375728940024324 acc_train: 0.9826086953930233 loss_val: 0.313881092860053 acc_val: 0.7999999970197678\n",
      "epoch 13 / 50 loss_train: 0.0891342556670956 acc_train: 0.9826086953930233 loss_val: 0.3007604433223605 acc_val: 0.7999999970197678\n",
      "epoch 14 / 50 loss_train: 0.07773573769499427 acc_train: 0.9826086953930233 loss_val: 0.2906378204934299 acc_val: 0.7999999970197678\n",
      "epoch 15 / 50 loss_train: 0.06871022332621657 acc_train: 0.9826086953930233 loss_val: 0.28243907541036606 acc_val: 0.8333333308498064\n",
      "epoch 16 / 50 loss_train: 0.06148429172437476 acc_train: 0.9913043476965117 loss_val: 0.27637377676243585 acc_val: 0.8333333308498064\n",
      "epoch 17 / 50 loss_train: 0.055688365787753595 acc_train: 0.9913043476965117 loss_val: 0.2712567205308005 acc_val: 0.8333333308498064\n",
      "epoch 18 / 50 loss_train: 0.05092693084810415 acc_train: 0.9913043476965117 loss_val: 0.26723988133016974 acc_val: 0.8333333308498064\n",
      "epoch 19 / 50 loss_train: 0.04691058873077449 acc_train: 0.9913043476965117 loss_val: 0.26362266690315056 acc_val: 0.8333333308498064\n",
      "epoch 20 / 50 loss_train: 0.04349616654824628 acc_train: 0.9913043476965117 loss_val: 0.2603883106106271 acc_val: 0.8333333308498064\n",
      "epoch 21 / 50 loss_train: 0.04052169751578375 acc_train: 1.0 loss_val: 0.25757193251047283 acc_val: 0.8333333308498064\n",
      "epoch 22 / 50 loss_train: 0.03788534102394529 acc_train: 1.0 loss_val: 0.25538902238865074 acc_val: 0.8333333308498064\n",
      "epoch 23 / 50 loss_train: 0.03556878109077882 acc_train: 1.0 loss_val: 0.2535166146505314 acc_val: 0.8333333308498064\n",
      "epoch 24 / 50 loss_train: 0.033475055490903884 acc_train: 1.0 loss_val: 0.25182969600427896 acc_val: 0.8333333308498064\n",
      "epoch 25 / 50 loss_train: 0.031612240152595485 acc_train: 1.0 loss_val: 0.2503180699616981 acc_val: 0.8666666646798452\n",
      "epoch 26 / 50 loss_train: 0.029886269537003143 acc_train: 1.0 loss_val: 0.2487387714597086 acc_val: 0.8666666646798452\n",
      "epoch 27 / 50 loss_train: 0.028308698345664077 acc_train: 1.0 loss_val: 0.2473005770104161 acc_val: 0.8666666646798452\n",
      "epoch 28 / 50 loss_train: 0.026828345397244328 acc_train: 1.0 loss_val: 0.24578572791263772 acc_val: 0.8666666646798452\n",
      "epoch 29 / 50 loss_train: 0.02569453921371504 acc_train: 1.0 loss_val: 0.24467362826302028 acc_val: 0.8666666646798452\n",
      "epoch 30 / 50 loss_train: 0.024466188515678212 acc_train: 1.0 loss_val: 0.2436610713969761 acc_val: 0.8666666646798452\n",
      "epoch 31 / 50 loss_train: 0.023444972955115627 acc_train: 1.0 loss_val: 0.2426490962582951 acc_val: 0.8666666646798452\n",
      "epoch 32 / 50 loss_train: 0.022427751996513944 acc_train: 1.0 loss_val: 0.24172603242914192 acc_val: 0.8666666646798452\n",
      "epoch 33 / 50 loss_train: 0.021461960738119873 acc_train: 1.0 loss_val: 0.24057900639794147 acc_val: 0.8666666646798452\n",
      "epoch 34 / 50 loss_train: 0.020687398784186527 acc_train: 1.0 loss_val: 0.23989758453293084 acc_val: 0.8666666646798452\n",
      "epoch 35 / 50 loss_train: 0.019806771232928757 acc_train: 1.0 loss_val: 0.23898616300236122 acc_val: 0.8666666646798452\n",
      "epoch 36 / 50 loss_train: 0.019105887655472463 acc_train: 1.0 loss_val: 0.23766487566172145 acc_val: 0.8666666646798452\n",
      "epoch 37 / 50 loss_train: 0.018375493755624833 acc_train: 1.0 loss_val: 0.23681320727822217 acc_val: 0.8666666646798452\n",
      "epoch 38 / 50 loss_train: 0.017650668357458453 acc_train: 1.0 loss_val: 0.23556989036054196 acc_val: 0.8666666646798452\n",
      "epoch 39 / 50 loss_train: 0.017094556895165664 acc_train: 1.0 loss_val: 0.23471270180986417 acc_val: 0.8666666646798452\n",
      "epoch 40 / 50 loss_train: 0.01645284338914992 acc_train: 1.0 loss_val: 0.23368281154641105 acc_val: 0.8666666646798452\n",
      "epoch 41 / 50 loss_train: 0.01583731838162867 acc_train: 1.0 loss_val: 0.23222018653177656 acc_val: 0.8666666646798452\n",
      "epoch 42 / 50 loss_train: 0.01536394493102663 acc_train: 1.0 loss_val: 0.23143867268421067 acc_val: 0.8666666646798452\n",
      "epoch 43 / 50 loss_train: 0.014796670808446715 acc_train: 1.0 loss_val: 0.23002979321366487 acc_val: 0.8666666646798452\n",
      "epoch 44 / 50 loss_train: 0.014278806205687073 acc_train: 1.0 loss_val: 0.2287374339600016 acc_val: 0.8666666646798452\n",
      "epoch 45 / 50 loss_train: 0.01384935259019308 acc_train: 1.0 loss_val: 0.22793953959626378 acc_val: 0.8666666646798452\n",
      "epoch 46 / 50 loss_train: 0.01335040184304766 acc_train: 1.0 loss_val: 0.22647932973632123 acc_val: 0.8666666646798452\n",
      "epoch 47 / 50 loss_train: 0.012890577827499288 acc_train: 1.0 loss_val: 0.22527267497450035 acc_val: 0.8666666646798452\n",
      "epoch 48 / 50 loss_train: 0.012507873262106643 acc_train: 1.0 loss_val: 0.22423898810423756 acc_val: 0.8666666646798452\n",
      "epoch 49 / 50 loss_train: 0.012075154925696552 acc_train: 1.0 loss_val: 0.22320743920621075 acc_val: 0.8666666646798452\n",
      "Validation accuracy for lr 0.001 bs 5 hl [50, 25] : 0.8666666646798452\n",
      "epoch 0 / 50 loss_train: 1.0258828142414922 acc_train: 0.33043476939201355 loss_val: 0.9236880739529928 acc_val: 0.6333333303531011\n",
      "epoch 1 / 50 loss_train: 0.7429082704626996 acc_train: 0.7391304289517195 loss_val: 0.6540636966625849 acc_val: 0.6999999955296516\n",
      "epoch 2 / 50 loss_train: 0.437064523930135 acc_train: 0.8608695624963097 loss_val: 0.4743528018395106 acc_val: 0.7666666607062022\n",
      "epoch 3 / 50 loss_train: 0.27088633276846097 acc_train: 0.9130434769651165 loss_val: 0.4018562603741884 acc_val: 0.7999999970197678\n",
      "epoch 4 / 50 loss_train: 0.18234231727926628 acc_train: 0.9565217384825582 loss_val: 0.3589572391162316 acc_val: 0.8333333308498064\n",
      "epoch 5 / 50 loss_train: 0.12616515847975793 acc_train: 0.9652173907860465 loss_val: 0.3238140791654587 acc_val: 0.8333333308498064\n",
      "epoch 6 / 50 loss_train: 0.08968830740322238 acc_train: 0.9826086953930233 loss_val: 0.3012208570726216 acc_val: 0.8666666646798452\n",
      "epoch 7 / 50 loss_train: 0.06596240198806576 acc_train: 0.9913043476965117 loss_val: 0.2897146209919204 acc_val: 0.8666666646798452\n",
      "epoch 8 / 50 loss_train: 0.050969619020495724 acc_train: 0.9913043476965117 loss_val: 0.2833437924273312 acc_val: 0.7666666607062022\n",
      "epoch 9 / 50 loss_train: 0.04128065361114948 acc_train: 1.0 loss_val: 0.2791122352549185 acc_val: 0.7666666607062022\n",
      "epoch 10 / 50 loss_train: 0.03442643673928535 acc_train: 1.0 loss_val: 0.27511167429232347 acc_val: 0.7999999945362409\n",
      "epoch 11 / 50 loss_train: 0.02926650686877901 acc_train: 1.0 loss_val: 0.27187019359553233 acc_val: 0.7999999945362409\n",
      "epoch 12 / 50 loss_train: 0.025386179235520893 acc_train: 1.0 loss_val: 0.26923388211677474 acc_val: 0.7999999945362409\n",
      "epoch 13 / 50 loss_train: 0.0221712558945317 acc_train: 1.0 loss_val: 0.26709450414637104 acc_val: 0.7999999945362409\n",
      "epoch 14 / 50 loss_train: 0.019666991510387997 acc_train: 1.0 loss_val: 0.26631342544957687 acc_val: 0.7999999945362409\n",
      "epoch 15 / 50 loss_train: 0.017419964983634163 acc_train: 1.0 loss_val: 0.2648732577993845 acc_val: 0.7999999945362409\n",
      "epoch 16 / 50 loss_train: 0.015460218406667042 acc_train: 1.0 loss_val: 0.263282773240159 acc_val: 0.7999999945362409\n",
      "epoch 17 / 50 loss_train: 0.013890277918266214 acc_train: 1.0 loss_val: 0.26145480750710703 acc_val: 0.7999999945362409\n",
      "epoch 18 / 50 loss_train: 0.012415798036786526 acc_train: 1.0 loss_val: 0.2598613916876881 acc_val: 0.7999999945362409\n",
      "epoch 19 / 50 loss_train: 0.010964500259749753 acc_train: 1.0 loss_val: 0.26049120018918376 acc_val: 0.7999999945362409\n",
      "epoch 20 / 50 loss_train: 0.010353223262506577 acc_train: 1.0 loss_val: 0.25906197260095115 acc_val: 0.7999999945362409\n",
      "epoch 21 / 50 loss_train: 0.008964011071087874 acc_train: 1.0 loss_val: 0.25751622109479894 acc_val: 0.7999999945362409\n",
      "epoch 22 / 50 loss_train: 0.008022254679615246 acc_train: 1.0 loss_val: 0.25732097409976024 acc_val: 0.7999999945362409\n",
      "epoch 23 / 50 loss_train: 0.007343306766731826 acc_train: 1.0 loss_val: 0.25580591356022825 acc_val: 0.8333333283662796\n",
      "epoch 24 / 50 loss_train: 0.006489201134409877 acc_train: 1.0 loss_val: 0.25672161291731754 acc_val: 0.8333333283662796\n",
      "epoch 25 / 50 loss_train: 0.0060128383034759245 acc_train: 1.0 loss_val: 0.25390137164261734 acc_val: 0.8333333283662796\n",
      "epoch 26 / 50 loss_train: 0.00541242159417142 acc_train: 1.0 loss_val: 0.2551546118047554 acc_val: 0.8333333283662796\n",
      "epoch 27 / 50 loss_train: 0.004999677730349662 acc_train: 1.0 loss_val: 0.25341025842741755 acc_val: 0.8666666646798452\n",
      "epoch 28 / 50 loss_train: 0.004432970336537161 acc_train: 1.0 loss_val: 0.2571794512574949 acc_val: 0.8666666621963183\n",
      "epoch 29 / 50 loss_train: 0.004198432240746511 acc_train: 1.0 loss_val: 0.2528397347875095 acc_val: 0.8999999985098839\n",
      "epoch 30 / 50 loss_train: 0.0037233373716488763 acc_train: 1.0 loss_val: 0.2496466198184256 acc_val: 0.8666666621963183\n",
      "epoch 31 / 50 loss_train: 0.0034564955023360317 acc_train: 1.0 loss_val: 0.24240005426690914 acc_val: 0.8999999985098839\n",
      "epoch 32 / 50 loss_train: 0.003072989229226748 acc_train: 1.0 loss_val: 0.24724635153233976 acc_val: 0.8666666621963183\n",
      "epoch 33 / 50 loss_train: 0.0029204171414280804 acc_train: 1.0 loss_val: 0.2434573272757916 acc_val: 0.8999999985098839\n",
      "epoch 34 / 50 loss_train: 0.0025923644598546875 acc_train: 1.0 loss_val: 0.2501936613775797 acc_val: 0.8666666621963183\n",
      "epoch 35 / 50 loss_train: 0.0024715942840291814 acc_train: 1.0 loss_val: 0.24676314015429548 acc_val: 0.8999999985098839\n",
      "epoch 36 / 50 loss_train: 0.002180534107102848 acc_train: 1.0 loss_val: 0.25455225212347915 acc_val: 0.8666666621963183\n",
      "epoch 37 / 50 loss_train: 0.002157654669368323 acc_train: 1.0 loss_val: 0.2494712403711977 acc_val: 0.8999999985098839\n",
      "epoch 38 / 50 loss_train: 0.001881538488305523 acc_train: 1.0 loss_val: 0.25905845924497345 acc_val: 0.8666666621963183\n",
      "epoch 39 / 50 loss_train: 0.0018264993287907007 acc_train: 1.0 loss_val: 0.25332335594657707 acc_val: 0.8999999985098839\n",
      "epoch 40 / 50 loss_train: 0.0016359917809574615 acc_train: 1.0 loss_val: 0.261244568891622 acc_val: 0.8666666621963183\n",
      "epoch 41 / 50 loss_train: 0.001590180646140336 acc_train: 1.0 loss_val: 0.2575846618451578 acc_val: 0.8999999985098839\n",
      "epoch 42 / 50 loss_train: 0.0014168903245935444 acc_train: 1.0 loss_val: 0.26459211738574595 acc_val: 0.8999999985098839\n",
      "epoch 43 / 50 loss_train: 0.0013716603152715834 acc_train: 1.0 loss_val: 0.2623504785332595 acc_val: 0.8999999985098839\n",
      "epoch 44 / 50 loss_train: 0.0012625532748761939 acc_train: 1.0 loss_val: 0.26687945966386906 acc_val: 0.8999999985098839\n",
      "epoch 45 / 50 loss_train: 0.0011892575092271886 acc_train: 1.0 loss_val: 0.26698016165634425 acc_val: 0.8999999985098839\n",
      "epoch 46 / 50 loss_train: 0.0011110981407830916 acc_train: 1.0 loss_val: 0.26955977431517414 acc_val: 0.8999999985098839\n",
      "epoch 47 / 50 loss_train: 0.0010568409880049687 acc_train: 1.0 loss_val: 0.2706362206715009 acc_val: 0.8999999985098839\n",
      "epoch 48 / 50 loss_train: 0.0009819370710750302 acc_train: 1.0 loss_val: 0.27395570276428316 acc_val: 0.8999999985098839\n",
      "epoch 49 / 50 loss_train: 0.0009400885587539327 acc_train: 1.0 loss_val: 0.27397619614619845 acc_val: 0.8999999985098839\n",
      "Validation accuracy for lr 0.001 bs 5 hl [100, 50, 25] : 0.8999999985098839\n",
      "epoch 0 / 50 loss_train: 1.0934514024040916 acc_train: 0.4363636259328235 loss_val: 1.0204752286275227 acc_val: 0.4999999950329463\n",
      "epoch 1 / 50 loss_train: 1.0122617320580916 acc_train: 0.4818181747739965 loss_val: 0.9535224437713623 acc_val: 0.46666666865348816\n",
      "epoch 2 / 50 loss_train: 0.9453036460009488 acc_train: 0.4909090833230452 loss_val: 0.8919056057929993 acc_val: 0.5333333363135656\n",
      "epoch 3 / 50 loss_train: 0.872260879386555 acc_train: 0.6454545401714065 loss_val: 0.8226558963457743 acc_val: 0.6666666666666666\n",
      "epoch 4 / 50 loss_train: 0.7825123721903021 acc_train: 0.836363633247939 loss_val: 0.7458892464637756 acc_val: 0.6666666666666666\n",
      "epoch 5 / 50 loss_train: 0.6777887669476595 acc_train: 0.836363633247939 loss_val: 0.6664030949274699 acc_val: 0.699999988079071\n",
      "epoch 6 / 50 loss_train: 0.5667418837547302 acc_train: 0.8454545417969878 loss_val: 0.5913877983887991 acc_val: 0.699999988079071\n",
      "epoch 7 / 50 loss_train: 0.46622426130554895 acc_train: 0.8636363616043871 loss_val: 0.5290229519208273 acc_val: 0.699999988079071\n",
      "epoch 8 / 50 loss_train: 0.3904865058985623 acc_train: 0.8818181800571355 loss_val: 0.4828820526599884 acc_val: 0.7666666607062022\n",
      "epoch 9 / 50 loss_train: 0.3365739015015689 acc_train: 0.8818181800571355 loss_val: 0.44887229800224304 acc_val: 0.7999999970197678\n",
      "epoch 10 / 50 loss_train: 0.29581016437573865 acc_train: 0.8999999985098839 loss_val: 0.4223875105381012 acc_val: 0.8666666646798452\n",
      "epoch 11 / 50 loss_train: 0.26250697807832196 acc_train: 0.9181818169626322 loss_val: 0.40103134016195935 acc_val: 0.8666666646798452\n",
      "epoch 12 / 50 loss_train: 0.2341261397708546 acc_train: 0.9454545446417548 loss_val: 0.38297295570373535 acc_val: 0.8333333308498064\n",
      "epoch 13 / 50 loss_train: 0.2095883380283009 acc_train: 0.9636363630945032 loss_val: 0.36753756801287335 acc_val: 0.8333333308498064\n",
      "epoch 14 / 50 loss_train: 0.18822945112531836 acc_train: 0.9636363630945032 loss_val: 0.3538591613372167 acc_val: 0.8666666646798452\n",
      "epoch 15 / 50 loss_train: 0.16943052478811957 acc_train: 0.9727272723208774 loss_val: 0.3409145971139272 acc_val: 0.8666666646798452\n",
      "epoch 16 / 50 loss_train: 0.15271706134080887 acc_train: 0.9818181815472516 loss_val: 0.32853203515211743 acc_val: 0.7999999945362409\n",
      "epoch 17 / 50 loss_train: 0.13783623074943369 acc_train: 0.9818181815472516 loss_val: 0.31673143804073334 acc_val: 0.7999999945362409\n",
      "epoch 18 / 50 loss_train: 0.12459105964411389 acc_train: 0.9818181815472516 loss_val: 0.3054633488257726 acc_val: 0.7999999945362409\n",
      "epoch 19 / 50 loss_train: 0.11285555904561823 acc_train: 0.9818181815472516 loss_val: 0.2946779529253642 acc_val: 0.7999999945362409\n",
      "epoch 20 / 50 loss_train: 0.10250857709483667 acc_train: 0.9818181815472516 loss_val: 0.284979651371638 acc_val: 0.7999999945362409\n",
      "epoch 21 / 50 loss_train: 0.09343807196075266 acc_train: 0.9818181815472516 loss_val: 0.2763284047444661 acc_val: 0.7999999945362409\n",
      "epoch 22 / 50 loss_train: 0.08548024262894284 acc_train: 0.9818181815472516 loss_val: 0.2687973827123642 acc_val: 0.8333333283662796\n",
      "epoch 23 / 50 loss_train: 0.07849013890055093 acc_train: 0.9818181815472516 loss_val: 0.26216141879558563 acc_val: 0.8333333283662796\n",
      "epoch 24 / 50 loss_train: 0.07236999070102518 acc_train: 0.9909090907736258 loss_val: 0.25649429857730865 acc_val: 0.8333333283662796\n",
      "epoch 25 / 50 loss_train: 0.06702840819277546 acc_train: 0.9909090907736258 loss_val: 0.25147877633571625 acc_val: 0.8333333283662796\n",
      "epoch 26 / 50 loss_train: 0.06234621307389303 acc_train: 0.9909090907736258 loss_val: 0.24697724481423697 acc_val: 0.8333333283662796\n",
      "epoch 27 / 50 loss_train: 0.05823956514624032 acc_train: 1.0 loss_val: 0.24301626781622568 acc_val: 0.8333333283662796\n",
      "epoch 28 / 50 loss_train: 0.05460403216156093 acc_train: 1.0 loss_val: 0.2394944131374359 acc_val: 0.8333333283662796\n",
      "epoch 29 / 50 loss_train: 0.05138853801922365 acc_train: 1.0 loss_val: 0.23634278774261475 acc_val: 0.8333333283662796\n",
      "epoch 30 / 50 loss_train: 0.04847496578639204 acc_train: 1.0 loss_val: 0.23354106644789377 acc_val: 0.8333333283662796\n",
      "epoch 31 / 50 loss_train: 0.045872284353456715 acc_train: 1.0 loss_val: 0.2309927741686503 acc_val: 0.8333333283662796\n",
      "epoch 32 / 50 loss_train: 0.04350330350412564 acc_train: 1.0 loss_val: 0.22872362037499747 acc_val: 0.8666666646798452\n",
      "epoch 33 / 50 loss_train: 0.04138729365711862 acc_train: 1.0 loss_val: 0.22667182485262552 acc_val: 0.8666666646798452\n",
      "epoch 34 / 50 loss_train: 0.03948414892974225 acc_train: 1.0 loss_val: 0.2248841772476832 acc_val: 0.8666666646798452\n",
      "epoch 35 / 50 loss_train: 0.03773214147341522 acc_train: 1.0 loss_val: 0.22305097679297128 acc_val: 0.8666666646798452\n",
      "epoch 36 / 50 loss_train: 0.03611743632196025 acc_train: 1.0 loss_val: 0.22135927279790243 acc_val: 0.8666666646798452\n",
      "epoch 37 / 50 loss_train: 0.03464056517590176 acc_train: 1.0 loss_val: 0.21974936624368033 acc_val: 0.8666666646798452\n",
      "epoch 38 / 50 loss_train: 0.033256771847267046 acc_train: 1.0 loss_val: 0.21847874422868094 acc_val: 0.8666666646798452\n",
      "epoch 39 / 50 loss_train: 0.031990180130709304 acc_train: 1.0 loss_val: 0.21735196312268576 acc_val: 0.8666666646798452\n",
      "epoch 40 / 50 loss_train: 0.03081259254196828 acc_train: 1.0 loss_val: 0.216190367937088 acc_val: 0.8666666646798452\n",
      "epoch 41 / 50 loss_train: 0.029693921985612673 acc_train: 1.0 loss_val: 0.21487095952033997 acc_val: 0.8666666646798452\n",
      "epoch 42 / 50 loss_train: 0.028639693786813455 acc_train: 1.0 loss_val: 0.21353885034720102 acc_val: 0.8666666646798452\n",
      "epoch 43 / 50 loss_train: 0.027650460981848566 acc_train: 1.0 loss_val: 0.21236214538415274 acc_val: 0.8666666646798452\n",
      "epoch 44 / 50 loss_train: 0.026726603296331385 acc_train: 1.0 loss_val: 0.2113390862941742 acc_val: 0.8666666646798452\n",
      "epoch 45 / 50 loss_train: 0.025851780421693216 acc_train: 1.0 loss_val: 0.21027804414431253 acc_val: 0.8666666646798452\n",
      "epoch 46 / 50 loss_train: 0.02505101047625596 acc_train: 1.0 loss_val: 0.20948755741119385 acc_val: 0.8666666646798452\n",
      "epoch 47 / 50 loss_train: 0.024274311134253036 acc_train: 1.0 loss_val: 0.20859681069850922 acc_val: 0.8666666646798452\n",
      "epoch 48 / 50 loss_train: 0.02354536260562864 acc_train: 1.0 loss_val: 0.20789243280887604 acc_val: 0.8666666646798452\n",
      "epoch 49 / 50 loss_train: 0.022837989349764855 acc_train: 1.0 loss_val: 0.2072001794974009 acc_val: 0.8666666646798452\n",
      "Validation accuracy for lr 0.001 bs 10 hl [50, 25] : 0.8666666646798452\n",
      "epoch 0 / 50 loss_train: 1.0410782423886387 acc_train: 0.4454545399004763 loss_val: 0.9803183476130167 acc_val: 0.6333333353201548\n",
      "epoch 1 / 50 loss_train: 0.8781013922257856 acc_train: 0.7454545403068716 loss_val: 0.8474546074867249 acc_val: 0.6666666567325592\n",
      "epoch 2 / 50 loss_train: 0.6916615421121771 acc_train: 0.8090909048914909 loss_val: 0.7015828490257263 acc_val: 0.699999988079071\n",
      "epoch 3 / 50 loss_train: 0.5294296741485596 acc_train: 0.836363631893288 loss_val: 0.5823968251546224 acc_val: 0.699999988079071\n",
      "epoch 4 / 50 loss_train: 0.41331171176650305 acc_train: 0.8545454517006874 loss_val: 0.5013905962308248 acc_val: 0.7333333293596903\n",
      "epoch 5 / 50 loss_train: 0.326198628002947 acc_train: 0.8909090892835096 loss_val: 0.448354110121727 acc_val: 0.7666666631897291\n",
      "epoch 6 / 50 loss_train: 0.2577578669244593 acc_train: 0.9181818169626322 loss_val: 0.41329116622606915 acc_val: 0.7666666631897291\n",
      "epoch 7 / 50 loss_train: 0.20228551599112424 acc_train: 0.9454545446417548 loss_val: 0.38497018317381543 acc_val: 0.8333333308498064\n",
      "epoch 8 / 50 loss_train: 0.15730352225628766 acc_train: 0.9818181815472516 loss_val: 0.3547310183445613 acc_val: 0.8333333283662796\n",
      "epoch 9 / 50 loss_train: 0.12069840018044818 acc_train: 0.9909090907736258 loss_val: 0.32588725288709003 acc_val: 0.7999999945362409\n",
      "epoch 10 / 50 loss_train: 0.09244691749865358 acc_train: 0.9909090907736258 loss_val: 0.30083784461021423 acc_val: 0.7999999945362409\n",
      "epoch 11 / 50 loss_train: 0.072258379987695 acc_train: 0.9909090907736258 loss_val: 0.2797643840312958 acc_val: 0.8333333283662796\n",
      "epoch 12 / 50 loss_train: 0.05764381926168095 acc_train: 0.9909090907736258 loss_val: 0.265962948401769 acc_val: 0.8333333283662796\n",
      "epoch 13 / 50 loss_train: 0.04764851961623539 acc_train: 0.9909090907736258 loss_val: 0.258156086007754 acc_val: 0.8333333283662796\n",
      "epoch 14 / 50 loss_train: 0.040554046503860845 acc_train: 1.0 loss_val: 0.2522370169560115 acc_val: 0.8333333283662796\n",
      "epoch 15 / 50 loss_train: 0.035135911269621414 acc_train: 1.0 loss_val: 0.2479020059108734 acc_val: 0.8333333283662796\n",
      "epoch 16 / 50 loss_train: 0.031065974130549213 acc_train: 1.0 loss_val: 0.24511905014514923 acc_val: 0.8333333283662796\n",
      "epoch 17 / 50 loss_train: 0.02782954298891127 acc_train: 1.0 loss_val: 0.24345687528451285 acc_val: 0.8333333283662796\n",
      "epoch 18 / 50 loss_train: 0.025203166106207806 acc_train: 1.0 loss_val: 0.24197040994962057 acc_val: 0.8333333283662796\n",
      "epoch 19 / 50 loss_train: 0.0229991050648757 acc_train: 1.0 loss_val: 0.240310067931811 acc_val: 0.8333333283662796\n",
      "epoch 20 / 50 loss_train: 0.021138664631342344 acc_train: 1.0 loss_val: 0.2393431415160497 acc_val: 0.8333333283662796\n",
      "epoch 21 / 50 loss_train: 0.01951039857654409 acc_train: 1.0 loss_val: 0.2381077061096827 acc_val: 0.8333333283662796\n",
      "epoch 22 / 50 loss_train: 0.018080079525878482 acc_train: 1.0 loss_val: 0.23681703209877014 acc_val: 0.8333333283662796\n",
      "epoch 23 / 50 loss_train: 0.01680043624417687 acc_train: 1.0 loss_val: 0.2356426864862442 acc_val: 0.8333333283662796\n",
      "epoch 24 / 50 loss_train: 0.015666243016973815 acc_train: 1.0 loss_val: 0.23436666776736578 acc_val: 0.8333333283662796\n",
      "epoch 25 / 50 loss_train: 0.01460932243869386 acc_train: 1.0 loss_val: 0.2331450084845225 acc_val: 0.8333333283662796\n",
      "epoch 26 / 50 loss_train: 0.013648182450031692 acc_train: 1.0 loss_val: 0.23188060522079468 acc_val: 0.8333333283662796\n",
      "epoch 27 / 50 loss_train: 0.012771176946857437 acc_train: 1.0 loss_val: 0.23037485778331757 acc_val: 0.8333333283662796\n",
      "epoch 28 / 50 loss_train: 0.011958321360659531 acc_train: 1.0 loss_val: 0.22901582717895508 acc_val: 0.8333333283662796\n",
      "epoch 29 / 50 loss_train: 0.011229080809491941 acc_train: 1.0 loss_val: 0.22782857219378153 acc_val: 0.8333333283662796\n",
      "epoch 30 / 50 loss_train: 0.010543164426714859 acc_train: 1.0 loss_val: 0.22648747265338898 acc_val: 0.8333333283662796\n",
      "epoch 31 / 50 loss_train: 0.009908566140273417 acc_train: 1.0 loss_val: 0.22536120315392813 acc_val: 0.8666666621963183\n",
      "epoch 32 / 50 loss_train: 0.009310502842047506 acc_train: 1.0 loss_val: 0.2241659164428711 acc_val: 0.8666666621963183\n",
      "epoch 33 / 50 loss_train: 0.008768353288442913 acc_train: 1.0 loss_val: 0.22329886506001154 acc_val: 0.8666666621963183\n",
      "epoch 34 / 50 loss_train: 0.008276825442656198 acc_train: 1.0 loss_val: 0.2227165512740612 acc_val: 0.8666666621963183\n",
      "epoch 35 / 50 loss_train: 0.007805634591601451 acc_train: 1.0 loss_val: 0.2216063067317009 acc_val: 0.8666666621963183\n",
      "epoch 36 / 50 loss_train: 0.007364755231802436 acc_train: 1.0 loss_val: 0.22070019443829855 acc_val: 0.8666666621963183\n",
      "epoch 37 / 50 loss_train: 0.006943140758349645 acc_train: 1.0 loss_val: 0.2201798881093661 acc_val: 0.8666666621963183\n",
      "epoch 38 / 50 loss_train: 0.00655804138329007 acc_train: 1.0 loss_val: 0.21986847122510275 acc_val: 0.8666666621963183\n",
      "epoch 39 / 50 loss_train: 0.006194411779605699 acc_train: 1.0 loss_val: 0.219062810142835 acc_val: 0.8666666621963183\n",
      "epoch 40 / 50 loss_train: 0.005855393610545434 acc_train: 1.0 loss_val: 0.21842519814769426 acc_val: 0.8666666621963183\n",
      "epoch 41 / 50 loss_train: 0.005536039582115005 acc_train: 1.0 loss_val: 0.2175421298791965 acc_val: 0.8666666621963183\n",
      "epoch 42 / 50 loss_train: 0.005231380960057405 acc_train: 1.0 loss_val: 0.218022208660841 acc_val: 0.8999999985098839\n",
      "epoch 43 / 50 loss_train: 0.004970123975495385 acc_train: 1.0 loss_val: 0.21849893033504486 acc_val: 0.8999999985098839\n",
      "epoch 44 / 50 loss_train: 0.0047107153474247425 acc_train: 1.0 loss_val: 0.2186982724815607 acc_val: 0.8999999985098839\n",
      "epoch 45 / 50 loss_train: 0.0044464582262497224 acc_train: 1.0 loss_val: 0.21862286639710268 acc_val: 0.8999999985098839\n",
      "epoch 46 / 50 loss_train: 0.004226907504330898 acc_train: 1.0 loss_val: 0.219133992989858 acc_val: 0.8999999985098839\n",
      "epoch 47 / 50 loss_train: 0.003994231109655547 acc_train: 1.0 loss_val: 0.21946292743086815 acc_val: 0.8999999985098839\n",
      "epoch 48 / 50 loss_train: 0.0038112998445285484 acc_train: 1.0 loss_val: 0.21954627831776938 acc_val: 0.8999999985098839\n",
      "epoch 49 / 50 loss_train: 0.0036086201282821344 acc_train: 1.0 loss_val: 0.2203935623789827 acc_val: 0.8999999985098839\n",
      "Validation accuracy for lr 0.001 bs 10 hl [100, 50, 25] : 0.8999999985098839\n",
      "epoch 0 / 50 loss_train: 1.088326702947202 acc_train: 0.22608694175015326 loss_val: 1.0899662971496582 acc_val: 0.19999998807907104\n",
      "epoch 1 / 50 loss_train: 1.0747579025185627 acc_train: 0.38260868580445 loss_val: 1.0774838328361511 acc_val: 0.3333333258827527\n",
      "epoch 2 / 50 loss_train: 1.0615593091301296 acc_train: 0.48695651215055713 loss_val: 1.0649697581926982 acc_val: 0.43333333482344943\n",
      "epoch 3 / 50 loss_train: 1.0481574794520503 acc_train: 0.5999999920959058 loss_val: 1.052330772082011 acc_val: 0.6333333303531011\n",
      "epoch 4 / 50 loss_train: 1.0343544275864311 acc_train: 0.6695652105238127 loss_val: 1.039659559726715 acc_val: 0.6999999955296516\n",
      "epoch 5 / 50 loss_train: 1.0200008646301602 acc_train: 0.8173913009788679 loss_val: 1.0268021424611409 acc_val: 0.8333333308498064\n",
      "epoch 6 / 50 loss_train: 1.0053529246993687 acc_train: 0.878260867751163 loss_val: 1.0139258404572804 acc_val: 0.8666666646798452\n",
      "epoch 7 / 50 loss_train: 0.9901905578115712 acc_train: 0.8956521723581397 loss_val: 1.0006211002667744 acc_val: 0.8666666646798452\n",
      "epoch 8 / 50 loss_train: 0.97404167185659 acc_train: 0.8695652154476746 loss_val: 0.9867227971553802 acc_val: 0.8333333308498064\n",
      "epoch 9 / 50 loss_train: 0.956434252469436 acc_train: 0.8695652154476746 loss_val: 0.9718125661214193 acc_val: 0.7999999970197678\n",
      "epoch 10 / 50 loss_train: 0.9378639615100363 acc_train: 0.878260867751163 loss_val: 0.9560169378916422 acc_val: 0.7666666631897291\n",
      "epoch 11 / 50 loss_train: 0.9183632938758187 acc_train: 0.878260867751163 loss_val: 0.9401160776615143 acc_val: 0.7666666631897291\n",
      "epoch 12 / 50 loss_train: 0.8985922492068746 acc_train: 0.878260867751163 loss_val: 0.9236932297547659 acc_val: 0.8333333308498064\n",
      "epoch 13 / 50 loss_train: 0.8784550324730251 acc_train: 0.8869565200546513 loss_val: 0.9071784913539886 acc_val: 0.7999999970197678\n",
      "epoch 14 / 50 loss_train: 0.8581574999767801 acc_train: 0.8869565200546513 loss_val: 0.8905397554238638 acc_val: 0.7999999970197678\n",
      "epoch 15 / 50 loss_train: 0.8377245560936306 acc_train: 0.8869565200546513 loss_val: 0.8737966120243073 acc_val: 0.7999999970197678\n",
      "epoch 16 / 50 loss_train: 0.8171088254970053 acc_train: 0.878260867751163 loss_val: 0.8569578528404236 acc_val: 0.7999999970197678\n",
      "epoch 17 / 50 loss_train: 0.796215269876563 acc_train: 0.878260867751163 loss_val: 0.8400469919045767 acc_val: 0.7999999970197678\n",
      "epoch 18 / 50 loss_train: 0.775043785572052 acc_train: 0.878260867751163 loss_val: 0.8230213125546774 acc_val: 0.7999999970197678\n",
      "epoch 19 / 50 loss_train: 0.7535123591837676 acc_train: 0.878260867751163 loss_val: 0.8058452010154724 acc_val: 0.7999999970197678\n",
      "epoch 20 / 50 loss_train: 0.7317217173783676 acc_train: 0.878260867751163 loss_val: 0.788688580195109 acc_val: 0.7999999970197678\n",
      "epoch 21 / 50 loss_train: 0.7099378342213838 acc_train: 0.878260867751163 loss_val: 0.7715847392876943 acc_val: 0.7999999970197678\n",
      "epoch 22 / 50 loss_train: 0.6882381802019866 acc_train: 0.878260867751163 loss_val: 0.7545610070228577 acc_val: 0.7999999970197678\n",
      "epoch 23 / 50 loss_train: 0.6668428649073062 acc_train: 0.8956521723581397 loss_val: 0.737891435623169 acc_val: 0.7999999970197678\n",
      "epoch 24 / 50 loss_train: 0.6458210504573324 acc_train: 0.8956521723581397 loss_val: 0.7215849161148071 acc_val: 0.7999999970197678\n",
      "epoch 25 / 50 loss_train: 0.6252431804719178 acc_train: 0.8956521723581397 loss_val: 0.7057351619005203 acc_val: 0.7999999970197678\n",
      "epoch 26 / 50 loss_train: 0.6052180697088656 acc_train: 0.8956521723581397 loss_val: 0.6903702517350515 acc_val: 0.7999999970197678\n",
      "epoch 27 / 50 loss_train: 0.5858314348303754 acc_train: 0.8956521723581397 loss_val: 0.6755349189043045 acc_val: 0.7999999970197678\n",
      "epoch 28 / 50 loss_train: 0.5671177130678425 acc_train: 0.8956521723581397 loss_val: 0.661252369483312 acc_val: 0.7999999970197678\n",
      "epoch 29 / 50 loss_train: 0.5491107546764872 acc_train: 0.9043478246616281 loss_val: 0.6475313156843185 acc_val: 0.7999999970197678\n",
      "epoch 30 / 50 loss_train: 0.5318488945131716 acc_train: 0.9043478246616281 loss_val: 0.6343824217716852 acc_val: 0.7999999970197678\n",
      "epoch 31 / 50 loss_train: 0.5153643035370371 acc_train: 0.9043478246616281 loss_val: 0.6218695441881815 acc_val: 0.7999999970197678\n",
      "epoch 32 / 50 loss_train: 0.49964707960253174 acc_train: 0.9043478246616281 loss_val: 0.609905461470286 acc_val: 0.7999999970197678\n",
      "epoch 33 / 50 loss_train: 0.48470112422238226 acc_train: 0.9043478246616281 loss_val: 0.5985272824764252 acc_val: 0.7999999970197678\n",
      "epoch 34 / 50 loss_train: 0.4704845910486968 acc_train: 0.9043478246616281 loss_val: 0.5877424577871958 acc_val: 0.7999999970197678\n",
      "epoch 35 / 50 loss_train: 0.456985671883044 acc_train: 0.9043478246616281 loss_val: 0.5774857799212137 acc_val: 0.7999999970197678\n",
      "epoch 36 / 50 loss_train: 0.4441774837348772 acc_train: 0.9043478246616281 loss_val: 0.5677438527345657 acc_val: 0.7999999970197678\n",
      "epoch 37 / 50 loss_train: 0.432019525895948 acc_train: 0.9043478246616281 loss_val: 0.5584907780090967 acc_val: 0.7999999970197678\n",
      "epoch 38 / 50 loss_train: 0.4204987080200859 acc_train: 0.9043478246616281 loss_val: 0.5497040823101997 acc_val: 0.8333333308498064\n",
      "epoch 39 / 50 loss_train: 0.40956393920856976 acc_train: 0.9043478246616281 loss_val: 0.5413583740592003 acc_val: 0.8333333308498064\n",
      "epoch 40 / 50 loss_train: 0.39919783434142236 acc_train: 0.9043478246616281 loss_val: 0.5334351932009062 acc_val: 0.8333333308498064\n",
      "epoch 41 / 50 loss_train: 0.38938153891459754 acc_train: 0.9043478246616281 loss_val: 0.5259254376093546 acc_val: 0.8333333308498064\n",
      "epoch 42 / 50 loss_train: 0.3800554826207783 acc_train: 0.9043478246616281 loss_val: 0.5187739183505377 acc_val: 0.8333333308498064\n",
      "epoch 43 / 50 loss_train: 0.37117222728936566 acc_train: 0.9043478246616281 loss_val: 0.5119480192661285 acc_val: 0.8333333308498064\n",
      "epoch 44 / 50 loss_train: 0.3627016324064006 acc_train: 0.9043478246616281 loss_val: 0.5054423163334528 acc_val: 0.8333333308498064\n",
      "epoch 45 / 50 loss_train: 0.35462344599806744 acc_train: 0.9043478246616281 loss_val: 0.49922740707794827 acc_val: 0.8333333308498064\n",
      "epoch 46 / 50 loss_train: 0.3469013021044109 acc_train: 0.9043478246616281 loss_val: 0.49328883240620297 acc_val: 0.7999999970197678\n",
      "epoch 47 / 50 loss_train: 0.3395063993723496 acc_train: 0.9043478246616281 loss_val: 0.48758576810359955 acc_val: 0.7999999970197678\n",
      "epoch 48 / 50 loss_train: 0.3324161612469217 acc_train: 0.9043478246616281 loss_val: 0.4820966174205144 acc_val: 0.7999999970197678\n",
      "epoch 49 / 50 loss_train: 0.3256092265896175 acc_train: 0.9043478246616281 loss_val: 0.47680001705884933 acc_val: 0.7999999970197678\n",
      "Validation accuracy for lr 0.0001 bs 5 hl [50, 25] : 0.7999999970197678\n",
      "epoch 0 / 50 loss_train: 1.0820254191108372 acc_train: 0.46956520948721014 loss_val: 1.0771124362945557 acc_val: 0.4999999950329463\n",
      "epoch 1 / 50 loss_train: 1.0579893692680027 acc_train: 0.660869555628818 loss_val: 1.0580421487490337 acc_val: 0.6333333303531011\n",
      "epoch 2 / 50 loss_train: 1.0318599965261377 acc_train: 0.660869555628818 loss_val: 1.0369572937488556 acc_val: 0.6333333303531011\n",
      "epoch 3 / 50 loss_train: 1.0011456142301145 acc_train: 0.660869555628818 loss_val: 1.0127737720807393 acc_val: 0.5666666626930237\n",
      "epoch 4 / 50 loss_train: 0.966581378294074 acc_train: 0.660869555628818 loss_val: 0.9867507716019949 acc_val: 0.5666666626930237\n",
      "epoch 5 / 50 loss_train: 0.9292838780776315 acc_train: 0.660869555628818 loss_val: 0.9595122635364532 acc_val: 0.5666666626930237\n",
      "epoch 6 / 50 loss_train: 0.8898206223612246 acc_train: 0.6956521667864012 loss_val: 0.9305813411871592 acc_val: 0.5999999940395355\n",
      "epoch 7 / 50 loss_train: 0.8490292274433634 acc_train: 0.7478260812552079 loss_val: 0.9006714920202891 acc_val: 0.6999999955296516\n",
      "epoch 8 / 50 loss_train: 0.8076142435488494 acc_train: 0.7826086911170379 loss_val: 0.8696431716283163 acc_val: 0.6999999955296516\n",
      "epoch 9 / 50 loss_train: 0.766653413357942 acc_train: 0.7999999963718912 loss_val: 0.8392292459805807 acc_val: 0.6999999955296516\n",
      "epoch 10 / 50 loss_train: 0.7274085555387579 acc_train: 0.8173913009788679 loss_val: 0.8098045786221822 acc_val: 0.6999999955296516\n",
      "epoch 11 / 50 loss_train: 0.6908557479796202 acc_train: 0.8260869532823563 loss_val: 0.7812211910883585 acc_val: 0.6999999955296516\n",
      "epoch 12 / 50 loss_train: 0.6573107670182767 acc_train: 0.8347826062337212 loss_val: 0.7543110350767771 acc_val: 0.6999999955296516\n",
      "epoch 13 / 50 loss_train: 0.6267127109610516 acc_train: 0.8434782585372096 loss_val: 0.7289809981981913 acc_val: 0.6999999955296516\n",
      "epoch 14 / 50 loss_train: 0.5985858427441638 acc_train: 0.8521739108406979 loss_val: 0.7050278335809708 acc_val: 0.7333333293596903\n",
      "epoch 15 / 50 loss_train: 0.5726428245720656 acc_train: 0.8695652154476746 loss_val: 0.6816359808047613 acc_val: 0.7333333293596903\n",
      "epoch 16 / 50 loss_train: 0.5483316915190738 acc_train: 0.8869565200546513 loss_val: 0.6593695878982544 acc_val: 0.7333333293596903\n",
      "epoch 17 / 50 loss_train: 0.5253246271091959 acc_train: 0.8869565200546513 loss_val: 0.6380857676267624 acc_val: 0.7333333293596903\n",
      "epoch 18 / 50 loss_train: 0.5034477438615717 acc_train: 0.9043478246616281 loss_val: 0.6177476743857065 acc_val: 0.7333333293596903\n",
      "epoch 19 / 50 loss_train: 0.48248654992684076 acc_train: 0.9043478246616281 loss_val: 0.598503458003203 acc_val: 0.7333333293596903\n",
      "epoch 20 / 50 loss_train: 0.4623765569666158 acc_train: 0.9043478246616281 loss_val: 0.580173671245575 acc_val: 0.7666666631897291\n",
      "epoch 21 / 50 loss_train: 0.4430355624012325 acc_train: 0.9043478246616281 loss_val: 0.5627873266736666 acc_val: 0.7999999945362409\n",
      "epoch 22 / 50 loss_train: 0.4243216916270878 acc_train: 0.9043478246616281 loss_val: 0.546521283686161 acc_val: 0.7999999945362409\n",
      "epoch 23 / 50 loss_train: 0.40639536212319916 acc_train: 0.9130434769651165 loss_val: 0.5307690675059954 acc_val: 0.7999999945362409\n",
      "epoch 24 / 50 loss_train: 0.3889691713063613 acc_train: 0.9130434769651165 loss_val: 0.5160579085350037 acc_val: 0.8333333283662796\n",
      "epoch 25 / 50 loss_train: 0.3722813913355703 acc_train: 0.9217391292686048 loss_val: 0.5022154673933983 acc_val: 0.8666666646798452\n",
      "epoch 26 / 50 loss_train: 0.35614445157673047 acc_train: 0.9304347815720931 loss_val: 0.4892577702800433 acc_val: 0.8666666646798452\n",
      "epoch 27 / 50 loss_train: 0.34073715106300684 acc_train: 0.9304347815720931 loss_val: 0.47688060998916626 acc_val: 0.8666666646798452\n",
      "epoch 28 / 50 loss_train: 0.3259541560774264 acc_train: 0.9304347815720931 loss_val: 0.4654648204644521 acc_val: 0.8666666646798452\n",
      "epoch 29 / 50 loss_train: 0.31181970627411554 acc_train: 0.9391304338755815 loss_val: 0.4545200454692046 acc_val: 0.8666666646798452\n",
      "epoch 30 / 50 loss_train: 0.2982438436668852 acc_train: 0.9478260861790698 loss_val: 0.4442390004793803 acc_val: 0.8666666646798452\n",
      "epoch 31 / 50 loss_train: 0.2852489585461824 acc_train: 0.9565217384825582 loss_val: 0.43464746574560803 acc_val: 0.8666666646798452\n",
      "epoch 32 / 50 loss_train: 0.2728712455086086 acc_train: 0.9565217384825582 loss_val: 0.42575225606560707 acc_val: 0.8666666646798452\n",
      "epoch 33 / 50 loss_train: 0.26110789276983426 acc_train: 0.9565217384825582 loss_val: 0.4173929765820503 acc_val: 0.8333333308498064\n",
      "epoch 34 / 50 loss_train: 0.24984900912512903 acc_train: 0.9565217384825582 loss_val: 0.40957267830769223 acc_val: 0.8333333308498064\n",
      "epoch 35 / 50 loss_train: 0.23914829609186752 acc_train: 0.9565217384825582 loss_val: 0.4022290234764417 acc_val: 0.8333333308498064\n",
      "epoch 36 / 50 loss_train: 0.22897898729728616 acc_train: 0.9565217384825582 loss_val: 0.39528945460915565 acc_val: 0.8666666646798452\n",
      "epoch 37 / 50 loss_train: 0.2193009513227836 acc_train: 0.973913043089535 loss_val: 0.38870126629869145 acc_val: 0.8666666646798452\n",
      "epoch 38 / 50 loss_train: 0.21009717460559763 acc_train: 0.973913043089535 loss_val: 0.3824271609385808 acc_val: 0.8666666646798452\n",
      "epoch 39 / 50 loss_train: 0.2013163699404053 acc_train: 0.973913043089535 loss_val: 0.3764447153856357 acc_val: 0.8666666646798452\n",
      "epoch 40 / 50 loss_train: 0.19291428735722665 acc_train: 0.973913043089535 loss_val: 0.3707267493009567 acc_val: 0.8666666646798452\n",
      "epoch 41 / 50 loss_train: 0.18492319191927495 acc_train: 0.973913043089535 loss_val: 0.36527489498257637 acc_val: 0.8666666646798452\n",
      "epoch 42 / 50 loss_train: 0.17732241215265315 acc_train: 0.9826086953930233 loss_val: 0.3600792655100425 acc_val: 0.8666666646798452\n",
      "epoch 43 / 50 loss_train: 0.17007367928390918 acc_train: 0.9826086953930233 loss_val: 0.35523606340090436 acc_val: 0.8666666646798452\n",
      "epoch 44 / 50 loss_train: 0.16314339475787204 acc_train: 0.9826086953930233 loss_val: 0.35065597978730995 acc_val: 0.8666666646798452\n",
      "epoch 45 / 50 loss_train: 0.15656225755810738 acc_train: 0.9826086953930233 loss_val: 0.3462388465801875 acc_val: 0.8666666646798452\n",
      "epoch 46 / 50 loss_train: 0.1502569840978021 acc_train: 0.9826086953930233 loss_val: 0.342092410971721 acc_val: 0.8666666646798452\n",
      "epoch 47 / 50 loss_train: 0.1442773892827656 acc_train: 0.9826086953930233 loss_val: 0.33805806810657185 acc_val: 0.8666666646798452\n",
      "epoch 48 / 50 loss_train: 0.13856789242962134 acc_train: 0.9826086953930233 loss_val: 0.33414710126817226 acc_val: 0.8666666646798452\n",
      "epoch 49 / 50 loss_train: 0.13317357562482357 acc_train: 0.9826086953930233 loss_val: 0.33044496923685074 acc_val: 0.8666666646798452\n",
      "Validation accuracy for lr 0.0001 bs 5 hl [100, 50, 25] : 0.8666666646798452\n",
      "epoch 0 / 50 loss_train: 1.12055504322052 acc_train: -0.21818181601437656 loss_val: 1.1102626323699951 acc_val: -0.10000002384185791\n",
      "epoch 1 / 50 loss_train: 1.1139800873669712 acc_train: -0.18181818181818182 loss_val: 1.1057090361913045 acc_val: -0.1333333651224772\n",
      "epoch 2 / 50 loss_train: 1.1075612631711094 acc_train: -0.07272727381099355 loss_val: 1.101308027903239 acc_val: -0.0666666825612386\n",
      "epoch 3 / 50 loss_train: 1.1012781099839644 acc_train: 0.12727271968668158 loss_val: 1.096975286801656 acc_val: 0.03333332141240438\n",
      "epoch 4 / 50 loss_train: 1.095093477856029 acc_train: 0.3181818127632141 loss_val: 1.092704216639201 acc_val: 0.09999998410542806\n",
      "epoch 5 / 50 loss_train: 1.0889237468892878 acc_train: 0.3909090892835097 loss_val: 1.0883779923121135 acc_val: 0.23333332935969034\n",
      "epoch 6 / 50 loss_train: 1.0827609300613403 acc_train: 0.4454545405778018 loss_val: 1.0838673114776611 acc_val: 0.3333333134651184\n",
      "epoch 7 / 50 loss_train: 1.0765528245405718 acc_train: 0.6090909039432352 loss_val: 1.0792879263559978 acc_val: 0.46666664878527325\n",
      "epoch 8 / 50 loss_train: 1.070278612050143 acc_train: 0.6999999934976752 loss_val: 1.0746139685312908 acc_val: 0.5333333214124044\n",
      "epoch 9 / 50 loss_train: 1.063878362829035 acc_train: 0.7545454495332458 loss_val: 1.0698604583740234 acc_val: 0.5999999890724818\n",
      "epoch 10 / 50 loss_train: 1.0572039647535845 acc_train: 0.7727272679859941 loss_val: 1.0648783445358276 acc_val: 0.5999999890724818\n",
      "epoch 11 / 50 loss_train: 1.0501970486207441 acc_train: 0.7999999970197678 loss_val: 1.0595789750417073 acc_val: 0.5999999890724818\n",
      "epoch 12 / 50 loss_train: 1.0428430167111484 acc_train: 0.809090906246142 loss_val: 1.0538541475931804 acc_val: 0.6666666567325592\n",
      "epoch 13 / 50 loss_train: 1.0351130853999744 acc_train: 0.8272727246988903 loss_val: 1.047603686650594 acc_val: 0.6666666567325592\n",
      "epoch 14 / 50 loss_train: 1.0269260081377896 acc_train: 0.8363636339252646 loss_val: 1.040919820467631 acc_val: 0.6666666567325592\n",
      "epoch 15 / 50 loss_train: 1.018417943607677 acc_train: 0.88181817937981 loss_val: 1.0339814027150471 acc_val: 0.6666666567325592\n",
      "epoch 16 / 50 loss_train: 1.0095577781850642 acc_train: 0.88181817937981 loss_val: 1.026780128479004 acc_val: 0.6666666567325592\n",
      "epoch 17 / 50 loss_train: 1.0003606839613481 acc_train: 0.8727272701534358 loss_val: 1.0192939440409343 acc_val: 0.6666666567325592\n",
      "epoch 18 / 50 loss_train: 0.9908042062412609 acc_train: 0.8727272701534358 loss_val: 1.0115269025166829 acc_val: 0.6999999980131785\n",
      "epoch 19 / 50 loss_train: 0.9809653488072482 acc_train: 0.8818181800571355 loss_val: 1.0034607847531636 acc_val: 0.6999999980131785\n",
      "epoch 20 / 50 loss_train: 0.9708268967541781 acc_train: 0.8818181800571355 loss_val: 0.9952192306518555 acc_val: 0.6999999980131785\n",
      "epoch 21 / 50 loss_train: 0.96036743034016 acc_train: 0.8818181800571355 loss_val: 0.9866998195648193 acc_val: 0.6999999980131785\n",
      "epoch 22 / 50 loss_train: 0.9496439695358276 acc_train: 0.8727272708307613 loss_val: 0.9779350956281027 acc_val: 0.7333333293596903\n",
      "epoch 23 / 50 loss_train: 0.9386935396627947 acc_train: 0.8727272708307613 loss_val: 0.9690000216166178 acc_val: 0.7333333293596903\n",
      "epoch 24 / 50 loss_train: 0.9275259104642001 acc_train: 0.8727272708307613 loss_val: 0.9598919351895651 acc_val: 0.7333333293596903\n",
      "epoch 25 / 50 loss_train: 0.9161079200831327 acc_train: 0.8727272708307613 loss_val: 0.9506070216496786 acc_val: 0.7333333293596903\n",
      "epoch 26 / 50 loss_train: 0.9044971736994657 acc_train: 0.8727272708307613 loss_val: 0.9411730368932089 acc_val: 0.7333333293596903\n",
      "epoch 27 / 50 loss_train: 0.8926982771266591 acc_train: 0.8727272708307613 loss_val: 0.931629498799642 acc_val: 0.7333333293596903\n",
      "epoch 28 / 50 loss_train: 0.8807405070825056 acc_train: 0.8727272708307613 loss_val: 0.9219713807106018 acc_val: 0.7333333293596903\n",
      "epoch 29 / 50 loss_train: 0.8686134706843983 acc_train: 0.8727272708307613 loss_val: 0.9121870001157125 acc_val: 0.7333333293596903\n",
      "epoch 30 / 50 loss_train: 0.8563265150243585 acc_train: 0.8727272708307613 loss_val: 0.9022983511288961 acc_val: 0.7333333293596903\n",
      "epoch 31 / 50 loss_train: 0.8438770066608082 acc_train: 0.8636363616043871 loss_val: 0.8923051953315735 acc_val: 0.7333333293596903\n",
      "epoch 32 / 50 loss_train: 0.8313120711933483 acc_train: 0.8636363616043871 loss_val: 0.8822111487388611 acc_val: 0.7333333293596903\n",
      "epoch 33 / 50 loss_train: 0.8186950791965831 acc_train: 0.8636363616043871 loss_val: 0.8720308542251587 acc_val: 0.7333333293596903\n",
      "epoch 34 / 50 loss_train: 0.8059996691617098 acc_train: 0.8545454517006874 loss_val: 0.8618019223213196 acc_val: 0.7333333293596903\n",
      "epoch 35 / 50 loss_train: 0.7932744784788652 acc_train: 0.8545454517006874 loss_val: 0.8514896631240845 acc_val: 0.7333333293596903\n",
      "epoch 36 / 50 loss_train: 0.7805189056829973 acc_train: 0.8545454517006874 loss_val: 0.8411944707234701 acc_val: 0.7333333293596903\n",
      "epoch 37 / 50 loss_train: 0.7677799138155851 acc_train: 0.8545454517006874 loss_val: 0.8309390147527059 acc_val: 0.7333333293596903\n",
      "epoch 38 / 50 loss_train: 0.7550779743628069 acc_train: 0.8545454517006874 loss_val: 0.8207046786944071 acc_val: 0.7333333293596903\n",
      "epoch 39 / 50 loss_train: 0.7424224235794761 acc_train: 0.8545454517006874 loss_val: 0.810482124487559 acc_val: 0.7666666607062022\n",
      "epoch 40 / 50 loss_train: 0.7297957864674655 acc_train: 0.8545454517006874 loss_val: 0.8003501296043396 acc_val: 0.7333333293596903\n",
      "epoch 41 / 50 loss_train: 0.7172168818387118 acc_train: 0.8545454517006874 loss_val: 0.7902657985687256 acc_val: 0.7333333293596903\n",
      "epoch 42 / 50 loss_train: 0.704674319787459 acc_train: 0.8545454517006874 loss_val: 0.7802174886067709 acc_val: 0.7333333293596903\n",
      "epoch 43 / 50 loss_train: 0.6922385421666232 acc_train: 0.8545454517006874 loss_val: 0.7702994147936503 acc_val: 0.7333333293596903\n",
      "epoch 44 / 50 loss_train: 0.67995530908758 acc_train: 0.8545454517006874 loss_val: 0.7605344653129578 acc_val: 0.7333333293596903\n",
      "epoch 45 / 50 loss_train: 0.6678462028503418 acc_train: 0.8545454517006874 loss_val: 0.750893751780192 acc_val: 0.7333333293596903\n",
      "epoch 46 / 50 loss_train: 0.6559149785475298 acc_train: 0.8545454517006874 loss_val: 0.7414273222287496 acc_val: 0.7333333293596903\n",
      "epoch 47 / 50 loss_train: 0.6441836709325964 acc_train: 0.8545454517006874 loss_val: 0.7320829431215922 acc_val: 0.7333333293596903\n",
      "epoch 48 / 50 loss_train: 0.6325454657728021 acc_train: 0.8545454517006874 loss_val: 0.7227528889973959 acc_val: 0.7333333293596903\n",
      "epoch 49 / 50 loss_train: 0.6209288916804574 acc_train: 0.8545454517006874 loss_val: 0.7133501172065735 acc_val: 0.7333333293596903\n",
      "Validation accuracy for lr 0.0001 bs 10 hl [50, 25] : 0.7333333293596903\n",
      "epoch 0 / 50 loss_train: 1.1024584553458474 acc_train: 0.08181817423213612 loss_val: 1.1186178127924602 acc_val: -0.1666666865348816\n",
      "epoch 1 / 50 loss_train: 1.0864980220794678 acc_train: 0.08181817423213612 loss_val: 1.1063299973805745 acc_val: -0.1666666865348816\n",
      "epoch 2 / 50 loss_train: 1.0717144933613865 acc_train: 0.08181817423213612 loss_val: 1.0947292645772297 acc_val: -0.1666666865348816\n",
      "epoch 3 / 50 loss_train: 1.0577078353274951 acc_train: 0.1363636228171262 loss_val: 1.0830173095067341 acc_val: -0.03333336114883423\n",
      "epoch 4 / 50 loss_train: 1.0440732728351245 acc_train: 0.31818181005391205 loss_val: 1.0709201097488403 acc_val: 0.36666666467984516\n",
      "epoch 5 / 50 loss_train: 1.0305784398859197 acc_train: 0.5727272622964599 loss_val: 1.058459997177124 acc_val: 0.5666666676600774\n",
      "epoch 6 / 50 loss_train: 1.0168988542123274 acc_train: 0.6454545361074534 loss_val: 1.045768717924754 acc_val: 0.6333333353201548\n",
      "epoch 7 / 50 loss_train: 1.003064664927396 acc_train: 0.6636363559148528 loss_val: 1.0329914291699727 acc_val: 0.6333333353201548\n",
      "epoch 8 / 50 loss_train: 0.9889230565591292 acc_train: 0.6636363559148528 loss_val: 1.0201460321744282 acc_val: 0.5666666726271311\n",
      "epoch 9 / 50 loss_train: 0.9744845086877997 acc_train: 0.6636363559148528 loss_val: 1.006881872812907 acc_val: 0.5666666726271311\n",
      "epoch 10 / 50 loss_train: 0.9596415595574812 acc_train: 0.6636363559148528 loss_val: 0.99275141954422 acc_val: 0.5666666726271311\n",
      "epoch 11 / 50 loss_train: 0.9441092827103355 acc_train: 0.6636363559148528 loss_val: 0.9776105483373007 acc_val: 0.5666666726271311\n",
      "epoch 12 / 50 loss_train: 0.9277697693217885 acc_train: 0.6636363559148528 loss_val: 0.9613450566927592 acc_val: 0.5666666726271311\n",
      "epoch 13 / 50 loss_train: 0.9103917696259238 acc_train: 0.6636363559148528 loss_val: 0.9442071914672852 acc_val: 0.5666666726271311\n",
      "epoch 14 / 50 loss_train: 0.8923240520737388 acc_train: 0.6636363559148528 loss_val: 0.9266834855079651 acc_val: 0.5666666726271311\n",
      "epoch 15 / 50 loss_train: 0.8736918893727389 acc_train: 0.6636363559148528 loss_val: 0.9086256225903829 acc_val: 0.5666666726271311\n",
      "epoch 16 / 50 loss_train: 0.8540638034993951 acc_train: 0.6636363559148528 loss_val: 0.8894895911216736 acc_val: 0.5666666726271311\n",
      "epoch 17 / 50 loss_train: 0.8332756161689758 acc_train: 0.6636363559148528 loss_val: 0.869648297627767 acc_val: 0.5666666726271311\n",
      "epoch 18 / 50 loss_train: 0.8120117295872081 acc_train: 0.6636363559148528 loss_val: 0.8497284253438314 acc_val: 0.5666666726271311\n",
      "epoch 19 / 50 loss_train: 0.790665864944458 acc_train: 0.6636363559148528 loss_val: 0.8302416205406189 acc_val: 0.5666666726271311\n",
      "epoch 20 / 50 loss_train: 0.7693194530226968 acc_train: 0.6636363559148528 loss_val: 0.8110791047414144 acc_val: 0.5666666726271311\n",
      "epoch 21 / 50 loss_train: 0.7478244846517389 acc_train: 0.6636363559148528 loss_val: 0.7922640442848206 acc_val: 0.5666666726271311\n",
      "epoch 22 / 50 loss_train: 0.726096738468517 acc_train: 0.6636363559148528 loss_val: 0.7739085555076599 acc_val: 0.5666666726271311\n",
      "epoch 23 / 50 loss_train: 0.7043129476633939 acc_train: 0.6727272644639015 loss_val: 0.7559605638186137 acc_val: 0.5666666726271311\n",
      "epoch 24 / 50 loss_train: 0.6826038252223622 acc_train: 0.6818181736902758 loss_val: 0.7385395566622416 acc_val: 0.5666666726271311\n",
      "epoch 25 / 50 loss_train: 0.6610967896201394 acc_train: 0.6909090835939754 loss_val: 0.7214493354161581 acc_val: 0.5666666726271311\n",
      "epoch 26 / 50 loss_train: 0.6398644555698741 acc_train: 0.6999999934976752 loss_val: 0.7048669854799906 acc_val: 0.5999999940395355\n",
      "epoch 27 / 50 loss_train: 0.619065826589411 acc_train: 0.718181811273098 loss_val: 0.688799520333608 acc_val: 0.5999999940395355\n",
      "epoch 28 / 50 loss_train: 0.5988548398017883 acc_train: 0.7272727198221467 loss_val: 0.6733111043771108 acc_val: 0.6333333353201548\n",
      "epoch 29 / 50 loss_train: 0.5792520506815477 acc_train: 0.7363636310804974 loss_val: 0.6582584281762441 acc_val: 0.6333333353201548\n",
      "epoch 30 / 50 loss_train: 0.5603119297461077 acc_train: 0.7545454495332458 loss_val: 0.6436087489128113 acc_val: 0.699999988079071\n",
      "epoch 31 / 50 loss_train: 0.5418679280714556 acc_train: 0.7727272679859941 loss_val: 0.629463771979014 acc_val: 0.699999988079071\n",
      "epoch 32 / 50 loss_train: 0.5241861722686074 acc_train: 0.7999999963424422 loss_val: 0.6160845955212911 acc_val: 0.699999988079071\n",
      "epoch 33 / 50 loss_train: 0.5072177296335046 acc_train: 0.8272727240215648 loss_val: 0.6032481491565704 acc_val: 0.699999988079071\n",
      "epoch 34 / 50 loss_train: 0.49057197028940375 acc_train: 0.8272727240215648 loss_val: 0.5908294022083282 acc_val: 0.699999988079071\n",
      "epoch 35 / 50 loss_train: 0.47462689876556396 acc_train: 0.8454545431516387 loss_val: 0.5790714025497437 acc_val: 0.699999988079071\n",
      "epoch 36 / 50 loss_train: 0.45955384319478815 acc_train: 0.8454545431516387 loss_val: 0.5679099261760712 acc_val: 0.699999988079071\n",
      "epoch 37 / 50 loss_train: 0.4452516463669864 acc_train: 0.8454545431516387 loss_val: 0.5572471221288046 acc_val: 0.699999988079071\n",
      "epoch 38 / 50 loss_train: 0.4316819350827824 acc_train: 0.8545454517006874 loss_val: 0.5472730795542399 acc_val: 0.699999988079071\n",
      "epoch 39 / 50 loss_train: 0.4188510057601062 acc_train: 0.8545454517006874 loss_val: 0.5378557542959849 acc_val: 0.7333333293596903\n",
      "epoch 40 / 50 loss_train: 0.4066538309509104 acc_train: 0.8727272708307613 loss_val: 0.5287907520929972 acc_val: 0.7333333293596903\n",
      "epoch 41 / 50 loss_train: 0.3950228677554564 acc_train: 0.8818181800571355 loss_val: 0.5199984908103943 acc_val: 0.7333333293596903\n",
      "epoch 42 / 50 loss_train: 0.3838997767730193 acc_train: 0.8818181800571355 loss_val: 0.5115566154321035 acc_val: 0.7333333293596903\n",
      "epoch 43 / 50 loss_train: 0.37311582402749494 acc_train: 0.8818181800571355 loss_val: 0.503425657749176 acc_val: 0.7333333293596903\n",
      "epoch 44 / 50 loss_train: 0.36253489689393475 acc_train: 0.8999999985098839 loss_val: 0.495628297328949 acc_val: 0.7666666607062022\n",
      "epoch 45 / 50 loss_train: 0.3522657183083621 acc_train: 0.8999999985098839 loss_val: 0.48814045389493305 acc_val: 0.7666666607062022\n",
      "epoch 46 / 50 loss_train: 0.34230498563159595 acc_train: 0.8909090892835096 loss_val: 0.4809492727120717 acc_val: 0.7999999970197678\n",
      "epoch 47 / 50 loss_train: 0.3326831378720023 acc_train: 0.8909090892835096 loss_val: 0.47393744190533954 acc_val: 0.7999999970197678\n",
      "epoch 48 / 50 loss_train: 0.3233797157352621 acc_train: 0.8999999985098839 loss_val: 0.46726946036020917 acc_val: 0.7999999970197678\n",
      "epoch 49 / 50 loss_train: 0.31440452147613873 acc_train: 0.8999999985098839 loss_val: 0.4609321852525075 acc_val: 0.7999999970197678\n",
      "Validation accuracy for lr 0.0001 bs 10 hl [100, 50, 25] : 0.7999999970197678\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"./data/iris_data.csv\", sep=\";\")\n",
    "codes = { 'Iris-setosa' : 0, 'Iris-versicolor' : 1, 'Iris-virginica' : 2 }\n",
    "\n",
    "X, Y = extract_numpy_from_df( dataframe, 4 )\n",
    "indices_train,indices_validation = get_validation_and_training_indices( X.shape[0] )\n",
    "\n",
    "X   = normalize( X )\n",
    "Y   = hot_1_encode( Y, codes )\n",
    "\n",
    "for lr in [0.001, 0.0001]:\n",
    "    for batch_size in [5, 10]:\n",
    "        for hidden_layers in [[50, 25], [100, 50, 25]]:\n",
    "            writer = SummaryWriter(log_dir= 'runs/iris', comment=f\"lr_{lr}_bs_{batch_size}_layers_{hidden_layers}\")\n",
    "            model = create_model(X.shape[1], hidden_layers, Y.shape[1])\n",
    "            optimizer = torch.optim.Adam( params = model.parameters(), lr=lr )\n",
    "            loss_fn   = torch.nn.CrossEntropyLoss()\n",
    "            \n",
    "            losses_train           = []\n",
    "            accuracies_train       = []\n",
    "            losses_validation      = []\n",
    "            accuracies_validation  = []\n",
    "            n_epochs = 50\n",
    "\n",
    "            for i in range (n_epochs):\n",
    "                metrics_train = train_one_epoch( i, indices_train, X, Y, optimizer, loss_fn, batch_size, writer )\n",
    "                losses_train.append(metrics_train[0])\n",
    "                accuracies_train.append(metrics_train[1])\n",
    "                metrics_val = validate_one_epoch( i, indices_validation, X, Y, model, loss_fn, batch_size, writer )\n",
    "                losses_validation.append(metrics_val[0])\n",
    "                accuracies_validation.append(metrics_val[1])\n",
    "                print(\"epoch\",i,\"/\",n_epochs,\"loss_train:\",metrics_train[0],\"acc_train:\",metrics_train[1],\"loss_val:\",metrics_val[0],\"acc_val:\",metrics_val[1])\n",
    "\n",
    "            print(\"Validation accuracy for lr\",lr,\"bs\",batch_size,\"hl\",hidden_layers,\":\",accuracies_validation[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
